{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Zeus","text":"Deep Learning Energy Measurement and Optimization <p>Project News \u26a1 </p> <ul> <li>[2024/05] Zeus is now a PyTorch ecosystem project. Read the PyTorch blog post here!</li> <li>[2024/02] Zeus was selected as a 2024 Mozilla Technology Fund awardee!</li> <li>[2023/12] We released Perseus, an energy optimizer for large model training: Preprint | Blog | Optimizer</li> <li>[2023/07] We used the <code>ZeusMonitor</code> to profile GPU time and energy consumption for the ML.ENERGY leaderboard &amp; Colosseum.</li> </ul> <p>Zeus is a library for (1) measuring the energy consumption of Deep Learning workloads and (2) optimizing their energy consumption.</p> <p>Zeus is part of The ML.ENERGY Initiative.</p>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<ul> <li>Getting Started: Instructions on installation and setup.</li> <li>Measuring Energy: How to measure time and energy programmatically and on the command line.</li> <li>Optimizing Energy: How to optimize energy.</li> <li>Research Overview: Overview of the research papers Zeus is rooted on.</li> <li>Source Code Reference: Auto-generated source code reference for the entire codebase.</li> </ul> <p>We also provide usage examples in our GitHub repository.</p>"},{"location":"#contact","title":"Contact","text":"<p>Jae-Won Chung (jwnchung@umich.edu)</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Most of the common setup steps are described in this page. Some optimizers or examples may require some extra setup steps, which are described in the corresponding documentation.</p>"},{"location":"getting_started/#installing-the-python-package","title":"Installing the Python package","text":""},{"location":"getting_started/#from-pypi","title":"From PyPI","text":"<p>Install the Zeus Python package simply with:</p> <pre><code>pip install zeus-ml\n</code></pre>"},{"location":"getting_started/#from-source-for-development","title":"From source for development","text":"<p>You can also install Zeus from source by cloning our GitHub repository. Specifically for development, you can do an editable installation with extra dev dependencies:</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\ncd zeus\npip install -e '.[dev]'\n</code></pre>"},{"location":"getting_started/#using-docker","title":"Using Docker","text":"<p>Dependencies</p> <p>You should have the following already installed on your system:</p> <ul> <li>Docker</li> <li>NVIDIA Container Toolkit</li> </ul> <p>Our Docker image should suit most of the use cases for Zeus. On top of the <code>nvidia/cuda:11.8.0-base-ubuntu22.04</code> image, we add:</p> <ul> <li>Miniconda 3, PyTorch, and Torchvision</li> <li>A copy of the Zeus repo in <code>/workspace/zeus</code></li> </ul> docker/Dockerfile Dockerfile<pre><code># Copyright (C) 2023 Jae-Won Chung &lt;jwnchung@umich.edu&gt;\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Build instructions\n#   If you're building this image locally, make sure you specify `TARGETARCH`.\n#   Currently, this image supports `amd64` and `arm64`. For instance:\n#     docker build -t mlenergy/zeus:master --build-arg TARGETARCH=amd64 .\n\nFROM nvidia/cuda:11.8.0-base-ubuntu22.04\n\n# Basic installs\nARG DEBIAN_FRONTEND=noninteractive\nENV TZ='America/Detroit'\nRUN apt-get update -qq \\\n    &amp;&amp; apt-get -y --no-install-recommends install \\\n       build-essential software-properties-common wget git tar rsync cmake \\\n    &amp;&amp; apt-get clean all \\\n    &amp;&amp; rm -r /var/lib/apt/lists/*\n\n# Install Miniconda3 23.3.1\nENV PATH=\"/root/.local/miniconda3/bin:$PATH\"\nARG TARGETARCH\nRUN if [ \"$TARGETARCH\" = \"amd64\" ]; then \\\n      export CONDA_INSTALLER_PATH=\"Miniconda3-py39_23.3.1-0-Linux-x86_64.sh\"; \\\n    elif [ \"$TARGETARCH\" = \"arm64\" ]; then \\\n      export CONDA_INSTALLER_PATH=\"Miniconda3-py39_23.3.1-0-Linux-aarch64.sh\"; \\\n    else \\\n      echo \"Unsupported architecture ${TARGETARCH}\" &amp;&amp; exit 1; \\\n    fi \\\n    &amp;&amp; mkdir -p /root/.local \\\n    &amp;&amp; wget \"https://repo.anaconda.com/miniconda/$CONDA_INSTALLER_PATH\" \\\n    &amp;&amp; mkdir /root/.conda \\\n    &amp;&amp; bash \"$CONDA_INSTALLER_PATH\" -b -p /root/.local/miniconda3 \\\n    &amp;&amp; rm -f \"$CONDA_INSTALLER_PATH\" \\\n    &amp;&amp; ln -sf /root/.local/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh\n\n# Install PyTorch and CUDA Toolkit\nRUN pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n\n# Place stuff under /workspace\nWORKDIR /workspace\n\n# Snapshot of Zeus\nADD . /workspace/zeus\n\n# When an outside zeus directory is mounted, have it apply immediately.\nRUN cd /workspace/zeus &amp;&amp; pip install --no-cache-dir -e .\n</code></pre> <p>The default command would be:</p> <pre><code>docker run -it \\\n    --gpus all \\                 # (1)!\n    --cap-add SYS_ADMIN \\       # (2)!\n    --ipc host \\               # (3)!\n    mlenergy/zeus:latest \\\n    bash\n</code></pre> <ol> <li>Mounts all GPUs into the Docker container.</li> <li><code>SYS_ADMIN</code> capability is needed to change the GPU's power limit or frequency. See here.</li> <li>PyTorch DataLoader workers need enough shared memory for IPC. Without this, they may run out of shared memory and die.</li> </ol> <p>Overriding Zeus installation</p> <p>Inside the container, <code>zeus</code>'s installation is editable (<code>pip install -e</code>). So, you can mount your locally modified Zeus repository into the right path in the container (<code>-v /path/to/zeus:/workspace/zeus</code>), and your modifications will automatically be applied without you having to run <code>pip install</code> again.</p>"},{"location":"getting_started/#pulling-from-docker-hub","title":"Pulling from Docker Hub","text":"<p>Pre-built images are hosted on Docker Hub. There are three types of images available:</p> <ul> <li><code>latest</code>: The latest versioned release.</li> <li><code>v*</code>: Each versioned release.</li> <li><code>master</code>: The <code>HEAD</code> commit of Zeus. Usually stable enough, and you will get all the new features.</li> </ul>"},{"location":"getting_started/#building-the-image-locally","title":"Building the image locally","text":"<p>You should specify <code>TARGETARCH</code> to be one of <code>amd64</code> or <code>arm64</code> based on your environment:</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\ncd zeus\ndocker build -t mlenergy/zeus:master --build-arg TARGETARCH=amd64 -f docker/Dockerfile .\n</code></pre>"},{"location":"getting_started/#system-privileges","title":"System privileges","text":"<p>Nevermind if you're just measuring</p> <p>No special system-level privileges are needed if you are just measuring time and energy. However, when you're looking into optimizing energy and if that method requires changing the GPU's power limit or SM frequency, special system-level privileges are required.</p>"},{"location":"getting_started/#when-are-extra-system-privileges-needed","title":"When are extra system privileges needed?","text":"<p>The Linux capability <code>SYS_ADMIN</code> is required in order to change the GPU's power limit or frequency. Specifically, this is needed by the <code>GlobalPowerLimitOptimizer</code> and the <code>PipelineFrequencyOptimizer</code>.</p>"},{"location":"getting_started/#obtaining-privileges-with-docker","title":"Obtaining privileges with Docker","text":"<p>Using Docker, you can pass <code>--cap-add SYS_ADMIN</code> to <code>docker run</code>. Since this significantly simplifies running Zeus, we recommend users to consider this option first. Also, since Zeus is running inside a container, there is less potential for damage even if things go wrong.</p>"},{"location":"getting_started/#obtaining-privileges-with-sudo","title":"Obtaining privileges with <code>sudo</code>","text":"<p>If you cannot use Docker, you can run your application with <code>sudo</code>. This is not recommended due to security reasons, but it will work.</p>"},{"location":"getting_started/#gpu-management-server","title":"GPU management server","text":"<p>It is fair to say that granting <code>SYS_ADMIN</code> to the application is itself giving too much privilege. We just need to be able to change the GPU's power limit or frequency, instead of giving the process privileges to administer the system. Thus, to reduce the attack surface, we are considering solutions such as a separate GPU management server process on a node (tracking issue), which has <code>SYS_ADMIN</code>. Then, an unprivileged application process can ask the GPU management server via a UDS to change the GPU's configuration on its behalf.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Measuring energy with the <code>ZeusMonitor</code>, programmatically or in the command line.</li> <li>Optimizing energy with Zeus energy optimizers.</li> </ul>"},{"location":"measure/","title":"Measuring Energy","text":"<p>Zeus makes it very easy to measure time, power, and energy both programmatically in Python and also on the command line. Measuring power and energy is also very low overhead, typically taking less than 10 ms for each call.</p>"},{"location":"measure/#programmatic-measurement","title":"Programmatic measurement","text":"<p><code>ZeusMonitor</code> makes it very simple to measure the GPU time and energy consumption of arbitrary Python code blocks.</p> <p>A measurement window is defined by a code block wrapped with <code>begin_window</code> and <code>end_window</code>. <code>end_window</code> will return a <code>Measurement</code> object, which holds the time and energy consumption of the window. Users can specify and measure multiple measurement windows at the same time, and they can be arbitrarily nested or overlapping as long as they are given different names.</p> <pre><code>from zeus.monitor import ZeusMonitor\n\nif __name__ == \"__main__\":\n    # All GPUs are measured simultaneously if `gpu_indices` is not given.\n    monitor = ZeusMonitor(gpu_indices=[torch.cuda.current_device()])\n\n    for epoch in range(100):\n        monitor.begin_window(\"epoch\")\n\n        steps = []\n        for x, y in train_loader:\n            monitor.begin_window(\"step\")\n            train_one_step(x, y)\n            result = monitor.end_window(\"step\")\n            steps.append(result)\n\n        mes = monitor.end_window(\"epoch\")\n        print(f\"Epoch {epoch} consumed {mes.time} s and {mes.total_energy} J.\")\n\n        avg_time = sum(map(lambda m: m.time, steps)) / len(steps)\n        avg_energy = sum(map(lambda m: m.total_energy, steps)) / len(steps)\n        print(f\"One step took {avg_time} s and {avg_energy} J on average.\")\n</code></pre> <p><code>zeus.monitor.PowerMonitor</code></p> <p>This monitor spawns a process that polls the instantaneous GPU power consumption API and exposes two methods: <code>get_power</code> and <code>get_energy</code>. For GPUs older than Volta that do not support querying energy directly, <code>ZeusMonitor</code> automatically uses the <code>PowerMonitor</code> internally.</p> <p>Use of global variables on GPUs older than Volta</p> <p>On GPUs older than Volta, you should not instantiate <code>ZeusMonitor</code> as a global variable without protecting it with <code>if __name__ == \"__main__\"</code>. It's because the energy query API is only available on Volta or newer NVIDIA GPU microarchitectures, and for older GPUs, a separate process that polls the power API has to be spawned (i.e., <code>PowerMonitor</code>). In this case, global code that spawns the process should be guarded with <code>if __name__ == \"__main__\"</code>. More details in Python docs.</p> <p><code>gpu_indices</code> and <code>CUDA_VISIBLE_DEVICES</code></p> <p>Zeus always respects <code>CUDA_VISIBLE_DEVICES</code> if set. In other words, if <code>CUDA_VISIBLE_DEVICES=1,3</code> and <code>gpu_indices=[1]</code>, Zeus will understand that as GPU 3 in the system.</p> <p><code>gpu_indices</code> and optimization</p> <p>In general, energy optimizers measure the energy of the GPU through a <code>ZeusMonitor</code> instance that is passed to their constructor. Thus, only the GPUs specified by <code>gpu_indices</code> will be the target of optimization.</p>"},{"location":"measure/#cli-power-and-energy-monitor","title":"CLI power and energy monitor","text":"<p>The energy monitor measures the total energy consumed by the GPU during the lifetime of the monitor process. It's a simple wrapper around <code>ZeusMonitor</code>.</p> <pre><code>$ python -m zeus.monitor energy\n[2023-08-22 22:44:45,106] [ZeusMonitor](energy.py:157) Monitoring GPU [0, 1, 2, 3].\n[2023-08-22 22:44:46,210] [zeus.utils.framework](framework.py:38) PyTorch with CUDA support is available.\n[2023-08-22 22:44:46,760] [ZeusMonitor](energy.py:329) Measurement window 'zeus.monitor.energy' started.\n^C[2023-08-22 22:44:50,205] [ZeusMonitor](energy.py:329) Measurement window 'zeus.monitor.energy' ended.\nTotal energy (J):\nMeasurement(time=3.4480526447296143, energy={0: 224.2969999909401, 1: 232.83799999952316, 2: 233.3100000023842, 3: 234.53700000047684})\n</code></pre> <p>The power monitor periodically prints out the GPU's power draw. It's a simple wrapper around <code>PowerMonitor</code>.</p> <pre><code>$ python -m zeus.monitor power\n[2023-08-22 22:39:59,787] [PowerMonitor](power.py:134) Monitoring power usage of GPUs [0, 1, 2, 3]\n2023-08-22 22:40:00.800576\n{'GPU0': 66.176, 'GPU1': 68.792, 'GPU2': 66.898, 'GPU3': 67.53}\n2023-08-22 22:40:01.842590\n{'GPU0': 66.078, 'GPU1': 68.595, 'GPU2': 66.996, 'GPU3': 67.138}\n2023-08-22 22:40:02.845734\n{'GPU0': 66.078, 'GPU1': 68.693, 'GPU2': 66.898, 'GPU3': 67.236}\n2023-08-22 22:40:03.848818\n{'GPU0': 66.177, 'GPU1': 68.675, 'GPU2': 67.094, 'GPU3': 66.926}\n^C\nTotal time (s): 4.421529293060303\nTotal energy (J):\n{'GPU0': 198.52566362297537, 'GPU1': 206.22215216255188, 'GPU2': 201.08565518283845, 'GPU3': 201.79834523367884}\n</code></pre>"},{"location":"optimize/","title":"Optimizing Energy","text":"<p>Zeus provides multiple optimizers that tune different knobs either in the Deep Learning workload-side or the GPU-side.</p>"},{"location":"optimize/#power-limit-optimizer","title":"Power limit optimizer","text":"<p>Finds the optimal GPU power limit for DNN training. Users can control what optimal means, including minimum energy, minimum energy given maximum training slowdown, and minimum cost (linear combination of time and energy).</p>"},{"location":"optimize/#batch-size-optimizer","title":"Batch size optimizer","text":"<p>Finds the optimal DNN training batch size for training jobs that recur over time. This would be especially useful for production training jobs where the underlying dataset is constantly updated, and the model is periodically re-trained to keep it up-to-date.</p>"},{"location":"optimize/#pipeline-frequency-optimizer","title":"Pipeline frequency optimizer","text":"<p>In large model training (e.g., pre-training Large Language Models), pipeline parallelism is almost essential today. The pipeline frequency optimizer plans the GPU SM frequency across time for an iteration of pipeline parallel training. It generates a set of frequency plans, including a plan that reduces energy the most, another that reduces energy with negligible slowdown, and plans in the middle.</p>"},{"location":"optimize/batch_size_optimizer/","title":"Batch Size Optimizer","text":"<p>The batch size optimizer (BSO) finds the optimal DNN training batch size that minimizes cost:</p> \\[ \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA} \\] <p>where ETA and TTA stands for Energy-to-Accuracy and Time-to-Accuracy, respectively, and accuracy is the target validation metric of training. Users can trade-off between energy and time by setting the \\(\\eta\\) parameter to be between 0 and 1.</p>"},{"location":"optimize/batch_size_optimizer/#usage","title":"Usage","text":"<p>In production environments, it is common for a single DNN to be trained and re-trained many times over time. This is because the data distribution changes over time, and the model needs to be re-trained to adapt to the new data distribution. The batch size optimizer uses these recurring training jobs as opportunities to find the optimal batch size for the DNN training job. To do this, the batch size optimizer uses a Multi-Armed Bandit algorithm to explore the batch size space. For more details, please refer to the Zeus paper.</p> <p>Constraints</p> <p>Currently, the batch size optimizer only supports cases where the number and type of GPUs used for each recurrent training job is always the same.</p>"},{"location":"optimize/batch_size_optimizer/#high-level-architecture","title":"High-level architecture","text":"<pre><code>sequenceDiagram;\n    participant BSO server\n    participant BSO client\n    loop Every recurrent training\n        BSO client-&gt;&gt;BSO server: Register the training job and ask for the batch size to use\n        BSO server-&gt;&gt;BSO client: Return the batch size to use and an integer trial number\n        loop Every epoch\n            BSO client-&gt;&gt;BSO server: At the end of each epoch, report time and energy consumption\n            BSO server-&gt;&gt;BSO client: Compute cost and determine whether the client should terminate training\n        end\n        BSO client-&gt;&gt;BSO server: Notify the server that the trial finished\n    end</code></pre> <p>In order to persist the state of the optimizer across all recurring training runs, we need to have a server that outlives individual training jobs. Therefore, the batch size optimizer consists of two parts: the server and the client. The server is a FastAPI server that manages the Multi-Armed Bandit algorithm and the database, while the client (<code>BatchSizeOptimizer</code>) is integrated into the training script.</p>"},{"location":"optimize/batch_size_optimizer/#deploying-the-server","title":"Deploying the server","text":"<p>Largely three steps: (1) starting the database, (2) running migration on the database, and (3) starting the BSO server.</p>"},{"location":"optimize/batch_size_optimizer/#clone-the-zeus-repository","title":"Clone the Zeus repository","text":"<p>To get example docker files and database migration scripts, clone the Zeus repository.</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\n</code></pre>"},{"location":"optimize/batch_size_optimizer/#decide-on-the-database","title":"Decide on the database","text":"<p>By default, our examples use MySQL. However, you can use any database supported by SQLAlchemy. Please make sure the database's corresponding async connection driver (e.g., <code>asyncmy</code> for MySQL, <code>aiosqlite</code> for SQLite) is installed. For instance, to adapt our examples, you can (1) add <code>pip install</code> to <code>migration.Dockerfile</code> and <code>server.Dockerfile</code>, and (2) change the <code>db</code> container specification in <code>server-docker-compose.yaml</code>.</p>"},{"location":"optimize/batch_size_optimizer/#server-configuration","title":"Server configuration","text":"<p>You can configure the server using enviornment variables or the <code>.env</code> file. Below are the complete list of environment variables you can set and example values.</p> <pre><code>ZEUS_BSO_DB_USER=\"me\" \nZEUS_BSO_DB_PASSWORD=\"secret\"\nZEUS_BSO_DATABASE_URL=\"mysql+asyncmy://me:secret@localhost:3306/Zeus\"\nZEUS_BSO_ROOT_PASSWORD=\"secret*\"\nZEUS_BSO_SERVER_PORT=8000\nZEUS_BSO_LOG_LEVEL=\"INFO\"\nZEUS_BSO_ECHO_SQL=\"True\"\n</code></pre>"},{"location":"optimize/batch_size_optimizer/#with-docker-compose","title":"With Docker Compose","text":"<pre><code>cd docker/batch_size_optimizer\ndocker-compose -f ./server-docker-compose.yaml up\n</code></pre> <p>Docker Compose will first build necessary images and spin up the containers.</p>"},{"location":"optimize/batch_size_optimizer/#with-kubernetes","title":"With Kubernetes","text":"<ol> <li> <p>Build the Docker image.</p> <pre><code># From the repository root\ndocker build -f ./docker/batch_size_optimizer/server.Dockerfile -t bso-server . \ndocker build -f ./docker/batch_size_optimizer/migration.Dockerfile -t bso-migration .\n</code></pre> <p>Make sure Kubernetes has access to these build images. If you are locally using <code>minikube</code>, then the images are already available. However, if you are using the cloud such as AWS EKS, you should push the image to the registry and modify the image path in <code>server-docker-compose.yaml</code> to allow Kubernetes to pull the image.</p> </li> <li> <p>Convert Docker Compose files to Kubernetes YAML files using Kompose.</p> <pre><code>cd docker/batch_size_optimizer\ndocker-compose -f server-docker-compose.yaml config &gt; server-docker-compose-resolved.yaml\nkompose convert -f server-docker-compose-resolved.yaml -o ./kube/\nrm server-docker-compose-resolved.yaml\n</code></pre> <p>This first resolves env files using <code>docker-compose</code>, then converts it into Kubernetes YAML files in <code>./kube/</code>.</p> </li> <li> <p>Apply the Kubernetes YAML files.</p> <pre><code>cd kube\nkubectl apply -f .\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#with-just-python","title":"With just Python","text":"<p>You can also run the server without Docker or Kubernetes.</p> <ol> <li>Spin up the database of your choice.</li> <li>Set server configuration environment variables described here.</li> <li> <p>Perform DB migration with Alembic.</p> <ol> <li> <p>Install dependencies</p> <pre><code># From the repository root\npip install asyncmy cryptography\npip install '.[migration]'\n</code></pre> </li> <li> <p>Create the migration script. This will create scripts in <code>./versions</code>.</p> <pre><code>cd zeus/optimizer/batch_size\nalembic revision --autogenerate -m \"Create tables\" \n</code></pre> </li> <li> <p>Apply migration, either online or offline.</p> <pre><code># Online migration (applied directly to the database)\nalembic upgrade head \n# Offline migration (just generate SQL)\nalembic upgrade head --sql\n</code></pre> </li> </ol> </li> <li> <p>Spin up the server using <code>uvicorn</code>.</p> <pre><code>uvicorn zeus.optimizer.batch_size.server.router:app\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#integrating-batchsizeoptimizer","title":"Integrating <code>BatchSizeOptimizer</code>","text":"<p>In order for your recurring training job to communicate with the BSO server, you need to integrate the <code>BatchSizeOptimizer</code> class into your training script.</p> <ol> <li> <p>Install the Zeus package, including dependencies needed for the batch size optimizer.</p> <pre><code>pip install zeus-ml[bso]\n</code></pre> </li> <li> <p>Integrate <code>BatchSizeOptimizer</code> to your training script.</p> <pre><code>from zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.batch_size import BatchSizeOptimizer, JobSpec\n\nmonitor = ZeusMonitor()\n\n# On instantiation, the BSO will register the job to the BSO server\nbso = BatchSizeOptimizer(\n    monitor=monitor,\n    server_url=\"http://bso-server:8000\",\n    job=JobSpec(\n        job_id=os.environ.get(\"ZEUS_JOB_ID\"),\n        job_id_prefix=\"mnist\",\n        default_batch_size=256,\n        batch_sizes=[32, 64, 256, 512, 1024, 4096, 2048],\n        max_epochs=100\n    ),\n)\n\n# Get batch size to use from the server\nbatch_size = bso.get_batch_size()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n\n# Start measuring the time and energy consumption of this training run\nbso.on_train_begin()\n\nfor epoch in range(100):\n    for batch in train_dataloader:\n        # Training loop\n        pass\n\n    # The BSO server needs to know whether training has converged\n    metric = evaluate(model, eval_dataloader)\n    bso.on_evaluate(metric)\n\n    # The BSO server will determine whether to stop training\n    if bso.training_finished:\n        break\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#when-does-training-stop","title":"When does training stop?","text":"<p>The BSO server will determine whether to stop training and this will be reflected in the <code>training_finished</code> attribute of the <code>BatchSizeOptimizer</code> instance.</p> <p>If the DNN reaches the target validation metric, training should stop. However, training fails if</p> <ol> <li>it failed to converge within the configured <code>JobSpec.max_epochs</code> (reference) epochs, or</li> <li>its cost exceeded the early stopping threshold configured by <code>JobSpec.beta_knob</code>(reference) .</li> </ol> <p>In such failure cases, the optimizer will raise a <code>ZeusBSOTrainFailError</code>. This means that the chosen batch size was not useful, and the BSO server will never try this batch size again. The user should re-launch the training run in this case, and the BSO server will try another batch size.</p>"},{"location":"optimize/batch_size_optimizer/#integration-examples","title":"Integration examples","text":"<p>Two full examples are given for the batch size optimizer:</p> <ul> <li>MNIST: Single-GPU and data parallel training, with integration examples with Kubeflow</li> <li>Sentiment Analysis: Full training example with HuggingFace transformers using the Capriccio dataset, a sentiment analysis dataset with data drift.</li> </ul>"},{"location":"optimize/pipeline_frequency_optimizer/","title":"Pipeline Frequency Optimizer","text":"<p>The pipeline frequency optimizer optimizes the energy consumption of large model training, e.g., LLM pretraining.</p> <p>The core observation is that in pipeline parallel training, it is very difficult to split pipeline stages in perfectly equal size. Even for models like GPT, the first stage has the embeddings and the last stage has the language model head, making perfect balance nearly impossible to achieve. The pipeline frequency optimizer is based on our research paper Perseus. For more details about Perseus, check out our blog post.</p> The pipeline frequency optimizer in action"},{"location":"optimize/pipeline_frequency_optimizer/#usage","title":"Usage","text":"<p>Currently, it's a three-step process:</p> <ol> <li>Profile: Profile the computation time and energy consumption of the forward and backward instructions in each stage and each GPU frequency and the P2P blocking power consumption of the GPU.</li> <li>Optimize: Use <code>lowtime</code> to generate all Pareto-optimal frequency plans.</li> <li>Choose and start training: Among all the frequency plans generated by <code>lowtime</code>, choose the one that suits your use case.</li> </ol> <p>We have a reference integration with the large model training framework Merak, which supports 3D parallelism and automatically tracing and partitioning Hugging Face models. We've smoothed out some rough edges, integrated Zeus, and added example training scripts for GPT-3, BERT, and Wide-ResNet (pretty much any <code>torchvision</code> model).</p> <p>You don't have to be tied to Merak. If you have your own training framework, and you can integrate the pipeline frequency optimizer following the integration guide.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#profile","title":"Profile","text":"<p>In order to run our optimization algorithm, we need the time &amp; energy profiling information of the forward and backward instruction in each stage for every GPU frequency. The CSV file should look like this for a 4-stage pipeline:</p> <pre><code>stage,instruction,frequency,time,energy\n0,forward,1740,0.09373254776000976,28.4944\n0,forward,1725,0.09390360514322917,28.434366666666666\n0,forward,1710,0.09381131331125896,28.288966666666667\n...\n0,backward,1740,0.24533510557810465,69.5691\n0,backward,1725,0.24538559118906658,69.2552\n0,backward,1710,0.24548352559407552,68.89453333333334\n...\n3,backward,690,0.4184921979904175,68.12243333333333\n3,backward,675,0.42459266185760497,68.77603333333334\n3,backward,660,0.4306272824605306,69.39623333333334\n</code></pre> <p>Since different frameworks and model implementations will have different performance, it's best to obtain these profiling results on the framework and model you'll be using. That being said, you can obtain this profiling information in however way you want as long as they have all the columns in the reference CSV file above. But as a reference, we have implemented an automatic profiler in Merak. Please refer to the examples directory in Merak for profiling instructions.</p> <p>Finally, we also need to take into account the power consumption of the GPU while it is blocking on P2P communication, i.e., waiting for either the activation or gradient from its neighbor stage. You can use our profiling script for that.</p> <p>Tip</p> <p>As you profile the time and energy consumption of an instruction, you will scan down from the highest to the lowest frequency. However, as you lower the GPU's frequency, both time and energy will start to inflate after some point. In other words, those frequencies take more time and energy and are simply inefficient (i.e., Pareto-suboptimal), so we won't be running anything with those frequencies. Therefore, you actually don't need to profile time and energy for every frequency. A good heuristic is to scan from higher frequencies to lower ones, and once energy consumption increases more than five consecutive times, just stop there.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#optimize","title":"Optimize","text":"<p>With the CSV file that holds profiling results, you can use <code>lowtime</code> to generate all Pareto-optimal frequency plans.</p> <p>See <code>examples/pipeline_frequency_optimizer</code> to find the script <code>run_optimization.py</code>.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#choose-and-start-training","title":"Choose and start training","text":"<p>Running <code>lowtime</code> optimization will produce a set of frequency assignment files (<code>freqs_pipeline_%05d.py</code>). Each file is also annotated with estimates for time and cost. The larger the number, the shorter the expected iteration time.</p> <p>Then, start the PFO server and plug in the frequency plan you chose:</p> <pre><code>$ docker exec -it merak-zeus bash\n# pip install '.[pfo-server]'\n# ZEUS_PFO_SCHEDULER_ARGS='{\"solution_path\": \"path/to/freqs_pipeline_%05d.py\"}' uvicorn zeus.optimizer.pipeline_frequency.server.router:app --port 7787\n</code></pre> <p>When you run training (with the same <code>run.sh</code> but without <code>--profile true</code>), the <code>PipelineFrequencyOptimizer</code> integrated into your training framework will automatically talk with the PFO server to figure out the right GPU frequency to set for the upcoming pipeline instruction and transparently set the GPU's frequency.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#integrating-with-training-frameworks","title":"Integrating with training frameworks","text":"<p>This page aims to walk you through the process of integrating the pipeline frequency optimizer with arbitrary training frameworks. We also have a reference integration with Merak. Especially take a look at <code>Merak.runtime.pipe_engine</code>.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#assumptions","title":"Assumptions","text":"<p>We assume that there are concrete regions of the framework's code where the forward pass and the backward pass exclusively happens. For instance, in DeepSpeed, <code>PipelineEngine</code> has <code>_exec_forward_pass</code> and <code>_exec_backward_pass</code>. As another example, in Megatron-LM, users can pass in their custom <code>forward_step_func</code> to <code>pretrain</code>, and <code>forward_step</code> in the codebase calls it. The backward pass is done (roughly) in the <code>backward_step</code> function.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#integrate-pipelinefrequencyoptimizer","title":"Integrate <code>PipelineFrequencyOptimizer</code>","text":"<ol> <li>Add <code>zeus-ml[pfo]</code> to your dependencies.</li> <li>Instantiate the <code>PipelineFrequencyOptimizer</code> somewhere before actual training runs. Let's call the object <code>opt</code>.</li> <li>Surround one training step with <code>opt.on_step_begin()</code> and <code>opt.on_step_end()</code>.</li> <li>Wrap the forward pass region with <code>opt.on_instruction_begin(\"forward\")</code> and <code>opt.on_instruction_end(\"forward\")</code>.</li> <li>Wrap the backward pass region with <code>opt.on_instruction_begin(\"backward\")</code> and <code>opt.on_instruction_end(\"backward\")</code>.</li> </ol>"},{"location":"optimize/pipeline_frequency_optimizer/#profiling-instructions","title":"Profiling Instructions","text":"<p>It's important to optimize on top of accurate measurements of forward and backward instructions. For now, we're taking an offline approach, where we run each instruction under a given GPU frequency N times and average time and energy consumption. See Merak's <code>profile</code> function.</p> <p>We're on the process of implementing an online approach that is directly integrated into <code>PipelineFrequencyOptimizer</code> so that you don't need to implement a separate profiler inside your framework.</p>"},{"location":"optimize/power_limit_optimizer/","title":"Power Limit Optimizer","text":"<p>The power limit optimizer (<code>GlobalPowerLimitOptimizer</code>) finds the optimal GPU power limit for DNN training. Users can customize the power limit optimizer to choose the optimal power limit based on their own criteria using the <code>OptimumSelector</code> interface.</p>"},{"location":"optimize/power_limit_optimizer/#usage","title":"Usage","text":"<p>Use cases currently supported are single GPU training and data parallel training. For data parallel training, the power limit of all GPUs involved are changed together, since all GPUs have the same computation load.</p> <p>Upcoming</p> <p>Distributed data parallel training support is planned (tracking issue).</p> <p>Extra system privileges needed</p> <p>In order to optimize the GPU power limit, the power limit optimizer should be able to change the power limit. This requires extra system privileges. See here for details.</p>"},{"location":"optimize/power_limit_optimizer/#globalpowerlimitoptimizer","title":"<code>GlobalPowerLimitOptimizer</code>","text":"<p>You can use the power limit optimizer by integrating <code>GlobalPowerLimitOptimizer</code> into your training loop. In order to inform the optimizer of epoch and training step boundaries, a couple methods need to be called inside the training loop (highlighted):</p> <pre><code>from zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.power_limit import GlobalPowerLimitOptimizer\n\n# Data parallel training with four GPUs.\nmonitor = ZeusMonitor(gpu_indices=[0,1,2,3])\nplo = GlobalPowerLimitOptimizer(monitor)\n\nfor epoch in range(100):\n    plo.on_epoch_begin()\n\n    for x, y in train_dataloader:\n        plo.on_step_begin()\n        # Learn from x and y\n        plo.on_step_end()\n\n    plo.on_epoch_end()\n</code></pre> <p>We provide integration examples for Torchvision &amp; ImageNet single-GPU and data parallel training.</p> <p>What is the optimal power limit?</p> <p><code>GlobalPowerLimitOptimizer</code> accepts an optional <code>OptimumSelector</code> in its constructor, which defines how to choose one power limit among all the profiled power limits. Built-in optimum selectors are <code>Energy</code>, <code>Time</code>, <code>ZeusCost</code> and <code>MaxSlowdownConstraint</code>. Users can inherit from <code>OptimumSelector</code> to implement their custom optimum selector.</p>"},{"location":"optimize/power_limit_optimizer/#hfglobalpowerlimitoptimizer","title":"<code>HFGlobalPowerLimitOptimizer</code>","text":"<p>For easy use with HuggingFace Transformers, <code>HFGlobalPowerLimitOptimizer</code> is implemented as a HuggingFace Trainer Callback by inheriting from <code>TrainerCallback</code>. When initializing a HuggingFace Trainer or a TFL SFTTrainer, initialize and pass in <code>HFGlobalPowerLimitOptimizer</code> as shown below:</p> <pre><code>from transformers import Trainer\nfrom zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.power_limit import HFGlobalPowerLimitOptimizer\n\nmonitor = ZeusMonitor()\nplo = HFGlobalPowerLimitOptimizer(monitor)\n\n# Also works with trl.SFTTrainer.\ntrainer = Trainer(\n    ...,\n    callbacks=[plo],\n)\n</code></pre> <p>Refer to our HuggingFace integration examples for:</p> <ul> <li>Transformers <code>Trainer</code> integration for causal language modeling (i.e., pre-training)</li> <li>TRL <code>SFTTrainer</code> integration for Gemma 7B supervised fine-tuning with QLoRA</li> </ul>"},{"location":"reference/","title":"Source Code Reference","text":""},{"location":"reference/#zeus","title":"zeus","text":"<p>Zeus is a framework for deep learning energy measurement and optimization.</p> <ul> <li><code>optimizer</code>: A collection of optimizers for time and energy</li> <li><code>monitor</code>: Programmatic power and energy measurement tools</li> <li><code>utils</code>: Utility functions and classes.</li> <li><code>_legacy</code>: Legacy code mostly to keep our papers reproducible</li> <li><code>device</code>: Abstraction layer over compute devices</li> <li><code>callback</code>: Callback definition</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Source Code Reference<ul> <li>_legacy<ul> <li>job</li> <li>policy<ul> <li>interface</li> <li>mab</li> <li>optimizer</li> </ul> </li> <li>simulate</li> </ul> </li> <li>callback</li> <li>device<ul> <li>exception</li> <li>gpu<ul> <li>amd</li> <li>common</li> <li>nvidia</li> </ul> </li> </ul> </li> <li>exception</li> <li>monitor<ul> <li>energy</li> <li>power</li> </ul> </li> <li>optimizer<ul> <li>batch_size<ul> <li>client</li> <li>common</li> <li>exceptions</li> <li>server<ul> <li>batch_size_state<ul> <li>commands</li> <li>models</li> <li>repository</li> </ul> </li> <li>config</li> <li>database<ul> <li>db_connection</li> <li>repository</li> <li>schema</li> </ul> </li> <li>exceptions</li> <li>explorer</li> <li>job<ul> <li>commands</li> <li>models</li> <li>repository</li> </ul> </li> <li>mab</li> <li>optimizer</li> <li>router</li> <li>services<ul> <li>commands</li> <li>service</li> </ul> </li> </ul> </li> </ul> </li> <li>pipeline_frequency<ul> <li>common</li> <li>frequency_controller</li> <li>optimizer</li> <li>server<ul> <li>job_manager</li> <li>router</li> <li>scheduler</li> </ul> </li> </ul> </li> <li>power_limit</li> </ul> </li> <li>utils<ul> <li>async_utils</li> <li>env</li> <li>framework</li> <li>logging</li> <li>lr_scaler</li> <li>metric</li> <li>pydantic_v1</li> <li>testing</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/callback/","title":"callback","text":""},{"location":"reference/callback/#zeus.callback","title":"zeus.callback","text":"<p>Infrastructure for calling callbacks.</p>"},{"location":"reference/callback/#zeus.callback.Callback","title":"Callback","text":"<p>Base class for callbacks.</p> Source code in <code>zeus/callback.py</code> <pre><code>class Callback:\n    \"\"\"Base class for callbacks.\"\"\"\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of training.\"\"\"\n\n    def on_train_end(self) -&gt; None:\n        \"\"\"Called at the end of training.\"\"\"\n\n    def on_epoch_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Called at the end of each epoch.\"\"\"\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each step.\"\"\"\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Called at the end of each step.\"\"\"\n\n    def on_evaluate(self, metric: float) -&gt; None:\n        \"\"\"Called after evaluating the model.\"\"\"\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Called at the beginning of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of training.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end()\n</code></pre> <p>Called at the end of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_end(self) -&gt; None:\n    \"\"\"Called at the end of training.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_epoch_begin","title":"on_epoch_begin","text":"<pre><code>on_epoch_begin()\n</code></pre> <p>Called at the beginning of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Called at the end of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Called at the end of each epoch.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Called at the beginning of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each step.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Called at the end of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Called at the end of each step.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Called after evaluating the model.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_evaluate(self, metric: float) -&gt; None:\n    \"\"\"Called after evaluating the model.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Called at the beginning of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Called at the end of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet","title":"CallbackSet","text":"<p>               Bases: <code>Callback</code></p> <p>A set of callbacks.</p> Source code in <code>zeus/callback.py</code> <pre><code>class CallbackSet(Callback):\n    \"\"\"A set of callbacks.\"\"\"\n\n    def __init__(self, callbacks: list[Callback]) -&gt; None:\n        \"\"\"Initialize the callback set.\"\"\"\n        self.callbacks = callbacks\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of training.\"\"\"\n        for callback in self.callbacks:\n            callback.on_train_begin()\n\n    def on_train_end(self) -&gt; None:\n        \"\"\"Called at the end of training.\"\"\"\n        for callback in self.callbacks:\n            callback.on_train_end()\n\n    def on_epoch_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        for callback in self.callbacks:\n            callback.on_epoch_begin()\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Called at the end of each epoch.\"\"\"\n        for callback in self.callbacks:\n            callback.on_epoch_end()\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each step.\"\"\"\n        for callback in self.callbacks:\n            callback.on_step_begin()\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Called at the end of each step.\"\"\"\n        for callback in self.callbacks:\n            callback.on_step_end()\n\n    def on_evaluate(self, metric: float) -&gt; None:\n        \"\"\"Called after evaluating the model.\"\"\"\n        for callback in self.callbacks:\n            callback.on_evaluate(metric)\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n        for callback in self.callbacks:\n            callback.on_instruction_begin(name)\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n        for callback in self.callbacks:\n            callback.on_instruction_end(name)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.__init__","title":"__init__","text":"<pre><code>__init__(callbacks)\n</code></pre> Source code in <code>zeus/callback.py</code> <pre><code>def __init__(self, callbacks: list[Callback]) -&gt; None:\n    \"\"\"Initialize the callback set.\"\"\"\n    self.callbacks = callbacks\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Called at the beginning of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of training.\"\"\"\n    for callback in self.callbacks:\n        callback.on_train_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end()\n</code></pre> <p>Called at the end of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_end(self) -&gt; None:\n    \"\"\"Called at the end of training.\"\"\"\n    for callback in self.callbacks:\n        callback.on_train_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_epoch_begin","title":"on_epoch_begin","text":"<pre><code>on_epoch_begin()\n</code></pre> <p>Called at the beginning of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n    for callback in self.callbacks:\n        callback.on_epoch_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Called at the end of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Called at the end of each epoch.\"\"\"\n    for callback in self.callbacks:\n        callback.on_epoch_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Called at the beginning of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each step.\"\"\"\n    for callback in self.callbacks:\n        callback.on_step_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Called at the end of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Called at the end of each step.\"\"\"\n    for callback in self.callbacks:\n        callback.on_step_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Called after evaluating the model.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_evaluate(self, metric: float) -&gt; None:\n    \"\"\"Called after evaluating the model.\"\"\"\n    for callback in self.callbacks:\n        callback.on_evaluate(metric)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Called at the beginning of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n    for callback in self.callbacks:\n        callback.on_instruction_begin(name)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Called at the end of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n    for callback in self.callbacks:\n        callback.on_instruction_end(name)\n</code></pre>"},{"location":"reference/exception/","title":"exception","text":""},{"location":"reference/exception/#zeus.exception","title":"zeus.exception","text":"<p>Base Zeus Exception Class.</p>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError","title":"ZeusBaseError","text":"<p>               Bases: <code>Exception</code></p> <p>Zeus base exception class.</p> Source code in <code>zeus/exception.py</code> <pre><code>class ZeusBaseError(Exception):\n    \"\"\"Zeus base exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        self.message = message\n        super().__init__(message)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return message.\"\"\"\n        return self.message\n</code></pre>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return message.</p> Source code in <code>zeus/exception.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return message.\"\"\"\n    return self.message\n</code></pre>"},{"location":"reference/_legacy/","title":"_legacy","text":""},{"location":"reference/_legacy/#zeus._legacy","title":"zeus._legacy","text":"<p>Zeus legacy batch size optimizer.</p> <p>In order to reproduce the paper's result, use this legacy code. To actually use the batch size optimizer, use the <code>zeus.optimizer.batch_size</code></p>"},{"location":"reference/_legacy/job/","title":"job","text":""},{"location":"reference/_legacy/job/#zeus._legacy.job","title":"zeus._legacy.job","text":"<p>Defines the Job specification dataclass.</p>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job","title":"Job  <code>dataclass</code>","text":"<p>Job specification tuple.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>str</code> <p>Name of the dataset.</p> <code>network</code> <code>str</code> <p>Name of the DNN model.</p> <code>optimizer</code> <code>str</code> <p>Name of the optimizer, e.g. Adam.</p> <code>target_metric</code> <code>float</code> <p>Target validation metric.</p> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to train before terminating.</p> <code>default_bs</code> <code>int | None</code> <p>Initial batch size (b0) provided by the user.</p> <code>default_lr</code> <code>float | None</code> <p>Learning rate corresponding to the default batch size.</p> <code>workdir</code> <code>str | None</code> <p>Working directory in which to launch the job command.</p> <code>command</code> <code>list[str] | None</code> <p>Job command template. See <code>gen_command</code>.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>@dataclass(frozen=True, unsafe_hash=True)\nclass Job:\n    \"\"\"Job specification tuple.\n\n    Attributes:\n        dataset: Name of the dataset.\n        network: Name of the DNN model.\n        optimizer: Name of the optimizer, e.g. Adam.\n        target_metric: Target validation metric.\n        max_epochs: Maximum number of epochs to train before terminating.\n        default_bs: Initial batch size (b0) provided by the user.\n        default_lr: Learning rate corresponding to the default batch size.\n        workdir: Working directory in which to launch the job command.\n        command: Job command template. See [`gen_command`][zeus._legacy.job.Job.gen_command].\n    \"\"\"\n\n    dataset: str\n    network: str\n    optimizer: str\n    target_metric: float\n    max_epochs: int\n    default_bs: int | None = None\n    default_lr: float | None = None\n    workdir: str | None = None\n    command: list[str] | None = field(default=None, hash=False, compare=False)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Generate a more conside representation of the object.\"\"\"\n        return (\n            f\"Job({self.dataset},{self.network},{self.optimizer},{self.target_metric}\"\n            f\"{f',bs{self.default_bs}' if self.default_bs is not None else ''}~{self.max_epochs})\"\n        )\n\n    def to_logdir(self) -&gt; str:\n        \"\"\"Generate a logdir name that explains this job.\"\"\"\n        return (\n            f\"{self.dataset}+{self.network}+bs{self.default_bs}\"\n            f\"+{self.optimizer}+lr{self.default_lr}\"\n            f\"+tm{self.target_metric}+me{self.max_epochs}\"\n        )\n\n    def filter_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Pick out the rows corresponding to this job from the DataFrame.\"\"\"\n        return df.loc[\n            (df.dataset == self.dataset)\n            &amp; (df.network == self.network)\n            &amp; (df.optimizer == self.optimizer)\n            &amp; (df.target_metric == self.target_metric)\n        ]\n\n    def gen_command(\n        self,\n        batch_size: int,\n        learning_rate: float,\n        seed: int,\n        rec_i: int,\n    ) -&gt; list[str]:\n        \"\"\"Format the job command with given arguments.\n\n        Args:\n            batch_size: Batch size to use for this job launch.\n            learning_rate: Learning rate to use for this job launch.\n            seed: Random seed to use for this job launch.\n            rec_i: Recurrence number of this job launch.\n        \"\"\"\n        assert self.command, \"You must provide a command format string for this job.\"\n        command = []\n        for piece in self.command:\n            if piece in [\"{bs}\", \"{batch_size}\"]:\n                command.append(str(batch_size))\n            elif piece in [\"{lr}\", \"{learning_rate}\"]:\n                command.append(str(learning_rate))\n            elif piece == \"{seed}\":\n                command.append(str(seed))\n            elif piece in [\"{epoch}\", \"{epochs}\"]:\n                command.append(str(self.max_epochs))\n            elif piece == \"{slice_number}\":\n                command.append(str(rec_i))\n            elif piece == \"{target_metric}\":\n                command.append(str(self.target_metric))\n            else:\n                command.append(piece)\n        return command\n\n    def scale_lr(self, batch_size: int) -&gt; float:\n        \"\"\"Scale the learning rate for the given batch size.\n\n        Assumes that `self.default_bs` and `self.default_lr` were given.\n        Then, `self.default_lr` is scaled for the given `batch_size` using\n        square root scaling for adaptive optimizers (e.g. Adam, Adadelta,\n        AdamW) and linear scaling for others (e.g. SGD).\n        \"\"\"\n        assert self.default_bs, \"You must provide default_bs to scale LR.\"\n        assert self.default_lr, \"You must provide default_lr to scale LR.\"\n\n        optimizer = self.optimizer.lower()\n        if optimizer in [\"adam\", \"adadelta\", \"adamw\"]:\n            scaler = SquareRootScaler(bs=self.default_bs, lr=self.default_lr)\n            return scaler.compute_lr(batch_size)\n        if optimizer in [\"sgd\"]:\n            scaler = LinearScaler(bs=self.default_bs, lr=self.default_lr)\n            return scaler.compute_lr(batch_size)\n        raise NotImplementedError(f\"LR scaling for {self.optimizer} is not supported.\")\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Generate a more conside representation of the object.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Generate a more conside representation of the object.\"\"\"\n    return (\n        f\"Job({self.dataset},{self.network},{self.optimizer},{self.target_metric}\"\n        f\"{f',bs{self.default_bs}' if self.default_bs is not None else ''}~{self.max_epochs})\"\n    )\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.to_logdir","title":"to_logdir","text":"<pre><code>to_logdir()\n</code></pre> <p>Generate a logdir name that explains this job.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def to_logdir(self) -&gt; str:\n    \"\"\"Generate a logdir name that explains this job.\"\"\"\n    return (\n        f\"{self.dataset}+{self.network}+bs{self.default_bs}\"\n        f\"+{self.optimizer}+lr{self.default_lr}\"\n        f\"+tm{self.target_metric}+me{self.max_epochs}\"\n    )\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.filter_df","title":"filter_df","text":"<pre><code>filter_df(df)\n</code></pre> <p>Pick out the rows corresponding to this job from the DataFrame.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def filter_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Pick out the rows corresponding to this job from the DataFrame.\"\"\"\n    return df.loc[\n        (df.dataset == self.dataset)\n        &amp; (df.network == self.network)\n        &amp; (df.optimizer == self.optimizer)\n        &amp; (df.target_metric == self.target_metric)\n    ]\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.gen_command","title":"gen_command","text":"<pre><code>gen_command(batch_size, learning_rate, seed, rec_i)\n</code></pre> <p>Format the job command with given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size to use for this job launch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate to use for this job launch.</p> required <code>seed</code> <code>int</code> <p>Random seed to use for this job launch.</p> required <code>rec_i</code> <code>int</code> <p>Recurrence number of this job launch.</p> required Source code in <code>zeus/_legacy/job.py</code> <pre><code>def gen_command(\n    self,\n    batch_size: int,\n    learning_rate: float,\n    seed: int,\n    rec_i: int,\n) -&gt; list[str]:\n    \"\"\"Format the job command with given arguments.\n\n    Args:\n        batch_size: Batch size to use for this job launch.\n        learning_rate: Learning rate to use for this job launch.\n        seed: Random seed to use for this job launch.\n        rec_i: Recurrence number of this job launch.\n    \"\"\"\n    assert self.command, \"You must provide a command format string for this job.\"\n    command = []\n    for piece in self.command:\n        if piece in [\"{bs}\", \"{batch_size}\"]:\n            command.append(str(batch_size))\n        elif piece in [\"{lr}\", \"{learning_rate}\"]:\n            command.append(str(learning_rate))\n        elif piece == \"{seed}\":\n            command.append(str(seed))\n        elif piece in [\"{epoch}\", \"{epochs}\"]:\n            command.append(str(self.max_epochs))\n        elif piece == \"{slice_number}\":\n            command.append(str(rec_i))\n        elif piece == \"{target_metric}\":\n            command.append(str(self.target_metric))\n        else:\n            command.append(piece)\n    return command\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.scale_lr","title":"scale_lr","text":"<pre><code>scale_lr(batch_size)\n</code></pre> <p>Scale the learning rate for the given batch size.</p> <p>Assumes that <code>self.default_bs</code> and <code>self.default_lr</code> were given. Then, <code>self.default_lr</code> is scaled for the given <code>batch_size</code> using square root scaling for adaptive optimizers (e.g. Adam, Adadelta, AdamW) and linear scaling for others (e.g. SGD).</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def scale_lr(self, batch_size: int) -&gt; float:\n    \"\"\"Scale the learning rate for the given batch size.\n\n    Assumes that `self.default_bs` and `self.default_lr` were given.\n    Then, `self.default_lr` is scaled for the given `batch_size` using\n    square root scaling for adaptive optimizers (e.g. Adam, Adadelta,\n    AdamW) and linear scaling for others (e.g. SGD).\n    \"\"\"\n    assert self.default_bs, \"You must provide default_bs to scale LR.\"\n    assert self.default_lr, \"You must provide default_lr to scale LR.\"\n\n    optimizer = self.optimizer.lower()\n    if optimizer in [\"adam\", \"adadelta\", \"adamw\"]:\n        scaler = SquareRootScaler(bs=self.default_bs, lr=self.default_lr)\n        return scaler.compute_lr(batch_size)\n    if optimizer in [\"sgd\"]:\n        scaler = LinearScaler(bs=self.default_bs, lr=self.default_lr)\n        return scaler.compute_lr(batch_size)\n    raise NotImplementedError(f\"LR scaling for {self.optimizer} is not supported.\")\n</code></pre>"},{"location":"reference/_legacy/simulate/","title":"simulate","text":""},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate","title":"zeus._legacy.simulate","text":"<p>A simulator for running trace-driven Zeus experiments.</p>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.HistoryEntry","title":"HistoryEntry  <code>dataclass</code>","text":"<p>Represents the config and result of a job run that may have failed.</p> <p>Attributes:</p> Name Type Description <code>bs</code> <code>int</code> <p>Batch size</p> <code>pl</code> <code>int</code> <p>Power limit</p> <code>energy</code> <code>float</code> <p>Energy consumption in Joules</p> <code>reached</code> <code>bool</code> <p>Whether the target metric was reached at the end</p> <code>time</code> <code>float</code> <p>Time consumption in seconds</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>@dataclass\nclass HistoryEntry:\n    \"\"\"Represents the config and result of a job run that may have failed.\n\n    Attributes:\n        bs: Batch size\n        pl: Power limit\n        energy: Energy consumption in Joules\n        reached: Whether the target metric was reached at the end\n        time: Time consumption in seconds\n    \"\"\"\n\n    bs: int\n    pl: int\n    energy: float\n    reached: bool\n    time: float\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator","title":"Simulator","text":"<p>Simulates job execution optimized by Zeus.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>class Simulator:\n    \"\"\"Simulates job execution optimized by Zeus.\"\"\"\n\n    def __init__(\n        self,\n        summary_train: str | pd.DataFrame,\n        summary_power: str | pd.DataFrame,\n        batch_size_optimizer: BatchSizeOptimizer,\n        power_limit_optimizer: PowerLimitOptimizer,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the simulator.\n\n        Args:\n            summary_train: Path to or `pd.DataFrame` of the train trace.\n            summary_power: Path to or `pd.DataFrame` of the power trace.\n            batch_size_optimizer: The user is expected to construct\n                the BSO with the desired policy and pass it into the simulator.\n            power_limit_optimizer: The user is expected to construct\n                the PLO with the desired policy and pass it into the simulator.\n            seed: The random seed. Every invocation of any simulation method in this\n                class is deterministic given the random seed, because the internal RNG is\n                deepcopied before running the simulation.\n            verbose: Whether to log out the internal states of the simulator.\n        \"\"\"\n        # Generate relevant data.\n        train_df = (\n            pd.read_csv(summary_train)\n            if isinstance(summary_train, str)\n            else summary_train\n        )\n        power_df = (\n            pd.read_csv(summary_power)\n            if isinstance(summary_power, str)\n            else summary_power\n        )\n        df = train_df.merge(power_df, how=\"inner\")  # type: ignore\n        df[\"TTA\"] = df.target_epoch * df.time_per_epoch\n        df[\"ETA\"] = df.TTA * df.average_power\n        # 'energy_per_epoch' is used to compare different power limits with the same batch size\n        # when trying to figure out which power limit is the best one.\n        df[\"energy_per_epoch\"] = df.time_per_epoch * df.average_power\n        self.df = df\n\n        # Knob optimizers.\n        self.bso = batch_size_optimizer\n        self.plo = power_limit_optimizer\n\n        # Save arguments.\n        self.seed = seed\n        self.verbose = verbose\n\n    def simulate_one_job(\n        self,\n        job: Job,\n        num_recurrence: int,\n        beta_knob: float,\n        eta_knob: float,\n    ) -&gt; list[HistoryEntry]:\n        r\"\"\"Simulate a sequentially recurring job. Explore with early stopping.\n\n        Args:\n            job: Job spec to simulate.\n            num_recurrence: How many times the job recurs.\n            beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n                Set to `np.inf` to disable early stopping.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n        Returns:\n            A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n        \"\"\"\n        # Copy all internal state so that simulation does not modify any\n        # internal state and is deterministic w.r.t. the random seed.\n        bso = deepcopy(self.bso)\n        plo = deepcopy(self.plo)\n        rng = np.random.default_rng(self.seed)\n\n        # Figure out MAXPOWER.\n        max_power = self.df.power_limit.max().item()\n        if self.verbose:\n            print(f\"[Simulator] Max power = {max_power}W\")\n\n        # Track the minimum cost observed for the early stopping energy threshold.\n        min_cost = np.inf\n\n        # Simulate each job one at a time.\n        history: list[HistoryEntry] = []\n\n        if self.verbose:\n            print(f\"[Simulator] {job} recurring {num_recurrence} times.\")\n\n        # A new job. Profile the feasible batch size range.\n        batch_sizes = self._profile_batch_size_range(job)\n\n        # Register the job in the BSO.\n        bso.register_job(job, batch_sizes)\n\n        # Job recurs.\n        for i in range(num_recurrence):\n            if self.verbose:\n                print(f\"\\nRecurrence: {i+1}\")\n\n            # Run the job until convergence. Upper bound the number of retries to 20.\n            # Accumulate the cost of retries before convergence.\n            cost_acc = 0.0\n            for tries in range(1, 21):\n                # Whether this run of the job needed to profile power.\n                profiled_power = False\n\n                # Fetch knobs to use.\n                bs = bso.predict(job)\n                pl = plo.predict(job, bs)\n\n                # When the batch size is first explored, we need to profile power limit.\n                if pl is None:\n                    profiled_power = True\n                    result = self._profile_power_limit(job, bs, eta_knob)\n                    for pl, epe in result.items():\n                        plo.observe(job, bs, pl, epe)\n                    pl = plo.predict(job, bs)\n                    assert pl\n\n                # Run the job, potentially early stopping it.\n                eta, tta, reached = self._run_job(\n                    job=job,\n                    batch_size=bs,\n                    power_limit=pl,\n                    rng=rng,\n                    cost_ub=beta_knob * min_cost,\n                    eta_knob=eta_knob,\n                    profile_power=profiled_power,\n                )\n\n                # The job never ran because even one epoch exceeds the cost threshold.\n                # Let the BSO observe that this batch size is bad, but since the job\n                # did not run, do not add to the history and retry.\n                if eta == 0 and tta == 0 and not reached:\n                    bso.observe(job, bs, 100 * beta_knob * min_cost, False)\n                    continue\n\n                # Compute the hybrid cost metric.\n                cost = zeus_cost(eta, tta, eta_knob, max_power)\n                cost_acc += cost\n\n                # Provide feedback to the BSO.\n                bso.observe(job, bs, cost, reached)\n\n                # Record history for analysis.\n                history.append(HistoryEntry(bs, pl, eta, reached, tta))\n\n                # Reached the target metric. Update min_cost and go on to the next recurrence.\n                if reached:\n                    if self.verbose:\n                        print()\n                        print(\n                            f\"[Simulator] Reached target metric in {tries} {'try' if tries == 1 else 'tries'}.\"\n                        )\n                    if min_cost &gt; cost_acc:\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {cost_acc:.2f}.\"\n                            )\n                        min_cost = cost_acc\n                    break\n                # Didn't reach the target metric.\n                # We assume that the default BS (set by the user) will always converge.\n                # That is, reaching the target metric with the model should be a feasible task.\n                if i == 0:\n                    raise RuntimeError(\n                        f\"The default batch size {job.default_bs} did not converge.\"\n                    )\n\n            # Target metric was not reached in 20 tries. We consider this target metric to be unreachable.\n            else:\n                raise RuntimeError(\"Job did not reach the target metric in 20 tries.\")\n\n        if self.verbose:\n            print()\n            print(\n                f\"[Simulator] {job} (BS, PL, ETA, whether_reached, TTA) history: \\n{history}\"\n            )\n\n        return history\n\n    def simulate_one_alibaba_group(\n        self,\n        job: Job,\n        group_df: pd.DataFrame,\n        beta_knob: float,\n        eta_knob: float,\n    ) -&gt; list[HistoryEntry]:\n        r\"\"\"Run simulation on one group in the Alibaba trace.\n\n        Concurrent job submissions (jobs that start before the previous job\n        finishes) are launched with the batch size known to be of minimum\n        cost at that time. The BSO also observes the results of these jobs\n        when they are done, and these jobs may well finish before a job that\n        started before it. See `observe` in PruningGTSBatchSizeOptimizer for\n        an example of handing such a scenario.\n\n        Args:\n            job: Job spec of this group.\n            group_df: DataFrame of this group. Fields:\n                - group: Group ID in trace. Identical across all rows.\n                - dataset: Dataset name. Identical across all rows.\n                - start_time: Job start time in the trace.\n                - end_time: Job end time in the trace.\n                - runtime_ratio: runtime of this job over the mean runtime\n                    of all the jobs of this dataset. Captures intra-dataset\n                    job runtime differences.\n            beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n                Set to `np.inf` to disable early stopping.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n        Returns:\n            A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n        \"\"\"\n        # Copy all internal state so that simulation does not modify any\n        # internal state and is deterministic w.r.t. the random seed.\n        bso = deepcopy(self.bso)\n        plo = deepcopy(self.plo)\n        rng = np.random.default_rng(self.seed)\n\n        # Sanity check\n        if job.default_bs is None:\n            raise ValueError(\"You must give the job a default batch size.\")\n\n        # Figure out MAXPOWER.\n        max_power = self.df.power_limit.max().item()\n        if self.verbose:\n            print(f\"[Simulator] Max power = {max_power}W\")\n\n        # Track the minimum cost observed for the early stopping energy threshold.\n        # Also track the corresponding minimum cost BS to handle concurrent jobs.\n        min_cost = np.inf\n        best_bs = job.default_bs\n\n        # Simulate each job one at a time.\n        history: list[HistoryEntry] = []\n\n        if self.verbose:\n            print(f\"[Simulator] {job} recurring {len(group_df)} times.\")\n\n        # A new job. Profile the feasible batch size range.\n        batch_sizes = self._profile_batch_size_range(job)\n\n        # Register the job in the BSO.\n        bso.register_job(job, batch_sizes)\n\n        # List of jobs in flight.\n        @dataclass\n        class RunningJob:\n            \"\"\"Represents a job that is running.\n\n            We know what's going to happen to this job when we launch it.\n            Thus, pre-compute all results (using self.run_job) and have this\n            instance hold the information. Then, jobs will be removed from the\n            list of running jobs when the current time passes self.end_time and\n            the result will be observed.\n            \"\"\"\n\n            start_time: float\n            end_time: float\n            runtime_ratio: float\n            batch_size: int\n            power_limit: int\n            reached: bool\n            time: float\n            energy: float\n            cost: float\n\n        running_jobs: list[RunningJob] = []\n\n        # Jobs in group_df are already sorted by start_time.\n        current_time = 0.0\n        for rec_i, (_, job_row) in enumerate(group_df.iterrows()):\n            if self.verbose:\n                print(f\"\\nRecurrence: {rec_i+1}\")\n\n            # Update the current time.\n            current_time = job_row.start_time\n            if self.verbose:\n                print(f\"[Simulator] Current time is {current_time:.1f}\")\n\n            # Scan the in-flight job list to reap jobs that have finished.\n            # We need a while loop here because we might have submitted a retry job\n            # while reaping jobs that failed to reach the target metric, and that retry job\n            # may finish before the current job.\n            while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n                if self.verbose:\n                    print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n                # We copy running_jobs because we want to mutate it as we traverse it.\n                running_jobs_copy = deepcopy(running_jobs)\n\n                # Sort the jobs in the order they end.\n                for rjob in sorted(\n                    running_jobs_copy, key=operator.attrgetter(\"end_time\")\n                ):\n                    # We're only interested in jobs that finished at this point in time.\n                    if rjob.end_time &gt; current_time:\n                        continue\n\n                    # Job is finished at this point in time.\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                            f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                        )\n\n                    # Remove the job from the in-flight job list.\n                    running_jobs.remove(rjob)\n\n                    # Let the BSO observe the cost for this batch size.\n                    bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                    # If the job never ran because even one epoch exceeds the cost threshold,\n                    # do not add to the history and retry.\n                    if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                        # Record history for analysis.\n                        history.append(\n                            HistoryEntry(\n                                rjob.batch_size,\n                                rjob.power_limit,\n                                rjob.energy\n                                * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                                rjob.reached,\n                                rjob.time\n                                * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                            )\n                        )\n\n                    # Reached target metric (no need to retry)\n                    if rjob.reached:\n                        if min_cost &gt; rjob.cost:\n                            if self.verbose:\n                                print(\n                                    f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\"\n                                )\n                            min_cost = rjob.cost\n                            best_bs = rjob.batch_size\n\n                    # Didn't reach target metric. Need to run a retry job.\n                    else:\n                        profiled_power = False\n\n                        # If there are jobs in-flight, we just run additional concurrent\n                        # submissions with the best known knobs.\n                        if running_jobs:\n                            if self.verbose:\n                                print(\n                                    f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\"\n                                )\n                            bs = best_bs\n                            pl = plo.predict(job, bs)\n                            assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                        # There are no jobs in-flight. Just submit the job normally.\n                        else:\n                            # Determine the knobs.\n                            bs = bso.predict(job)\n                            pl = plo.predict(job, bs)\n\n                            if self.verbose:\n                                print(\n                                    f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                                )\n\n                            # When the batch size is first explored, we need to profile power limit.\n                            if pl is None:\n                                profiled_power = True\n                                result = self._profile_power_limit(job, bs, eta_knob)\n                                for pl, epe in result.items():\n                                    plo.observe(job, bs, pl, epe)\n                                pl = plo.predict(job, bs)\n                                assert pl\n\n                        # Pre-compute the result of the job.\n                        eta, tta, reached = self._run_job(\n                            job=job,\n                            batch_size=bs,\n                            power_limit=pl,\n                            rng=rng,\n                            cost_ub=beta_knob * min_cost,\n                            eta_knob=eta_knob,\n                            profile_power=profiled_power,\n                        )\n\n                        # Compute the hybrid cost metric.\n                        cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                        # Create the RunningJob instance.\n                        running_job = RunningJob(\n                            start_time=rjob.end_time,\n                            end_time=rjob.end_time\n                            + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                            runtime_ratio=rjob.runtime_ratio,\n                            batch_size=bs,\n                            power_limit=pl,\n                            reached=reached,\n                            time=tta,\n                            energy=eta,\n                            cost=cost,\n                        )\n                        running_jobs.append(running_job)\n\n                        if self.verbose:\n                            print(f\"[Simulator] Started retry job {running_job}\")\n\n                        # We must break from the loop that scans the running_jobs list, because\n                        # the job we just submitted might actually be the next job that finishes.\n                        break\n\n            # We're (finally) done reaping all finished jobs. Run the current job.\n            profiled_power = False\n\n            # If there are jobs in-flight, we just run additional concurrent\n            # submissions with the best known knobs.\n            if running_jobs:\n                if self.verbose:\n                    print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                bs = best_bs\n                pl = plo.predict(job, bs)\n                assert pl is not None, f\"Power not profiled for best known BS {bs}\"\n\n            # There are no jobs in-flight. Just submit the job.\n            else:\n                # Determine the knobs.\n                bs = bso.predict(job)\n                pl = plo.predict(job, bs)\n\n                if self.verbose:\n                    print(\n                        f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                    )\n\n                # When the batch size is first explored, we need to profile power limit.\n                if pl is None:\n                    profiled_power = True\n                    result = self._profile_power_limit(job, bs, eta_knob)\n                    for pl, epe in result.items():\n                        plo.observe(job, bs, pl, epe)\n                    pl = plo.predict(job, bs)\n                    assert pl\n\n            # Run the job, potentially early stopping it.\n            eta, tta, reached = self._run_job(\n                job=job,\n                batch_size=bs,\n                power_limit=pl,\n                rng=rng,\n                cost_ub=beta_knob * min_cost,\n                eta_knob=eta_knob,\n                profile_power=profiled_power,\n            )\n\n            # Compute the hybrid cost metric.\n            cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n            # Create the RunningJob instance and enqueue.\n            running_job = RunningJob(\n                start_time=job_row.start_time,\n                end_time=job_row.end_time,\n                runtime_ratio=job_row.runtime_ratio,\n                batch_size=bs,\n                power_limit=pl,\n                reached=reached,\n                time=tta,\n                energy=eta,\n                cost=cost,\n            )\n            running_jobs.append(running_job)\n\n            if self.verbose:\n                print(f\"[Simulator] Started job {running_job}\")\n\n        # Now, we're done iterating the rows of group_df.\n        # Set the current time to infinity so that all running jobs finish.\n        current_time = np.inf\n        if self.verbose:\n            print(\"\\n[Simulator] Reap all jobs\")\n\n        # Scan the remaining in-flight job list to reap jobs that have finished.\n        # Since current_time is infinity, this while loop will reap all the jobs ever launched.\n        while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n            if self.verbose:\n                print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n            # We copy running_jobs because we want to mutate it as we traverse it.\n            running_jobs_copy = deepcopy(running_jobs)\n\n            # Sort the jobs in the order they end.\n            for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n                # We're only interested in jobs that finished at this point in time.\n                if rjob.end_time &gt; current_time:\n                    continue\n\n                # Job is finished at this point in time.\n                if self.verbose:\n                    print(\n                        f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                        f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                    )\n\n                # Remove the job from the in-flight job list.\n                running_jobs.remove(rjob)\n\n                # Let the BSO observe the cost for this batch size.\n                bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                # If the job never ran because even one epoch exceeds the cost threshold,\n                # do not add to the history and retry.\n                if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                    # Record history for analysis.\n                    history.append(\n                        HistoryEntry(\n                            rjob.batch_size,\n                            rjob.power_limit,\n                            rjob.energy\n                            * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                            rjob.reached,\n                            rjob.time\n                            * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                        )\n                    )\n\n                # Reached target metric (no need to retry)\n                if rjob.reached:\n                    if min_cost &gt; rjob.cost:\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\"\n                            )\n                        min_cost = rjob.cost\n                        best_bs = rjob.batch_size\n\n                # Didin't reach target metric. Need to run a retry job.\n                else:\n                    profiled_power = False\n\n                    # If there are jobs in-flight, we just run additional concurrent\n                    # submissions with the best known knobs.\n                    if running_jobs:\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\"\n                            )\n                        bs = best_bs\n                        pl = plo.predict(job, bs)\n                        assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                    # There are no jobs in-flight. Just submit the job normally.\n                    else:\n                        # Determine the knobs.\n                        bs = bso.predict(job)\n                        pl = plo.predict(job, bs)\n\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                            )\n\n                        # When the batch size is first explored, we need to profile power limit.\n                        if pl is None:\n                            profiled_power = True\n                            result = self._profile_power_limit(job, bs, eta_knob)\n                            for pl, epe in result.items():\n                                plo.observe(job, bs, pl, epe)\n                            pl = plo.predict(job, bs)\n                            assert pl\n\n                    # Pre-compute the result of the job.\n                    eta, tta, reached = self._run_job(\n                        job=job,\n                        batch_size=bs,\n                        power_limit=pl,\n                        rng=rng,\n                        cost_ub=beta_knob * min_cost,\n                        eta_knob=eta_knob,\n                        profile_power=profiled_power,\n                    )\n\n                    # Compute the hybrid cost metric.\n                    cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                    # Create the RunningJob instance.\n                    running_job = RunningJob(\n                        start_time=rjob.end_time,\n                        end_time=rjob.end_time\n                        + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                        runtime_ratio=rjob.runtime_ratio,\n                        batch_size=bs,\n                        power_limit=pl,\n                        reached=reached,\n                        time=tta,\n                        energy=eta,\n                        cost=cost,\n                    )\n                    running_jobs.append(running_job)\n\n                    if self.verbose:\n                        print(f\"[Simulator] Started retry job {running_job}\")\n\n                    # We must break from the loop that scans the running_jobs list, because\n                    # the job we just submitted might actually be the next job that finishes.\n                    break\n\n        return history\n\n    def _run_job(\n        self,\n        job: Job,\n        batch_size: int,\n        power_limit: int,\n        rng: np.random.Generator,\n        cost_ub: float,\n        eta_knob: float,\n        profile_power: bool,\n    ) -&gt; tuple[float, float, bool]:\n        r\"\"\"Simulate running the job and return the energy consumed and whether it converged.\n\n        This method will randomly choose one of the possible training \"paths\". Then,\n        based on cost_ub, it will compute the maximum number of epochs the job can run.\n        If the path's target_epoch is smaller than or equal to the maximum number of\n        epochs, the cost incurred until target_epoch is returned. Otherwise, the cost\n        incurred until the maximum number of epochs is returned.\n\n        It is important to note that the job may never run when the first epoch's cost\n        is already expected to exceed the cost upper bound. In such a case, the returned\n        time and energy consumptions will be zero. This case must be treated separately\n        in the calling code.\n\n        If profile_power is True, the first epoch will JIT-profile power limits coarsely\n        by dividing the epoch evenly into len(available_power_limits) pieces. Thus the\n        the first epoch's energy and time consumption will be slightly adjusted.\n\n        Args:\n            job: Job spec to run.\n            batch_size: Batch size to use.\n            power_limit: Power limit to use. Regardless of whether this run of this\n                batch size requires power profiling, the simulator will input the optimal\n                power limit for the batch size. The first epoch energy consumption from\n                power profiling is adjusted in the latter half of this method based on the\n                profile_power flag.\n            rng: Random number generator used to sample one training path.\n            cost_ub: Cost upper bound. The job is terminated when the next epoch is going\n                to exceed the cost upper bound.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n            profile_power: Whether this run of the job should profile power during the\n                first epoch.\n\n        Returns:\n            Tuple of energy consumption, time consumption, and whether the job reached the target metric.\n        \"\"\"\n        # df is filtered with job spec, BS, and PL. We sample one possible training path.\n        # power_df is filtered with job spec and BS. We use this to compute the energy\n        # consumption of profiling power during the first epoch.\n        df = job.filter_df(self.df)\n        power_df = df.loc[df.batch_size == batch_size]\n        df = power_df.loc[df.power_limit == power_limit]\n        path = df.sample(n=1, random_state=rng)\n\n        # Max number of epochs is bound by either the cost upper bound or the user-specified\n        # max_epochs, whichever is smaller.\n        if cost_ub == np.inf:\n            # cost_ub is infinity in two cases:\n            # 1. The simulator has never observed any cost value in the early part of simulation.\n            # 2. We're simulating with no early stopping, i.e. beta_knob is infinity.\n            max_epochs = job.max_epochs\n            if self.verbose:\n                print(f\"[run job] Cost UB is inf. {max_epochs=}\")\n        else:\n            # Stop right before the first epoch when cost will cross the upper bound.\n            cost_per_epoch = (\n                eta_knob * path.energy_per_epoch.item()\n                + (1 - eta_knob)\n                * power_df.power_limit.max().item()\n                * path.time_per_epoch.item()\n            )\n            max_epochs = min(cost_ub // cost_per_epoch, job.max_epochs)\n            if self.verbose:\n                print(f\"[run job] {cost_ub=}\")\n                print(f\"[run job] {cost_per_epoch=}\")\n                print(f\"[run job] {max_epochs=}\")\n\n        # The job virtually never ran. Time and Energy being zero will be treated specially outside.\n        # If the min_cost is so low, this might even prevent this BS from running at all.\n        if max_epochs == 0:\n            print(\n                f\"[run job] {job} cannot run even one epoch without exceeding the cost UB.\"\n                f\" BS {batch_size}, PL {power_limit}, {eta_knob=}\"\n            )\n            return 0.0, 0.0, False\n\n        def compute_energy_and_time(\n            num_epochs: int, profile_power: bool\n        ) -&gt; tuple[float, float]:\n            \"\"\"Compute the energy and time consumed for running the job for num_epochs.\"\"\"\n            # This is the first run of this batch size, and we need to profile power\n            # during the first epoch.\n            if profile_power:\n                # Note that power_df holds rows with all power limits. Evenly splitting the\n                # epochs with the number of samples and running each slice with each power\n                # limit consumes (1/N) * e_100 + (1/N) * e_125 + ... + (1/N) * e_250.\n                # Also there are all runs 1, 2, ... included, but power info is actually\n                # completely duplicated across different runs in the DataFrame.\n                # Thus, taking the mean across the entire power_df gets us what we want.\n                energy_first_epoch = power_df.energy_per_epoch.mean().item()  # type: ignore\n                energy_from_second_epoch = path.energy_per_epoch.item() * (\n                    num_epochs - 1\n                )\n                energy_consumption = energy_first_epoch + energy_from_second_epoch\n                time_first_epoch = power_df.time_per_epoch.mean().item()  # type: ignore\n                time_from_second_epoch = path.time_per_epoch.item() * (num_epochs - 1)\n                time_consumption = time_first_epoch + time_from_second_epoch\n            # Just run num_epochs with the given power limit. Simple.\n            else:\n                energy_consumption = path.energy_per_epoch.item() * num_epochs\n                time_consumption = path.time_per_epoch.item() * num_epochs\n            return energy_consumption, time_consumption\n\n        # Job reached target metric.\n        target_epoch = path.target_epoch.item()\n        if path.target_epoch.notnull().item() and target_epoch &lt;= max_epochs:\n            eta, tta = compute_energy_and_time(target_epoch, profile_power)\n            if self.verbose:\n                print(\n                    f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                    f\"=&gt; \\033[31mReached in {int(target_epoch)} epochs, \"\n                    f\"TTA {tta:.2f} seconds, ETA {eta:.2f}\\033[0m\"\n                )\n            return eta, tta, True\n\n        # Job failed to reach the target metric.\n        energy_consumption, time_consumption = compute_energy_and_time(\n            max_epochs, profile_power\n        )\n        if self.verbose:\n            print(\n                f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                f\"=&gt; \\033[31mFailed (stopped after {int(max_epochs)} epochs), \"\n                f\"TTA {time_consumption:.2f} seconds, ETA {energy_consumption:.2f}\\033[0m\"\n            )\n        return energy_consumption, time_consumption, False\n\n    def _profile_power_limit(\n        self, job: Job, batch_size: int, eta_knob: float\n    ) -&gt; dict[int, float]:\n        \"\"\"Simulate running the job and profiling the power limit.\n\n        Returns:\n            Dictionary mapping PL to `energy_per_epoch`. PL is inserted in high to low order.\n        \"\"\"\n        # Filter by job spec and BS.\n        df = job.filter_df(self.df)\n        df = df.loc[(df.batch_size == batch_size)]\n\n        # Compute the epoch cost of each power limit (Equation 7).\n        max_pl = df.power_limit.max().item()\n        df = df.groupby([\"power_limit\"], as_index=False).mean(numeric_only=True)\n        df[\"epoch_cost\"] = (\n            eta_knob * df[\"average_power\"] + (1 - eta_knob) * max_pl\n        ) * df[\"time_per_epoch\"]\n\n        # We'll be profiling energy from larger to smaller power limits.\n        df = df.sort_values(by=\"power_limit\", ascending=False)\n        result = {rec.power_limit: rec.epoch_cost for rec in df.to_records(index=False)}\n        if self.verbose:\n            print(f\"[PL profile] {job} @ {batch_size} =&gt; PL = {min(result, key=result.get)}W\")  # type: ignore\n        return result\n\n    def _profile_batch_size_range(self, job: Job) -&gt; list[int]:\n        \"\"\"Simulate profiling the available batch size range of the job.\n\n        Returns:\n            A list of feasible batch sizes.\n        \"\"\"\n        df = self.df\n        # Do not filter by target_metric here since we do not want to constrain\n        # the feasible batch size range to only those that reached the target metric.\n        df = df.loc[\n            (df.dataset == job.dataset)\n            &amp; (df.network == job.network)\n            &amp; (df.optimizer == job.optimizer)\n        ]\n        result = sorted(list(df.batch_size.unique()))\n        if self.verbose:\n            print(f\"[BS profile] {job} =&gt; BS = {result}\")\n        return result\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.__init__","title":"__init__","text":"<pre><code>__init__(\n    summary_train,\n    summary_power,\n    batch_size_optimizer,\n    power_limit_optimizer,\n    seed=123456,\n    verbose=True,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>summary_train</code> <code>str | DataFrame</code> <p>Path to or <code>pd.DataFrame</code> of the train trace.</p> required <code>summary_power</code> <code>str | DataFrame</code> <p>Path to or <code>pd.DataFrame</code> of the power trace.</p> required <code>batch_size_optimizer</code> <code>BatchSizeOptimizer</code> <p>The user is expected to construct the BSO with the desired policy and pass it into the simulator.</p> required <code>power_limit_optimizer</code> <code>PowerLimitOptimizer</code> <p>The user is expected to construct the PLO with the desired policy and pass it into the simulator.</p> required <code>seed</code> <code>int</code> <p>The random seed. Every invocation of any simulation method in this class is deterministic given the random seed, because the internal RNG is deepcopied before running the simulation.</p> <code>123456</code> <code>verbose</code> <code>bool</code> <p>Whether to log out the internal states of the simulator.</p> <code>True</code> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def __init__(\n    self,\n    summary_train: str | pd.DataFrame,\n    summary_power: str | pd.DataFrame,\n    batch_size_optimizer: BatchSizeOptimizer,\n    power_limit_optimizer: PowerLimitOptimizer,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the simulator.\n\n    Args:\n        summary_train: Path to or `pd.DataFrame` of the train trace.\n        summary_power: Path to or `pd.DataFrame` of the power trace.\n        batch_size_optimizer: The user is expected to construct\n            the BSO with the desired policy and pass it into the simulator.\n        power_limit_optimizer: The user is expected to construct\n            the PLO with the desired policy and pass it into the simulator.\n        seed: The random seed. Every invocation of any simulation method in this\n            class is deterministic given the random seed, because the internal RNG is\n            deepcopied before running the simulation.\n        verbose: Whether to log out the internal states of the simulator.\n    \"\"\"\n    # Generate relevant data.\n    train_df = (\n        pd.read_csv(summary_train)\n        if isinstance(summary_train, str)\n        else summary_train\n    )\n    power_df = (\n        pd.read_csv(summary_power)\n        if isinstance(summary_power, str)\n        else summary_power\n    )\n    df = train_df.merge(power_df, how=\"inner\")  # type: ignore\n    df[\"TTA\"] = df.target_epoch * df.time_per_epoch\n    df[\"ETA\"] = df.TTA * df.average_power\n    # 'energy_per_epoch' is used to compare different power limits with the same batch size\n    # when trying to figure out which power limit is the best one.\n    df[\"energy_per_epoch\"] = df.time_per_epoch * df.average_power\n    self.df = df\n\n    # Knob optimizers.\n    self.bso = batch_size_optimizer\n    self.plo = power_limit_optimizer\n\n    # Save arguments.\n    self.seed = seed\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.simulate_one_job","title":"simulate_one_job","text":"<pre><code>simulate_one_job(job, num_recurrence, beta_knob, eta_knob)\n</code></pre> <p>Simulate a sequentially recurring job. Explore with early stopping.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec to simulate.</p> required <code>num_recurrence</code> <code>int</code> <p>How many times the job recurs.</p> required <code>beta_knob</code> <code>float</code> <p><code>beta_knob * min_eta</code> is the early stopping cost threshold. Set to <code>np.inf</code> to disable early stopping.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <p>Returns:</p> Type Description <code>list[HistoryEntry]</code> <p>A list of <code>HistoryEntry</code> objects for each job run.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def simulate_one_job(\n    self,\n    job: Job,\n    num_recurrence: int,\n    beta_knob: float,\n    eta_knob: float,\n) -&gt; list[HistoryEntry]:\n    r\"\"\"Simulate a sequentially recurring job. Explore with early stopping.\n\n    Args:\n        job: Job spec to simulate.\n        num_recurrence: How many times the job recurs.\n        beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n            Set to `np.inf` to disable early stopping.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n    Returns:\n        A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n    \"\"\"\n    # Copy all internal state so that simulation does not modify any\n    # internal state and is deterministic w.r.t. the random seed.\n    bso = deepcopy(self.bso)\n    plo = deepcopy(self.plo)\n    rng = np.random.default_rng(self.seed)\n\n    # Figure out MAXPOWER.\n    max_power = self.df.power_limit.max().item()\n    if self.verbose:\n        print(f\"[Simulator] Max power = {max_power}W\")\n\n    # Track the minimum cost observed for the early stopping energy threshold.\n    min_cost = np.inf\n\n    # Simulate each job one at a time.\n    history: list[HistoryEntry] = []\n\n    if self.verbose:\n        print(f\"[Simulator] {job} recurring {num_recurrence} times.\")\n\n    # A new job. Profile the feasible batch size range.\n    batch_sizes = self._profile_batch_size_range(job)\n\n    # Register the job in the BSO.\n    bso.register_job(job, batch_sizes)\n\n    # Job recurs.\n    for i in range(num_recurrence):\n        if self.verbose:\n            print(f\"\\nRecurrence: {i+1}\")\n\n        # Run the job until convergence. Upper bound the number of retries to 20.\n        # Accumulate the cost of retries before convergence.\n        cost_acc = 0.0\n        for tries in range(1, 21):\n            # Whether this run of the job needed to profile power.\n            profiled_power = False\n\n            # Fetch knobs to use.\n            bs = bso.predict(job)\n            pl = plo.predict(job, bs)\n\n            # When the batch size is first explored, we need to profile power limit.\n            if pl is None:\n                profiled_power = True\n                result = self._profile_power_limit(job, bs, eta_knob)\n                for pl, epe in result.items():\n                    plo.observe(job, bs, pl, epe)\n                pl = plo.predict(job, bs)\n                assert pl\n\n            # Run the job, potentially early stopping it.\n            eta, tta, reached = self._run_job(\n                job=job,\n                batch_size=bs,\n                power_limit=pl,\n                rng=rng,\n                cost_ub=beta_knob * min_cost,\n                eta_knob=eta_knob,\n                profile_power=profiled_power,\n            )\n\n            # The job never ran because even one epoch exceeds the cost threshold.\n            # Let the BSO observe that this batch size is bad, but since the job\n            # did not run, do not add to the history and retry.\n            if eta == 0 and tta == 0 and not reached:\n                bso.observe(job, bs, 100 * beta_knob * min_cost, False)\n                continue\n\n            # Compute the hybrid cost metric.\n            cost = zeus_cost(eta, tta, eta_knob, max_power)\n            cost_acc += cost\n\n            # Provide feedback to the BSO.\n            bso.observe(job, bs, cost, reached)\n\n            # Record history for analysis.\n            history.append(HistoryEntry(bs, pl, eta, reached, tta))\n\n            # Reached the target metric. Update min_cost and go on to the next recurrence.\n            if reached:\n                if self.verbose:\n                    print()\n                    print(\n                        f\"[Simulator] Reached target metric in {tries} {'try' if tries == 1 else 'tries'}.\"\n                    )\n                if min_cost &gt; cost_acc:\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {cost_acc:.2f}.\"\n                        )\n                    min_cost = cost_acc\n                break\n            # Didn't reach the target metric.\n            # We assume that the default BS (set by the user) will always converge.\n            # That is, reaching the target metric with the model should be a feasible task.\n            if i == 0:\n                raise RuntimeError(\n                    f\"The default batch size {job.default_bs} did not converge.\"\n                )\n\n        # Target metric was not reached in 20 tries. We consider this target metric to be unreachable.\n        else:\n            raise RuntimeError(\"Job did not reach the target metric in 20 tries.\")\n\n    if self.verbose:\n        print()\n        print(\n            f\"[Simulator] {job} (BS, PL, ETA, whether_reached, TTA) history: \\n{history}\"\n        )\n\n    return history\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.simulate_one_alibaba_group","title":"simulate_one_alibaba_group","text":"<pre><code>simulate_one_alibaba_group(\n    job, group_df, beta_knob, eta_knob\n)\n</code></pre> <p>Run simulation on one group in the Alibaba trace.</p> <p>Concurrent job submissions (jobs that start before the previous job finishes) are launched with the batch size known to be of minimum cost at that time. The BSO also observes the results of these jobs when they are done, and these jobs may well finish before a job that started before it. See <code>observe</code> in PruningGTSBatchSizeOptimizer for an example of handing such a scenario.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec of this group.</p> required <code>group_df</code> <code>DataFrame</code> <p>DataFrame of this group. Fields: - group: Group ID in trace. Identical across all rows. - dataset: Dataset name. Identical across all rows. - start_time: Job start time in the trace. - end_time: Job end time in the trace. - runtime_ratio: runtime of this job over the mean runtime     of all the jobs of this dataset. Captures intra-dataset     job runtime differences.</p> required <code>beta_knob</code> <code>float</code> <p><code>beta_knob * min_eta</code> is the early stopping cost threshold. Set to <code>np.inf</code> to disable early stopping.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <p>Returns:</p> Type Description <code>list[HistoryEntry]</code> <p>A list of <code>HistoryEntry</code> objects for each job run.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def simulate_one_alibaba_group(\n    self,\n    job: Job,\n    group_df: pd.DataFrame,\n    beta_knob: float,\n    eta_knob: float,\n) -&gt; list[HistoryEntry]:\n    r\"\"\"Run simulation on one group in the Alibaba trace.\n\n    Concurrent job submissions (jobs that start before the previous job\n    finishes) are launched with the batch size known to be of minimum\n    cost at that time. The BSO also observes the results of these jobs\n    when they are done, and these jobs may well finish before a job that\n    started before it. See `observe` in PruningGTSBatchSizeOptimizer for\n    an example of handing such a scenario.\n\n    Args:\n        job: Job spec of this group.\n        group_df: DataFrame of this group. Fields:\n            - group: Group ID in trace. Identical across all rows.\n            - dataset: Dataset name. Identical across all rows.\n            - start_time: Job start time in the trace.\n            - end_time: Job end time in the trace.\n            - runtime_ratio: runtime of this job over the mean runtime\n                of all the jobs of this dataset. Captures intra-dataset\n                job runtime differences.\n        beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n            Set to `np.inf` to disable early stopping.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n    Returns:\n        A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n    \"\"\"\n    # Copy all internal state so that simulation does not modify any\n    # internal state and is deterministic w.r.t. the random seed.\n    bso = deepcopy(self.bso)\n    plo = deepcopy(self.plo)\n    rng = np.random.default_rng(self.seed)\n\n    # Sanity check\n    if job.default_bs is None:\n        raise ValueError(\"You must give the job a default batch size.\")\n\n    # Figure out MAXPOWER.\n    max_power = self.df.power_limit.max().item()\n    if self.verbose:\n        print(f\"[Simulator] Max power = {max_power}W\")\n\n    # Track the minimum cost observed for the early stopping energy threshold.\n    # Also track the corresponding minimum cost BS to handle concurrent jobs.\n    min_cost = np.inf\n    best_bs = job.default_bs\n\n    # Simulate each job one at a time.\n    history: list[HistoryEntry] = []\n\n    if self.verbose:\n        print(f\"[Simulator] {job} recurring {len(group_df)} times.\")\n\n    # A new job. Profile the feasible batch size range.\n    batch_sizes = self._profile_batch_size_range(job)\n\n    # Register the job in the BSO.\n    bso.register_job(job, batch_sizes)\n\n    # List of jobs in flight.\n    @dataclass\n    class RunningJob:\n        \"\"\"Represents a job that is running.\n\n        We know what's going to happen to this job when we launch it.\n        Thus, pre-compute all results (using self.run_job) and have this\n        instance hold the information. Then, jobs will be removed from the\n        list of running jobs when the current time passes self.end_time and\n        the result will be observed.\n        \"\"\"\n\n        start_time: float\n        end_time: float\n        runtime_ratio: float\n        batch_size: int\n        power_limit: int\n        reached: bool\n        time: float\n        energy: float\n        cost: float\n\n    running_jobs: list[RunningJob] = []\n\n    # Jobs in group_df are already sorted by start_time.\n    current_time = 0.0\n    for rec_i, (_, job_row) in enumerate(group_df.iterrows()):\n        if self.verbose:\n            print(f\"\\nRecurrence: {rec_i+1}\")\n\n        # Update the current time.\n        current_time = job_row.start_time\n        if self.verbose:\n            print(f\"[Simulator] Current time is {current_time:.1f}\")\n\n        # Scan the in-flight job list to reap jobs that have finished.\n        # We need a while loop here because we might have submitted a retry job\n        # while reaping jobs that failed to reach the target metric, and that retry job\n        # may finish before the current job.\n        while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n            if self.verbose:\n                print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n            # We copy running_jobs because we want to mutate it as we traverse it.\n            running_jobs_copy = deepcopy(running_jobs)\n\n            # Sort the jobs in the order they end.\n            for rjob in sorted(\n                running_jobs_copy, key=operator.attrgetter(\"end_time\")\n            ):\n                # We're only interested in jobs that finished at this point in time.\n                if rjob.end_time &gt; current_time:\n                    continue\n\n                # Job is finished at this point in time.\n                if self.verbose:\n                    print(\n                        f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                        f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                    )\n\n                # Remove the job from the in-flight job list.\n                running_jobs.remove(rjob)\n\n                # Let the BSO observe the cost for this batch size.\n                bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                # If the job never ran because even one epoch exceeds the cost threshold,\n                # do not add to the history and retry.\n                if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                    # Record history for analysis.\n                    history.append(\n                        HistoryEntry(\n                            rjob.batch_size,\n                            rjob.power_limit,\n                            rjob.energy\n                            * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                            rjob.reached,\n                            rjob.time\n                            * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                        )\n                    )\n\n                # Reached target metric (no need to retry)\n                if rjob.reached:\n                    if min_cost &gt; rjob.cost:\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\"\n                            )\n                        min_cost = rjob.cost\n                        best_bs = rjob.batch_size\n\n                # Didn't reach target metric. Need to run a retry job.\n                else:\n                    profiled_power = False\n\n                    # If there are jobs in-flight, we just run additional concurrent\n                    # submissions with the best known knobs.\n                    if running_jobs:\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\"\n                            )\n                        bs = best_bs\n                        pl = plo.predict(job, bs)\n                        assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                    # There are no jobs in-flight. Just submit the job normally.\n                    else:\n                        # Determine the knobs.\n                        bs = bso.predict(job)\n                        pl = plo.predict(job, bs)\n\n                        if self.verbose:\n                            print(\n                                f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                            )\n\n                        # When the batch size is first explored, we need to profile power limit.\n                        if pl is None:\n                            profiled_power = True\n                            result = self._profile_power_limit(job, bs, eta_knob)\n                            for pl, epe in result.items():\n                                plo.observe(job, bs, pl, epe)\n                            pl = plo.predict(job, bs)\n                            assert pl\n\n                    # Pre-compute the result of the job.\n                    eta, tta, reached = self._run_job(\n                        job=job,\n                        batch_size=bs,\n                        power_limit=pl,\n                        rng=rng,\n                        cost_ub=beta_knob * min_cost,\n                        eta_knob=eta_knob,\n                        profile_power=profiled_power,\n                    )\n\n                    # Compute the hybrid cost metric.\n                    cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                    # Create the RunningJob instance.\n                    running_job = RunningJob(\n                        start_time=rjob.end_time,\n                        end_time=rjob.end_time\n                        + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                        runtime_ratio=rjob.runtime_ratio,\n                        batch_size=bs,\n                        power_limit=pl,\n                        reached=reached,\n                        time=tta,\n                        energy=eta,\n                        cost=cost,\n                    )\n                    running_jobs.append(running_job)\n\n                    if self.verbose:\n                        print(f\"[Simulator] Started retry job {running_job}\")\n\n                    # We must break from the loop that scans the running_jobs list, because\n                    # the job we just submitted might actually be the next job that finishes.\n                    break\n\n        # We're (finally) done reaping all finished jobs. Run the current job.\n        profiled_power = False\n\n        # If there are jobs in-flight, we just run additional concurrent\n        # submissions with the best known knobs.\n        if running_jobs:\n            if self.verbose:\n                print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n            bs = best_bs\n            pl = plo.predict(job, bs)\n            assert pl is not None, f\"Power not profiled for best known BS {bs}\"\n\n        # There are no jobs in-flight. Just submit the job.\n        else:\n            # Determine the knobs.\n            bs = bso.predict(job)\n            pl = plo.predict(job, bs)\n\n            if self.verbose:\n                print(\n                    f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                )\n\n            # When the batch size is first explored, we need to profile power limit.\n            if pl is None:\n                profiled_power = True\n                result = self._profile_power_limit(job, bs, eta_knob)\n                for pl, epe in result.items():\n                    plo.observe(job, bs, pl, epe)\n                pl = plo.predict(job, bs)\n                assert pl\n\n        # Run the job, potentially early stopping it.\n        eta, tta, reached = self._run_job(\n            job=job,\n            batch_size=bs,\n            power_limit=pl,\n            rng=rng,\n            cost_ub=beta_knob * min_cost,\n            eta_knob=eta_knob,\n            profile_power=profiled_power,\n        )\n\n        # Compute the hybrid cost metric.\n        cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n        # Create the RunningJob instance and enqueue.\n        running_job = RunningJob(\n            start_time=job_row.start_time,\n            end_time=job_row.end_time,\n            runtime_ratio=job_row.runtime_ratio,\n            batch_size=bs,\n            power_limit=pl,\n            reached=reached,\n            time=tta,\n            energy=eta,\n            cost=cost,\n        )\n        running_jobs.append(running_job)\n\n        if self.verbose:\n            print(f\"[Simulator] Started job {running_job}\")\n\n    # Now, we're done iterating the rows of group_df.\n    # Set the current time to infinity so that all running jobs finish.\n    current_time = np.inf\n    if self.verbose:\n        print(\"\\n[Simulator] Reap all jobs\")\n\n    # Scan the remaining in-flight job list to reap jobs that have finished.\n    # Since current_time is infinity, this while loop will reap all the jobs ever launched.\n    while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n        if self.verbose:\n            print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n        # We copy running_jobs because we want to mutate it as we traverse it.\n        running_jobs_copy = deepcopy(running_jobs)\n\n        # Sort the jobs in the order they end.\n        for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n            # We're only interested in jobs that finished at this point in time.\n            if rjob.end_time &gt; current_time:\n                continue\n\n            # Job is finished at this point in time.\n            if self.verbose:\n                print(\n                    f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                    f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                )\n\n            # Remove the job from the in-flight job list.\n            running_jobs.remove(rjob)\n\n            # Let the BSO observe the cost for this batch size.\n            bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n            # If the job never ran because even one epoch exceeds the cost threshold,\n            # do not add to the history and retry.\n            if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                # Record history for analysis.\n                history.append(\n                    HistoryEntry(\n                        rjob.batch_size,\n                        rjob.power_limit,\n                        rjob.energy\n                        * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                        rjob.reached,\n                        rjob.time\n                        * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                    )\n                )\n\n            # Reached target metric (no need to retry)\n            if rjob.reached:\n                if min_cost &gt; rjob.cost:\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\"\n                        )\n                    min_cost = rjob.cost\n                    best_bs = rjob.batch_size\n\n            # Didin't reach target metric. Need to run a retry job.\n            else:\n                profiled_power = False\n\n                # If there are jobs in-flight, we just run additional concurrent\n                # submissions with the best known knobs.\n                if running_jobs:\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\"\n                        )\n                    bs = best_bs\n                    pl = plo.predict(job, bs)\n                    assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                # There are no jobs in-flight. Just submit the job normally.\n                else:\n                    # Determine the knobs.\n                    bs = bso.predict(job)\n                    pl = plo.predict(job, bs)\n\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\"\n                        )\n\n                    # When the batch size is first explored, we need to profile power limit.\n                    if pl is None:\n                        profiled_power = True\n                        result = self._profile_power_limit(job, bs, eta_knob)\n                        for pl, epe in result.items():\n                            plo.observe(job, bs, pl, epe)\n                        pl = plo.predict(job, bs)\n                        assert pl\n\n                # Pre-compute the result of the job.\n                eta, tta, reached = self._run_job(\n                    job=job,\n                    batch_size=bs,\n                    power_limit=pl,\n                    rng=rng,\n                    cost_ub=beta_knob * min_cost,\n                    eta_knob=eta_knob,\n                    profile_power=profiled_power,\n                )\n\n                # Compute the hybrid cost metric.\n                cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                # Create the RunningJob instance.\n                running_job = RunningJob(\n                    start_time=rjob.end_time,\n                    end_time=rjob.end_time\n                    + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                    runtime_ratio=rjob.runtime_ratio,\n                    batch_size=bs,\n                    power_limit=pl,\n                    reached=reached,\n                    time=tta,\n                    energy=eta,\n                    cost=cost,\n                )\n                running_jobs.append(running_job)\n\n                if self.verbose:\n                    print(f\"[Simulator] Started retry job {running_job}\")\n\n                # We must break from the loop that scans the running_jobs list, because\n                # the job we just submitted might actually be the next job that finishes.\n                break\n\n    return history\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._run_job","title":"_run_job","text":"<pre><code>_run_job(\n    job,\n    batch_size,\n    power_limit,\n    rng,\n    cost_ub,\n    eta_knob,\n    profile_power,\n)\n</code></pre> <p>Simulate running the job and return the energy consumed and whether it converged.</p> <p>This method will randomly choose one of the possible training \"paths\". Then, based on cost_ub, it will compute the maximum number of epochs the job can run. If the path's target_epoch is smaller than or equal to the maximum number of epochs, the cost incurred until target_epoch is returned. Otherwise, the cost incurred until the maximum number of epochs is returned.</p> <p>It is important to note that the job may never run when the first epoch's cost is already expected to exceed the cost upper bound. In such a case, the returned time and energy consumptions will be zero. This case must be treated separately in the calling code.</p> <p>If profile_power is True, the first epoch will JIT-profile power limits coarsely by dividing the epoch evenly into len(available_power_limits) pieces. Thus the the first epoch's energy and time consumption will be slightly adjusted.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec to run.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use.</p> required <code>power_limit</code> <code>int</code> <p>Power limit to use. Regardless of whether this run of this batch size requires power profiling, the simulator will input the optimal power limit for the batch size. The first epoch energy consumption from power profiling is adjusted in the latter half of this method based on the profile_power flag.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator used to sample one training path.</p> required <code>cost_ub</code> <code>float</code> <p>Cost upper bound. The job is terminated when the next epoch is going to exceed the cost upper bound.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <code>profile_power</code> <code>bool</code> <p>Whether this run of the job should profile power during the first epoch.</p> required <p>Returns:</p> Type Description <code>tuple[float, float, bool]</code> <p>Tuple of energy consumption, time consumption, and whether the job reached the target metric.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _run_job(\n    self,\n    job: Job,\n    batch_size: int,\n    power_limit: int,\n    rng: np.random.Generator,\n    cost_ub: float,\n    eta_knob: float,\n    profile_power: bool,\n) -&gt; tuple[float, float, bool]:\n    r\"\"\"Simulate running the job and return the energy consumed and whether it converged.\n\n    This method will randomly choose one of the possible training \"paths\". Then,\n    based on cost_ub, it will compute the maximum number of epochs the job can run.\n    If the path's target_epoch is smaller than or equal to the maximum number of\n    epochs, the cost incurred until target_epoch is returned. Otherwise, the cost\n    incurred until the maximum number of epochs is returned.\n\n    It is important to note that the job may never run when the first epoch's cost\n    is already expected to exceed the cost upper bound. In such a case, the returned\n    time and energy consumptions will be zero. This case must be treated separately\n    in the calling code.\n\n    If profile_power is True, the first epoch will JIT-profile power limits coarsely\n    by dividing the epoch evenly into len(available_power_limits) pieces. Thus the\n    the first epoch's energy and time consumption will be slightly adjusted.\n\n    Args:\n        job: Job spec to run.\n        batch_size: Batch size to use.\n        power_limit: Power limit to use. Regardless of whether this run of this\n            batch size requires power profiling, the simulator will input the optimal\n            power limit for the batch size. The first epoch energy consumption from\n            power profiling is adjusted in the latter half of this method based on the\n            profile_power flag.\n        rng: Random number generator used to sample one training path.\n        cost_ub: Cost upper bound. The job is terminated when the next epoch is going\n            to exceed the cost upper bound.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n        profile_power: Whether this run of the job should profile power during the\n            first epoch.\n\n    Returns:\n        Tuple of energy consumption, time consumption, and whether the job reached the target metric.\n    \"\"\"\n    # df is filtered with job spec, BS, and PL. We sample one possible training path.\n    # power_df is filtered with job spec and BS. We use this to compute the energy\n    # consumption of profiling power during the first epoch.\n    df = job.filter_df(self.df)\n    power_df = df.loc[df.batch_size == batch_size]\n    df = power_df.loc[df.power_limit == power_limit]\n    path = df.sample(n=1, random_state=rng)\n\n    # Max number of epochs is bound by either the cost upper bound or the user-specified\n    # max_epochs, whichever is smaller.\n    if cost_ub == np.inf:\n        # cost_ub is infinity in two cases:\n        # 1. The simulator has never observed any cost value in the early part of simulation.\n        # 2. We're simulating with no early stopping, i.e. beta_knob is infinity.\n        max_epochs = job.max_epochs\n        if self.verbose:\n            print(f\"[run job] Cost UB is inf. {max_epochs=}\")\n    else:\n        # Stop right before the first epoch when cost will cross the upper bound.\n        cost_per_epoch = (\n            eta_knob * path.energy_per_epoch.item()\n            + (1 - eta_knob)\n            * power_df.power_limit.max().item()\n            * path.time_per_epoch.item()\n        )\n        max_epochs = min(cost_ub // cost_per_epoch, job.max_epochs)\n        if self.verbose:\n            print(f\"[run job] {cost_ub=}\")\n            print(f\"[run job] {cost_per_epoch=}\")\n            print(f\"[run job] {max_epochs=}\")\n\n    # The job virtually never ran. Time and Energy being zero will be treated specially outside.\n    # If the min_cost is so low, this might even prevent this BS from running at all.\n    if max_epochs == 0:\n        print(\n            f\"[run job] {job} cannot run even one epoch without exceeding the cost UB.\"\n            f\" BS {batch_size}, PL {power_limit}, {eta_knob=}\"\n        )\n        return 0.0, 0.0, False\n\n    def compute_energy_and_time(\n        num_epochs: int, profile_power: bool\n    ) -&gt; tuple[float, float]:\n        \"\"\"Compute the energy and time consumed for running the job for num_epochs.\"\"\"\n        # This is the first run of this batch size, and we need to profile power\n        # during the first epoch.\n        if profile_power:\n            # Note that power_df holds rows with all power limits. Evenly splitting the\n            # epochs with the number of samples and running each slice with each power\n            # limit consumes (1/N) * e_100 + (1/N) * e_125 + ... + (1/N) * e_250.\n            # Also there are all runs 1, 2, ... included, but power info is actually\n            # completely duplicated across different runs in the DataFrame.\n            # Thus, taking the mean across the entire power_df gets us what we want.\n            energy_first_epoch = power_df.energy_per_epoch.mean().item()  # type: ignore\n            energy_from_second_epoch = path.energy_per_epoch.item() * (\n                num_epochs - 1\n            )\n            energy_consumption = energy_first_epoch + energy_from_second_epoch\n            time_first_epoch = power_df.time_per_epoch.mean().item()  # type: ignore\n            time_from_second_epoch = path.time_per_epoch.item() * (num_epochs - 1)\n            time_consumption = time_first_epoch + time_from_second_epoch\n        # Just run num_epochs with the given power limit. Simple.\n        else:\n            energy_consumption = path.energy_per_epoch.item() * num_epochs\n            time_consumption = path.time_per_epoch.item() * num_epochs\n        return energy_consumption, time_consumption\n\n    # Job reached target metric.\n    target_epoch = path.target_epoch.item()\n    if path.target_epoch.notnull().item() and target_epoch &lt;= max_epochs:\n        eta, tta = compute_energy_and_time(target_epoch, profile_power)\n        if self.verbose:\n            print(\n                f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                f\"=&gt; \\033[31mReached in {int(target_epoch)} epochs, \"\n                f\"TTA {tta:.2f} seconds, ETA {eta:.2f}\\033[0m\"\n            )\n        return eta, tta, True\n\n    # Job failed to reach the target metric.\n    energy_consumption, time_consumption = compute_energy_and_time(\n        max_epochs, profile_power\n    )\n    if self.verbose:\n        print(\n            f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n            f\"=&gt; \\033[31mFailed (stopped after {int(max_epochs)} epochs), \"\n            f\"TTA {time_consumption:.2f} seconds, ETA {energy_consumption:.2f}\\033[0m\"\n        )\n    return energy_consumption, time_consumption, False\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._profile_power_limit","title":"_profile_power_limit","text":"<pre><code>_profile_power_limit(job, batch_size, eta_knob)\n</code></pre> <p>Simulate running the job and profiling the power limit.</p> <p>Returns:</p> Type Description <code>dict[int, float]</code> <p>Dictionary mapping PL to <code>energy_per_epoch</code>. PL is inserted in high to low order.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _profile_power_limit(\n    self, job: Job, batch_size: int, eta_knob: float\n) -&gt; dict[int, float]:\n    \"\"\"Simulate running the job and profiling the power limit.\n\n    Returns:\n        Dictionary mapping PL to `energy_per_epoch`. PL is inserted in high to low order.\n    \"\"\"\n    # Filter by job spec and BS.\n    df = job.filter_df(self.df)\n    df = df.loc[(df.batch_size == batch_size)]\n\n    # Compute the epoch cost of each power limit (Equation 7).\n    max_pl = df.power_limit.max().item()\n    df = df.groupby([\"power_limit\"], as_index=False).mean(numeric_only=True)\n    df[\"epoch_cost\"] = (\n        eta_knob * df[\"average_power\"] + (1 - eta_knob) * max_pl\n    ) * df[\"time_per_epoch\"]\n\n    # We'll be profiling energy from larger to smaller power limits.\n    df = df.sort_values(by=\"power_limit\", ascending=False)\n    result = {rec.power_limit: rec.epoch_cost for rec in df.to_records(index=False)}\n    if self.verbose:\n        print(f\"[PL profile] {job} @ {batch_size} =&gt; PL = {min(result, key=result.get)}W\")  # type: ignore\n    return result\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._profile_batch_size_range","title":"_profile_batch_size_range","text":"<pre><code>_profile_batch_size_range(job)\n</code></pre> <p>Simulate profiling the available batch size range of the job.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of feasible batch sizes.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _profile_batch_size_range(self, job: Job) -&gt; list[int]:\n    \"\"\"Simulate profiling the available batch size range of the job.\n\n    Returns:\n        A list of feasible batch sizes.\n    \"\"\"\n    df = self.df\n    # Do not filter by target_metric here since we do not want to constrain\n    # the feasible batch size range to only those that reached the target metric.\n    df = df.loc[\n        (df.dataset == job.dataset)\n        &amp; (df.network == job.network)\n        &amp; (df.optimizer == job.optimizer)\n    ]\n    result = sorted(list(df.batch_size.unique()))\n    if self.verbose:\n        print(f\"[BS profile] {job} =&gt; BS = {result}\")\n    return result\n</code></pre>"},{"location":"reference/_legacy/policy/","title":"policy","text":""},{"location":"reference/_legacy/policy/#zeus._legacy.policy","title":"zeus._legacy.policy","text":"<p>Optimization policies for Zeus.</p> <p><code>PowerLimitOptimizer</code> and <code>BatchSizeOptimizer</code> are abstract classes. Users can implement custom policies by extending the abstract classes, implementing required methods, and plugging them into the <code>Simulator</code>.</p>"},{"location":"reference/_legacy/policy/interface/","title":"interface","text":""},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface","title":"zeus._legacy.policy.interface","text":"<p>Abstract classes for implementing custom optimization policies.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer","title":"BatchSizeOptimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Finds out the best batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>class BatchSizeOptimizer(ABC):\n    \"\"\"Finds out the best batch size to use for the job.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n\n    @abstractmethod\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Prepare internal state so that it can handle the given job.\n\n        It is assumed that the state of each [`Job`][zeus._legacy.job.Job] will be\n        managed separately. Note that [`Job`][zeus._legacy.job.Job] is hashable,\n        and thus can be used as dictionary keys.\n\n        Args:\n            job: New jobs to register.\n            batch_sizes: Batch sizes to consider.\n        \"\"\"\n\n    @abstractmethod\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the best batch size to use for the job.\n\n        Args:\n            job: The job to pick the best batch size for.\n        \"\"\"\n\n    @abstractmethod\n    def observe(\n        self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n    ) -&gt; None:\n        \"\"\"Observe the cost of using the given batch size for the job.\n\n        Args:\n            job: The job from which this cost observation resulted.\n            batch_size: The batch size used for this run of the job.\n            cost: The energy-time cost of running the job.\n            converged: Whether the job reached its target metric. If may not have\n                reached its target if the job was early stopped based on cost or\n                the maximum epoch was reached. For BSO's that do not take this into\n                account, `None` can be passed.\n        \"\"\"\n\n    def _log(self, message: str) -&gt; None:\n        \"\"\"Log message with object name.\"\"\"\n        print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.register_job","title":"register_job  <code>abstractmethod</code>","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Prepare internal state so that it can handle the given job.</p> <p>It is assumed that the state of each <code>Job</code> will be managed separately. Note that <code>Job</code> is hashable, and thus can be used as dictionary keys.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>New jobs to register.</p> required <code>batch_sizes</code> <code>list[int]</code> <p>Batch sizes to consider.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Prepare internal state so that it can handle the given job.\n\n    It is assumed that the state of each [`Job`][zeus._legacy.job.Job] will be\n    managed separately. Note that [`Job`][zeus._legacy.job.Job] is hashable,\n    and thus can be used as dictionary keys.\n\n    Args:\n        job: New jobs to register.\n        batch_sizes: Batch sizes to consider.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(job)\n</code></pre> <p>Return the best batch size to use for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job to pick the best batch size for.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef predict(self, job: Job) -&gt; int:\n    \"\"\"Return the best batch size to use for the job.\n\n    Args:\n        job: The job to pick the best batch size for.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.observe","title":"observe  <code>abstractmethod</code>","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Observe the cost of using the given batch size for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job from which this cost observation resulted.</p> required <code>batch_size</code> <code>int</code> <p>The batch size used for this run of the job.</p> required <code>cost</code> <code>float</code> <p>The energy-time cost of running the job.</p> required <code>converged</code> <code>bool | None</code> <p>Whether the job reached its target metric. If may not have reached its target if the job was early stopped based on cost or the maximum epoch was reached. For BSO's that do not take this into account, <code>None</code> can be passed.</p> <code>None</code> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef observe(\n    self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n) -&gt; None:\n    \"\"\"Observe the cost of using the given batch size for the job.\n\n    Args:\n        job: The job from which this cost observation resulted.\n        batch_size: The batch size used for this run of the job.\n        cost: The energy-time cost of running the job.\n        converged: Whether the job reached its target metric. If may not have\n            reached its target if the job was early stopped based on cost or\n            the maximum epoch was reached. For BSO's that do not take this into\n            account, `None` can be passed.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer._log","title":"_log","text":"<pre><code>_log(message)\n</code></pre> <p>Log message with object name.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>def _log(self, message: str) -&gt; None:\n    \"\"\"Log message with object name.\"\"\"\n    print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer","title":"PowerLimitOptimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Finds out the best power limit to use for the job and batch size.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>class PowerLimitOptimizer(ABC):\n    \"\"\"Finds out the best power limit to use for the job and batch size.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Name of the power limit optimizer.\"\"\"\n\n    @abstractmethod\n    def predict(self, job: Job, batch_size: int) -&gt; int | None:\n        \"\"\"Return the best power limit for the job and batch size.\n\n        Args:\n            job: The job to pick the best power limit for.\n            batch_size: The batch size chosen by the\n                [`BatchSizeOptimizer`][zeus._legacy.policy.BatchSizeOptimizer] for this job.\n\n        Returns:\n            The best power limit, or `None` if profiling results via\n            [`observe`][zeus._legacy.policy.interface.PowerLimitOptimizer.observe] are needed.\n        \"\"\"\n\n    @abstractmethod\n    def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n        \"\"\"Observe the cost of using the given batch size and power limit for the job.\n\n        Args:\n            job: The job from which this cost observation resulted.\n            batch_size: The batch size used for this run of the job.\n            power_limit: The power limit used for this run of the job.\n            cost: The cost of running the job.\n        \"\"\"\n\n    def _log(self, message: str) -&gt; None:\n        \"\"\"Log message with object name.\"\"\"\n        print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the power limit optimizer.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(job, batch_size)\n</code></pre> <p>Return the best power limit for the job and batch size.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job to pick the best power limit for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size chosen by the <code>BatchSizeOptimizer</code> for this job.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>The best power limit, or <code>None</code> if profiling results via</p> <code>int | None</code> <p><code>observe</code> are needed.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef predict(self, job: Job, batch_size: int) -&gt; int | None:\n    \"\"\"Return the best power limit for the job and batch size.\n\n    Args:\n        job: The job to pick the best power limit for.\n        batch_size: The batch size chosen by the\n            [`BatchSizeOptimizer`][zeus._legacy.policy.BatchSizeOptimizer] for this job.\n\n    Returns:\n        The best power limit, or `None` if profiling results via\n        [`observe`][zeus._legacy.policy.interface.PowerLimitOptimizer.observe] are needed.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.observe","title":"observe  <code>abstractmethod</code>","text":"<pre><code>observe(job, batch_size, power_limit, cost)\n</code></pre> <p>Observe the cost of using the given batch size and power limit for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job from which this cost observation resulted.</p> required <code>batch_size</code> <code>int</code> <p>The batch size used for this run of the job.</p> required <code>power_limit</code> <code>int</code> <p>The power limit used for this run of the job.</p> required <code>cost</code> <code>float</code> <p>The cost of running the job.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n    \"\"\"Observe the cost of using the given batch size and power limit for the job.\n\n    Args:\n        job: The job from which this cost observation resulted.\n        batch_size: The batch size used for this run of the job.\n        power_limit: The power limit used for this run of the job.\n        cost: The cost of running the job.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer._log","title":"_log","text":"<pre><code>_log(message)\n</code></pre> <p>Log message with object name.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>def _log(self, message: str) -&gt; None:\n    \"\"\"Log message with object name.\"\"\"\n    print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/mab/","title":"mab","text":""},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab","title":"zeus._legacy.policy.mab","text":"<p>Multi-Armed Bandit implementations.</p>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS","title":"GaussianTS","text":"<p>Thompson Sampling policy for Gaussian bandits.</p> <p>For each arm, the reward is modeled as a Gaussian distribution with known precision. The conjugate priors are also Gaussian distributions.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>class GaussianTS:\n    \"\"\"Thompson Sampling policy for Gaussian bandits.\n\n    For each arm, the reward is modeled as a Gaussian distribution with\n    known precision. The conjugate priors are also Gaussian distributions.\n    \"\"\"\n\n    def __init__(\n        self,\n        arms: list[int],\n        reward_precision: list[float] | float,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        num_exploration: int = 1,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the object.\n\n        Args:\n            arms: Bandit arm values to use.\n            reward_precision: Precision (inverse variance) of the reward distribution.\n                Pass in a list of `float`s to set the reward precision differently for\n                each arm.\n            prior_mean: Mean of the belief prior distribution.\n            prior_precision: Precision of the belief prior distribution.\n            num_exploration: How many static explorations to run when no observations\n                are available.\n            seed: The random seed to use.\n            verbose: Whether to print out what's going on.\n        \"\"\"\n        self.name = \"GaussianTS\"\n\n        self.arms = arms\n        self.prior_mean = prior_mean\n        self.prior_prec = prior_precision\n        self.num_exploration = num_exploration\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.verbose = verbose\n\n        # Set the precision of the reward distribution of each arm.\n        if isinstance(reward_precision, list):\n            self.arm_reward_prec = dict(zip(arms, reward_precision))\n        else:\n            self.arm_reward_prec = {arm: reward_precision for arm in arms}\n\n        # Initialze the parameter distribution with the prior parameters.\n        self.arm_param_mean = dict.fromkeys(arms, prior_mean)\n        self.arm_param_prec = dict.fromkeys(arms, prior_precision)\n\n        # Track how many times an arm reward has been observed.\n        self.arm_num_observations = dict.fromkeys(arms, 0)\n\n    def fit(\n        self,\n        decisions: list[int] | np.ndarray,\n        rewards: list[float] | np.ndarray,\n        reset: bool,\n    ) -&gt; None:\n        \"\"\"Fit the bandit on the given list of observations.\n\n        Args:\n            decisions: A list of arms chosen.\n            rewards: A list of rewards that resulted from choosing the arms in `decisions`.\n            reset: Whether to reset all arms.\n        \"\"\"\n        decisions_arr = np.array(decisions)\n        rewards_arr = np.array(rewards)\n\n        # Fit all arms.\n        for arm in self.arms:\n            self.fit_arm(arm, rewards_arr[decisions_arr == arm], reset)\n\n    def fit_arm(self, arm: int, rewards: np.ndarray, reset: bool) -&gt; None:\n        \"\"\"Update the parameter distribution for one arm.\n\n        Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n        Args:\n            arm: Arm to fit.\n            rewards: Array of rewards observed by pulling that arm.\n            reset: Whether to reset the parameters of the arm before fitting.\n        \"\"\"\n        if reset:\n            self.arm_param_mean[arm] = self.prior_mean\n            self.arm_param_prec[arm] = self.prior_prec\n            self.arm_num_observations[arm] = 0\n\n        if len(rewards) == 0:\n            return\n\n        # Read previous state.\n        reward_prec = self.arm_reward_prec[arm]\n        mean = self.arm_param_mean[arm]\n        prec = self.arm_param_prec[arm]\n\n        # Compute the parameters of the posterior distribution.\n        # The reward distribution's precision is given as infinite only when we\n        # have exactly one observation for the arm, s.t. sampling yields that\n        # exact observation.\n        if reward_prec == np.inf:\n            new_prec = np.inf\n            new_mean = rewards.mean()\n        else:\n            new_prec = prec + len(rewards) * reward_prec\n            new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n        # Update state.\n        self.arm_param_mean[arm] = new_mean\n        self.arm_param_prec[arm] = new_prec\n        self.arm_num_observations[arm] += len(rewards)\n\n    def predict(self) -&gt; int:\n        \"\"\"Return the arm with the largest sampled expected reward.\"\"\"\n        # Exploration-only phase.\n        # Order is random considering concurrent bandit scenarios.\n        arrms = np.array(self.arms)\n        for arm in self.rng.choice(arrms, len(arrms), replace=False):\n            if self.arm_num_observations[arm] &lt; self.num_exploration:\n                if self.verbose:\n                    print(f\"[{self.name}] Explore arm {arm}.\")\n                return arm\n\n        # Thomopson Sampling phase.\n        expectations = self.predict_expectations()\n        if self.verbose:\n            print(f\"[{self.name}] Sampled mean rewards:\")\n            for arm, sample in expectations.items():\n                print(\n                    f\"[{self.name}] Arm {arm:4d}: mu ~ N({self.arm_param_mean[arm]:.2f}, \"\n                    f\"{1/self.arm_param_prec[arm]:.2f}) -&gt; {sample:.2f}\"\n                )\n        return max(expectations, key=expectations.get)  # type: ignore\n\n    def predict_expectations(self) -&gt; dict[int, float]:\n        \"\"\"Sample the expected reward for each arm.\n\n        Assumes that each arm has been explored at least once. Otherwise,\n        a value will be sampled from the prior.\n\n        Returns:\n            A mapping from every arm to their sampled expected reward.\n        \"\"\"\n        expectations = {}\n        for arm in self.arms:\n            mean = self.arm_param_mean[arm]\n            prec = self.arm_param_prec[arm]\n            if prec == self.prior_prec:\n                warnings.warn(\n                    f\"predict_expectations called when arm '{arm}' is cold.\",\n                    stacklevel=1,\n                )\n            expectations[arm] = self.rng.normal(mean, np.sqrt(np.reciprocal(prec)))\n        return expectations\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.__init__","title":"__init__","text":"<pre><code>__init__(\n    arms,\n    reward_precision,\n    prior_mean=0.0,\n    prior_precision=0.0,\n    num_exploration=1,\n    seed=123456,\n    verbose=True,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>list[int]</code> <p>Bandit arm values to use.</p> required <code>reward_precision</code> <code>list[float] | float</code> <p>Precision (inverse variance) of the reward distribution. Pass in a list of <code>float</code>s to set the reward precision differently for each arm.</p> required <code>prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>0.0</code> <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>0.0</code> <code>num_exploration</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> <code>1</code> <code>seed</code> <code>int</code> <p>The random seed to use.</p> <code>123456</code> <code>verbose</code> <code>bool</code> <p>Whether to print out what's going on.</p> <code>True</code> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def __init__(\n    self,\n    arms: list[int],\n    reward_precision: list[float] | float,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    num_exploration: int = 1,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the object.\n\n    Args:\n        arms: Bandit arm values to use.\n        reward_precision: Precision (inverse variance) of the reward distribution.\n            Pass in a list of `float`s to set the reward precision differently for\n            each arm.\n        prior_mean: Mean of the belief prior distribution.\n        prior_precision: Precision of the belief prior distribution.\n        num_exploration: How many static explorations to run when no observations\n            are available.\n        seed: The random seed to use.\n        verbose: Whether to print out what's going on.\n    \"\"\"\n    self.name = \"GaussianTS\"\n\n    self.arms = arms\n    self.prior_mean = prior_mean\n    self.prior_prec = prior_precision\n    self.num_exploration = num_exploration\n    self.seed = seed\n    self.rng = np.random.default_rng(seed)\n    self.verbose = verbose\n\n    # Set the precision of the reward distribution of each arm.\n    if isinstance(reward_precision, list):\n        self.arm_reward_prec = dict(zip(arms, reward_precision))\n    else:\n        self.arm_reward_prec = {arm: reward_precision for arm in arms}\n\n    # Initialze the parameter distribution with the prior parameters.\n    self.arm_param_mean = dict.fromkeys(arms, prior_mean)\n    self.arm_param_prec = dict.fromkeys(arms, prior_precision)\n\n    # Track how many times an arm reward has been observed.\n    self.arm_num_observations = dict.fromkeys(arms, 0)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.fit","title":"fit","text":"<pre><code>fit(decisions, rewards, reset)\n</code></pre> <p>Fit the bandit on the given list of observations.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>list[int] | ndarray</code> <p>A list of arms chosen.</p> required <code>rewards</code> <code>list[float] | ndarray</code> <p>A list of rewards that resulted from choosing the arms in <code>decisions</code>.</p> required <code>reset</code> <code>bool</code> <p>Whether to reset all arms.</p> required Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def fit(\n    self,\n    decisions: list[int] | np.ndarray,\n    rewards: list[float] | np.ndarray,\n    reset: bool,\n) -&gt; None:\n    \"\"\"Fit the bandit on the given list of observations.\n\n    Args:\n        decisions: A list of arms chosen.\n        rewards: A list of rewards that resulted from choosing the arms in `decisions`.\n        reset: Whether to reset all arms.\n    \"\"\"\n    decisions_arr = np.array(decisions)\n    rewards_arr = np.array(rewards)\n\n    # Fit all arms.\n    for arm in self.arms:\n        self.fit_arm(arm, rewards_arr[decisions_arr == arm], reset)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.fit_arm","title":"fit_arm","text":"<pre><code>fit_arm(arm, rewards, reset)\n</code></pre> <p>Update the parameter distribution for one arm.</p> <p>Reference: https://en.wikipedia.org/wiki/Conjugate_prior</p> <p>Parameters:</p> Name Type Description Default <code>arm</code> <code>int</code> <p>Arm to fit.</p> required <code>rewards</code> <code>ndarray</code> <p>Array of rewards observed by pulling that arm.</p> required <code>reset</code> <code>bool</code> <p>Whether to reset the parameters of the arm before fitting.</p> required Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def fit_arm(self, arm: int, rewards: np.ndarray, reset: bool) -&gt; None:\n    \"\"\"Update the parameter distribution for one arm.\n\n    Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n    Args:\n        arm: Arm to fit.\n        rewards: Array of rewards observed by pulling that arm.\n        reset: Whether to reset the parameters of the arm before fitting.\n    \"\"\"\n    if reset:\n        self.arm_param_mean[arm] = self.prior_mean\n        self.arm_param_prec[arm] = self.prior_prec\n        self.arm_num_observations[arm] = 0\n\n    if len(rewards) == 0:\n        return\n\n    # Read previous state.\n    reward_prec = self.arm_reward_prec[arm]\n    mean = self.arm_param_mean[arm]\n    prec = self.arm_param_prec[arm]\n\n    # Compute the parameters of the posterior distribution.\n    # The reward distribution's precision is given as infinite only when we\n    # have exactly one observation for the arm, s.t. sampling yields that\n    # exact observation.\n    if reward_prec == np.inf:\n        new_prec = np.inf\n        new_mean = rewards.mean()\n    else:\n        new_prec = prec + len(rewards) * reward_prec\n        new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n    # Update state.\n    self.arm_param_mean[arm] = new_mean\n    self.arm_param_prec[arm] = new_prec\n    self.arm_num_observations[arm] += len(rewards)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.predict","title":"predict","text":"<pre><code>predict()\n</code></pre> <p>Return the arm with the largest sampled expected reward.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def predict(self) -&gt; int:\n    \"\"\"Return the arm with the largest sampled expected reward.\"\"\"\n    # Exploration-only phase.\n    # Order is random considering concurrent bandit scenarios.\n    arrms = np.array(self.arms)\n    for arm in self.rng.choice(arrms, len(arrms), replace=False):\n        if self.arm_num_observations[arm] &lt; self.num_exploration:\n            if self.verbose:\n                print(f\"[{self.name}] Explore arm {arm}.\")\n            return arm\n\n    # Thomopson Sampling phase.\n    expectations = self.predict_expectations()\n    if self.verbose:\n        print(f\"[{self.name}] Sampled mean rewards:\")\n        for arm, sample in expectations.items():\n            print(\n                f\"[{self.name}] Arm {arm:4d}: mu ~ N({self.arm_param_mean[arm]:.2f}, \"\n                f\"{1/self.arm_param_prec[arm]:.2f}) -&gt; {sample:.2f}\"\n            )\n    return max(expectations, key=expectations.get)  # type: ignore\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.predict_expectations","title":"predict_expectations","text":"<pre><code>predict_expectations()\n</code></pre> <p>Sample the expected reward for each arm.</p> <p>Assumes that each arm has been explored at least once. Otherwise, a value will be sampled from the prior.</p> <p>Returns:</p> Type Description <code>dict[int, float]</code> <p>A mapping from every arm to their sampled expected reward.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def predict_expectations(self) -&gt; dict[int, float]:\n    \"\"\"Sample the expected reward for each arm.\n\n    Assumes that each arm has been explored at least once. Otherwise,\n    a value will be sampled from the prior.\n\n    Returns:\n        A mapping from every arm to their sampled expected reward.\n    \"\"\"\n    expectations = {}\n    for arm in self.arms:\n        mean = self.arm_param_mean[arm]\n        prec = self.arm_param_prec[arm]\n        if prec == self.prior_prec:\n            warnings.warn(\n                f\"predict_expectations called when arm '{arm}' is cold.\",\n                stacklevel=1,\n            )\n        expectations[arm] = self.rng.normal(mean, np.sqrt(np.reciprocal(prec)))\n    return expectations\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/","title":"optimizer","text":""},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer","title":"zeus._legacy.policy.optimizer","text":"<p>Implementations of various optimization policies.</p> <p><code>JITPowerLimitOptimizer</code> and <code>PruningGTSBatchSizeOptimizer</code> are the implementations used in Zeus's publication.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer","title":"GTSBatchSizeOptimizer","text":"<p>               Bases: <code>BatchSizeOptimizer</code></p> <p>One Gaussian Thompson Sampling MAB for each job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class GTSBatchSizeOptimizer(BatchSizeOptimizer):\n    \"\"\"One Gaussian Thompson Sampling MAB for each job.\"\"\"\n\n    # ruff: noqa: D417\n    def __init__(\n        self,\n        learn_reward_precision: bool,\n        reward_precision: float = 0.0,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        num_exploration: int = 1,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the optimizer.\n\n        Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n        for descriptions of other arguments.\n\n        Args:\n            learn_reward_precision: Whether to learn the reward precision of\n                each arm as we accumulate observations.\n        \"\"\"\n        self.learn_reward_precision = learn_reward_precision\n        self.reward_precision = reward_precision\n        self.prior_mean = prior_mean\n        self.prior_precision = prior_precision\n        self.num_exploration = num_exploration\n        self.seed = seed\n        self.verbose = verbose\n\n        # One MAB for each job.\n        self.mabs: dict[Job, GaussianTS] = {}\n\n        # Track the batch size range for each job.\n        self.batch_sizes: dict[Job, list[int]] = {}\n\n        # Observation history (batch size, reward) for each job.\n        self.history: dict[Job, defaultdict[int, list[float]]] = {}\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n        return \"GaussianTS BSO\"\n\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Instantiate a new GaussianTS MAB for the new job.\"\"\"\n        # We do not want to reset the state related to this job if\n        # anything already exists.\n        if job in self.mabs:\n            return\n        self.mabs[job] = GaussianTS(\n            arms=batch_sizes,\n            reward_precision=self.reward_precision,\n            prior_mean=self.prior_mean,\n            prior_precision=self.prior_precision,\n            num_exploration=self.num_exploration,\n            seed=self.seed,\n            verbose=self.verbose,\n        )\n        self.batch_sizes[job] = batch_sizes\n        self.history[job] = defaultdict(list)\n        if self.verbose:\n            self._log(f\"Registered {job}\")\n\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the batch size to use for the job.\"\"\"\n        if self.verbose:\n            self._log(f\"Prediction for {job}\")\n        pred = self.mabs[job].predict()\n        if self.verbose:\n            self._log(f\"{job} -&gt; \\033[31mBS = {pred}\\033[0m\")\n        return pred\n\n    def observe(\n        self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n    ) -&gt; None:\n        \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n        if batch_size not in self.batch_sizes[job]:\n            raise ValueError(f\"Unknown batch size '{batch_size}' for {job}.\")\n\n        # No normalization needed since we learn a separate bandit for each job.\n        reward = -cost\n\n        # Add observation to history.\n        self.history[job][batch_size].append(reward)\n\n        # When we're not learning the reward precision, everyting is\n        # simple. We can just call `partial_fit` on the job's MAB instance.\n        if not self.learn_reward_precision:\n            self.mabs[job].fit([batch_size], [reward], reset=False)\n            if self.verbose:\n                self._log(f\"{job} @ {batch_size}: reward = {reward:.2f}\")\n\n        # When we're learning the reward precision, we need to\n        # 1. re-compute the precision this arm based on the history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all past data.\n        else:\n            arm_rewards = np.array(self.history[job][batch_size])\n            variance = np.var(arm_rewards)\n            # When there is only one observation for the arm, the variance is zero.\n            # NOTE: We might still want to have a pre-determined reward precision here\n            #       because sampling from an infinite precision Gaussian distribution\n            #       always returns the mean (the observation), and it will hamper\n            #       exploration in the early stage.\n            precision = np.inf if variance == 0.0 else np.reciprocal(variance)\n            mab = self.mabs[job]\n            mab.arm_reward_prec[batch_size] = precision\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n            self.mabs[job] = mab\n            if self.verbose:\n                arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n                self._log(\n                    f\"{job} @ {batch_size}: \"\n                    f\"arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\"\n                )\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    learn_reward_precision,\n    reward_precision=0.0,\n    prior_mean=0.0,\n    prior_precision=0.0,\n    num_exploration=1,\n    seed=123456,\n    verbose=True,\n)\n</code></pre> <p>Refer to the constructor of <code>GaussianTS</code> for descriptions of other arguments.</p> <p>Parameters:</p> Name Type Description Default <code>learn_reward_precision</code> <code>bool</code> <p>Whether to learn the reward precision of each arm as we accumulate observations.</p> required Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    learn_reward_precision: bool,\n    reward_precision: float = 0.0,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    num_exploration: int = 1,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the optimizer.\n\n    Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n    for descriptions of other arguments.\n\n    Args:\n        learn_reward_precision: Whether to learn the reward precision of\n            each arm as we accumulate observations.\n    \"\"\"\n    self.learn_reward_precision = learn_reward_precision\n    self.reward_precision = reward_precision\n    self.prior_mean = prior_mean\n    self.prior_precision = prior_precision\n    self.num_exploration = num_exploration\n    self.seed = seed\n    self.verbose = verbose\n\n    # One MAB for each job.\n    self.mabs: dict[Job, GaussianTS] = {}\n\n    # Track the batch size range for each job.\n    self.batch_sizes: dict[Job, list[int]] = {}\n\n    # Observation history (batch size, reward) for each job.\n    self.history: dict[Job, defaultdict[int, list[float]]] = {}\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.register_job","title":"register_job","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Instantiate a new GaussianTS MAB for the new job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Instantiate a new GaussianTS MAB for the new job.\"\"\"\n    # We do not want to reset the state related to this job if\n    # anything already exists.\n    if job in self.mabs:\n        return\n    self.mabs[job] = GaussianTS(\n        arms=batch_sizes,\n        reward_precision=self.reward_precision,\n        prior_mean=self.prior_mean,\n        prior_precision=self.prior_precision,\n        num_exploration=self.num_exploration,\n        seed=self.seed,\n        verbose=self.verbose,\n    )\n    self.batch_sizes[job] = batch_sizes\n    self.history[job] = defaultdict(list)\n    if self.verbose:\n        self._log(f\"Registered {job}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.predict","title":"predict","text":"<pre><code>predict(job)\n</code></pre> <p>Return the batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job) -&gt; int:\n    \"\"\"Return the batch size to use for the job.\"\"\"\n    if self.verbose:\n        self._log(f\"Prediction for {job}\")\n    pred = self.mabs[job].predict()\n    if self.verbose:\n        self._log(f\"{job} -&gt; \\033[31mBS = {pred}\\033[0m\")\n    return pred\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Learn from the cost of using the given batch size for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(\n    self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n) -&gt; None:\n    \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n    if batch_size not in self.batch_sizes[job]:\n        raise ValueError(f\"Unknown batch size '{batch_size}' for {job}.\")\n\n    # No normalization needed since we learn a separate bandit for each job.\n    reward = -cost\n\n    # Add observation to history.\n    self.history[job][batch_size].append(reward)\n\n    # When we're not learning the reward precision, everyting is\n    # simple. We can just call `partial_fit` on the job's MAB instance.\n    if not self.learn_reward_precision:\n        self.mabs[job].fit([batch_size], [reward], reset=False)\n        if self.verbose:\n            self._log(f\"{job} @ {batch_size}: reward = {reward:.2f}\")\n\n    # When we're learning the reward precision, we need to\n    # 1. re-compute the precision this arm based on the history,\n    # 2. update the arm's reward precision\n    # 3. and `fit` the new MAB instance on all past data.\n    else:\n        arm_rewards = np.array(self.history[job][batch_size])\n        variance = np.var(arm_rewards)\n        # When there is only one observation for the arm, the variance is zero.\n        # NOTE: We might still want to have a pre-determined reward precision here\n        #       because sampling from an infinite precision Gaussian distribution\n        #       always returns the mean (the observation), and it will hamper\n        #       exploration in the early stage.\n        precision = np.inf if variance == 0.0 else np.reciprocal(variance)\n        mab = self.mabs[job]\n        mab.arm_reward_prec[batch_size] = precision\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n        self.mabs[job] = mab\n        if self.verbose:\n            arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n            self._log(\n                f\"{job} @ {batch_size}: \"\n                f\"arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\"\n            )\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager","title":"PruningExploreManager","text":"<p>Helper class that generates batch sizes to explore and prune.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class PruningExploreManager:\n    \"\"\"Helper class that generates batch sizes to explore and prune.\"\"\"\n\n    def __init__(\n        self,\n        batch_sizes: list[int],\n        default: int,\n        num_pruning_rounds: int = 2,\n    ) -&gt; None:\n        \"\"\"Initialze the object.\n\n        Args:\n            batch_sizes: The initial set of batch sizes to prune from.\n            default: The default batch size (b0) to begin exploration from.\n            num_pruning_rounds: How many rounds to run pruning.\n        \"\"\"\n        # Sanity checks.\n        if default not in batch_sizes:\n            raise ValueError(f\"Default batch size {default} not in {batch_sizes}.\")\n\n        # Save arguments.\n        self.batch_sizes = batch_sizes\n        self.default = default\n        self.num_pruning_rounds = num_pruning_rounds\n\n        # State\n        self.expecting = default\n\n        # Generator that returns batch sizes.\n        self.gen = self._exploration_engine()\n\n    def _exploration_engine(\n        self,\n    ) -&gt; Generator[int | None, tuple[int, float, bool], list[int]]:\n        \"\"\"Drive pruning exploration.\n\n        Yields the batch size to be explored.\n        The caller should `send` a tuple of (explored batch size, cost, whether reached).\n        As a safety measure, the explored batch size must match the most recently yielded\n        batch size, and otherwise a `RuntimeError` is raised.\n        Finally, when exploration is over, returns a sorted list of batch sizes that\n        survived pruning.\n        \"\"\"\n        for _ in range(self.num_pruning_rounds):\n            # A list of batch sizes that reached the target metric.\n            good: list[int] = []\n\n            # We first explore downwards form the default batch size, and then go upwards.\n            idx = self.batch_sizes.index(self.default)\n            down = sorted(self.batch_sizes[: idx + 1], reverse=True)\n            up = sorted(self.batch_sizes[idx + 1 :])\n\n            # We track the best cost because the default batch size is updated to the batch\n            # size that performed the best.\n            best_cost = np.inf\n\n            for bs_list in [down, up]:\n                for bs in bs_list:\n                    # We tell the outside world to explore `bs`, and we expect the outside\n                    # world to give us back the cost of that `bs`.\n                    self.expecting = bs\n                    batch_size, cost, reached = yield bs\n                    if self.expecting != batch_size:\n                        raise RuntimeError(\n                            f\"PruningExplorationManager: {self.expecting=}, {batch_size=}\"\n                        )\n                    self.expecting = 0\n\n                    # An empty `yield` to not proceed to the next batch size when the caller\n                    # `send`s in the results.\n                    yield\n\n                    # Only batch sizes that reached the target mteric are good.\n                    if reached:\n                        if best_cost &gt; cost:\n                            best_cost = cost\n                            self.default = bs\n                        good.append(bs)\n                    # If the batch size did not reach the target metric, `break`ing here will\n                    # allow us to move on to either the next direction of exploration (upwards)\n                    # or end this round of pruning exploration.\n                    else:\n                        break\n\n            self.expecting = 0\n            self.batch_sizes = sorted(good)\n\n        return sorted(self.batch_sizes)\n\n    def next_batch_size(self) -&gt; int:\n        \"\"\"Return the next batch size to explore.\n\n        Raises `StopIteration` when pruning exploration phase is over.\n        The exception instance contains the final set of batch sizes to consider.\n        Access it through `exception.value`.\n        \"\"\"\n        batch_size = next(self.gen)\n        assert batch_size is not None, \"Call order may have been wrong.\"\n        return batch_size\n\n    def report_batch_size_result(\n        self, batch_size: int, cost: float, reached: bool\n    ) -&gt; None:\n        \"\"\"Report whether the previous batch size reached the target metric.\n\n        Args:\n            batch_size: The batch size which this cost observation is from.\n            cost: The energy-time cost of running the job with this batch size.\n            reached: Whether the job reached the target metric.\n        \"\"\"\n        none = self.gen.send((batch_size, cost, reached))\n        assert none is None, \"Call order may have been wrong.\"\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.__init__","title":"__init__","text":"<pre><code>__init__(batch_sizes, default, num_pruning_rounds=2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_sizes</code> <code>list[int]</code> <p>The initial set of batch sizes to prune from.</p> required <code>default</code> <code>int</code> <p>The default batch size (b0) to begin exploration from.</p> required <code>num_pruning_rounds</code> <code>int</code> <p>How many rounds to run pruning.</p> <code>2</code> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    batch_sizes: list[int],\n    default: int,\n    num_pruning_rounds: int = 2,\n) -&gt; None:\n    \"\"\"Initialze the object.\n\n    Args:\n        batch_sizes: The initial set of batch sizes to prune from.\n        default: The default batch size (b0) to begin exploration from.\n        num_pruning_rounds: How many rounds to run pruning.\n    \"\"\"\n    # Sanity checks.\n    if default not in batch_sizes:\n        raise ValueError(f\"Default batch size {default} not in {batch_sizes}.\")\n\n    # Save arguments.\n    self.batch_sizes = batch_sizes\n    self.default = default\n    self.num_pruning_rounds = num_pruning_rounds\n\n    # State\n    self.expecting = default\n\n    # Generator that returns batch sizes.\n    self.gen = self._exploration_engine()\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager._exploration_engine","title":"_exploration_engine","text":"<pre><code>_exploration_engine()\n</code></pre> <p>Drive pruning exploration.</p> <p>Yields the batch size to be explored. The caller should <code>send</code> a tuple of (explored batch size, cost, whether reached). As a safety measure, the explored batch size must match the most recently yielded batch size, and otherwise a <code>RuntimeError</code> is raised. Finally, when exploration is over, returns a sorted list of batch sizes that survived pruning.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _exploration_engine(\n    self,\n) -&gt; Generator[int | None, tuple[int, float, bool], list[int]]:\n    \"\"\"Drive pruning exploration.\n\n    Yields the batch size to be explored.\n    The caller should `send` a tuple of (explored batch size, cost, whether reached).\n    As a safety measure, the explored batch size must match the most recently yielded\n    batch size, and otherwise a `RuntimeError` is raised.\n    Finally, when exploration is over, returns a sorted list of batch sizes that\n    survived pruning.\n    \"\"\"\n    for _ in range(self.num_pruning_rounds):\n        # A list of batch sizes that reached the target metric.\n        good: list[int] = []\n\n        # We first explore downwards form the default batch size, and then go upwards.\n        idx = self.batch_sizes.index(self.default)\n        down = sorted(self.batch_sizes[: idx + 1], reverse=True)\n        up = sorted(self.batch_sizes[idx + 1 :])\n\n        # We track the best cost because the default batch size is updated to the batch\n        # size that performed the best.\n        best_cost = np.inf\n\n        for bs_list in [down, up]:\n            for bs in bs_list:\n                # We tell the outside world to explore `bs`, and we expect the outside\n                # world to give us back the cost of that `bs`.\n                self.expecting = bs\n                batch_size, cost, reached = yield bs\n                if self.expecting != batch_size:\n                    raise RuntimeError(\n                        f\"PruningExplorationManager: {self.expecting=}, {batch_size=}\"\n                    )\n                self.expecting = 0\n\n                # An empty `yield` to not proceed to the next batch size when the caller\n                # `send`s in the results.\n                yield\n\n                # Only batch sizes that reached the target mteric are good.\n                if reached:\n                    if best_cost &gt; cost:\n                        best_cost = cost\n                        self.default = bs\n                    good.append(bs)\n                # If the batch size did not reach the target metric, `break`ing here will\n                # allow us to move on to either the next direction of exploration (upwards)\n                # or end this round of pruning exploration.\n                else:\n                    break\n\n        self.expecting = 0\n        self.batch_sizes = sorted(good)\n\n    return sorted(self.batch_sizes)\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.next_batch_size","title":"next_batch_size","text":"<pre><code>next_batch_size()\n</code></pre> <p>Return the next batch size to explore.</p> <p>Raises <code>StopIteration</code> when pruning exploration phase is over. The exception instance contains the final set of batch sizes to consider. Access it through <code>exception.value</code>.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def next_batch_size(self) -&gt; int:\n    \"\"\"Return the next batch size to explore.\n\n    Raises `StopIteration` when pruning exploration phase is over.\n    The exception instance contains the final set of batch sizes to consider.\n    Access it through `exception.value`.\n    \"\"\"\n    batch_size = next(self.gen)\n    assert batch_size is not None, \"Call order may have been wrong.\"\n    return batch_size\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.report_batch_size_result","title":"report_batch_size_result","text":"<pre><code>report_batch_size_result(batch_size, cost, reached)\n</code></pre> <p>Report whether the previous batch size reached the target metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size which this cost observation is from.</p> required <code>cost</code> <code>float</code> <p>The energy-time cost of running the job with this batch size.</p> required <code>reached</code> <code>bool</code> <p>Whether the job reached the target metric.</p> required Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def report_batch_size_result(\n    self, batch_size: int, cost: float, reached: bool\n) -&gt; None:\n    \"\"\"Report whether the previous batch size reached the target metric.\n\n    Args:\n        batch_size: The batch size which this cost observation is from.\n        cost: The energy-time cost of running the job with this batch size.\n        reached: Whether the job reached the target metric.\n    \"\"\"\n    none = self.gen.send((batch_size, cost, reached))\n    assert none is None, \"Call order may have been wrong.\"\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer","title":"PruningGTSBatchSizeOptimizer","text":"<p>               Bases: <code>BatchSizeOptimizer</code></p> <p>One Gaussian Thompson Sampling MAB for each job with double pruning exploration.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class PruningGTSBatchSizeOptimizer(BatchSizeOptimizer):\n    \"\"\"One Gaussian Thompson Sampling MAB for each job with double pruning exploration.\"\"\"\n\n    def __init__(\n        self,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        window_size: int = 0,\n        concurrency: bool = False,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the optimizer.\n\n        Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n        for descriptions of other arguments.\n\n        Args:\n            window_size: Size of the window for the MAB (for drift handling).\n            concurrency: Whether to support concurrent job submissions.\n        \"\"\"\n        self.prior_mean = prior_mean\n        self.prior_precision = prior_precision\n        self.window_size = window_size\n        self.concurrency = concurrency\n        self.seed = seed\n        self.verbose = verbose\n\n        # One MAB for each job.\n        self.mabs: dict[Job, GaussianTS] = {}\n\n        # One PruningExplorationManager for each job.\n        self.exp_manager: dict[Job, PruningExploreManager] = {}\n\n        # Observation history (batch size, reward) for each job.\n        self.history: dict[Job, list[tuple[int, float]]] = {}\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n        return \"Pruning GaussianTS BSO\"\n\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Register the job.\"\"\"\n        # Sanity checks.\n        if job.default_bs is None:\n            raise ValueError(f\"Default BS not specified for {job}.\")\n        if not batch_sizes:\n            raise ValueError(f\"Batch size list for {job} is empty.\")\n\n        # Set internal states.\n        self.exp_manager[job] = PruningExploreManager(\n            sorted(batch_sizes), job.default_bs\n        )\n        self.history[job] = []\n        if self.verbose:\n            self._log(f\"Registered {job}\")\n\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the batch size to use for the job.\"\"\"\n        # Try to see if the exploration manager has something.\n        try:\n            batch_size = self.exp_manager[job].next_batch_size()\n            if self.verbose:\n                self._log(f\"{job} in pruning stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n        except StopIteration as exp:\n            # Pruning stage is over.\n            if job not in self.mabs:\n                self._construct_mab(job, exp.value)\n            batch_size = self.mabs[job].predict()\n            if self.verbose:\n                self._log(\n                    f\"{job} in Thompson Sampling stage -&gt; \\033[31mBS = {batch_size}\\033[0m\"\n                )\n\n        return batch_size\n\n    def observe(\n        self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n    ) -&gt; None:\n        \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n        # Add observation to history.\n        self.history[job].append((batch_size, -cost))\n\n        # We're in Thompson Sampling stage.\n        if job in self.mabs:\n            # Since we're learning the reward precision, we need to\n            # 1. re-compute the precision of this arm based on the reward history,\n            # 2. update the arm's reward precision\n            # 3. and `fit` the new MAB instance on all the reward history.\n            # Note that `arm_rewards` always has more than one entry (and hence a\n            # non-zero variance) because we've been through pruning exploration.\n            arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n            precision = np.reciprocal(np.var(arm_rewards))\n            mab = self.mabs[job]\n            mab.arm_reward_prec[batch_size] = precision\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n            if self.verbose:\n                arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n                self._log(\n                    f\"{job} @ {batch_size}: \"\n                    f\"arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\"\n                )\n\n        # We're in pruning stage.\n        else:\n            assert converged is not None\n            # Log before we potentially error out.\n            if self.verbose:\n                self._log(\n                    f\"{job} in pruning stage, expecting BS {self.exp_manager[job].expecting}.\"\n                    f\" Current BS {batch_size} that did {'not ' * converged}converge.\"\n                )\n\n            # If we don't support concurrency, we can just pass the results to the\n            # exploration manager, and the manager will err if the order of batch sizes\n            # is screwed up.\n            if not self.concurrency:\n                self.exp_manager[job].report_batch_size_result(\n                    batch_size, cost, converged\n                )\n                return\n\n            # If we are supporting concurrency, there's a subtle issue.\n            # Pruning exploration demands a specific order of trying out a batch size\n            # and receiving the results (cost and whether reached). This breaks in the\n            # following situation, for example:\n            # 1. Job with BS 32 that is part of pruning exploration starts.\n            # 2. Concurrent job comes in, and we launch it with the best known BS 64.\n            # 3. Job with BS 64 finishes first, and calls bso.observe with BS 64.\n            # This breaks the observation order assumption of PruningExplorationManager.\n            # Thus we check whether the current batch size is the one expected by\n            # PruningExplorationManager, and then only if so, call bso.observe.\n            # Otherwise, we silently insert the cost observation into the bso's history\n            # (first line of this method) and don't touch the PruningExplorationManager.\n            if self.exp_manager[job].expecting == batch_size:\n                self.exp_manager[job].report_batch_size_result(\n                    batch_size, cost, converged\n                )\n\n    def _get_history_for_bs(self, job: Job, batch_size: int) -&gt; list[float]:\n        \"\"\"Return the windowed history for the given job's batch size.\"\"\"\n        history = self.history[job]\n        rewards = []\n        # Collect rewards starting from the most recent ones and backwards.\n        for bs, reward in reversed(history):\n            if bs == batch_size:\n                rewards.append(reward)\n                if len(rewards) == self.window_size:\n                    break\n        # There's no need to return this in time order, but just in case.\n        return list(reversed(rewards))\n\n    def _construct_mab(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"When exploration is over, this method is called to construct and learn GTS.\"\"\"\n        # Sanity check.\n        if not batch_sizes:\n            raise ValueError(\n                \"Empty batch size set when constructing MAB. \"\n                \"Probably all batch sizes have been pruned.\"\n            )\n\n        if self.verbose:\n            self._log(f\"Construct MAB for {job} with arms {batch_sizes}\")\n\n        mab = GaussianTS(\n            arms=batch_sizes,  # The MAB only has \"good\" arms.\n            reward_precision=0.0,\n            prior_mean=self.prior_mean,\n            prior_precision=self.prior_precision,\n            num_exploration=2,\n            seed=self.seed,\n            verbose=self.verbose,\n        )\n        # Fit the arm for each good batch size.\n        for batch_size in self.exp_manager[job].batch_sizes:\n            arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n            assert (\n                len(arm_rewards) &gt;= 2\n            ), f\"Number of observations for {batch_size} is {len(arm_rewards)}.\"\n            mab.arm_reward_prec[batch_size] = np.reciprocal(np.var(arm_rewards))\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n        # Save the MAB.\n        self.mabs[job] = mab\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    prior_mean=0.0,\n    prior_precision=0.0,\n    window_size=0,\n    concurrency=False,\n    seed=123456,\n    verbose=True,\n)\n</code></pre> <p>Refer to the constructor of <code>GaussianTS</code> for descriptions of other arguments.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Size of the window for the MAB (for drift handling).</p> <code>0</code> <code>concurrency</code> <code>bool</code> <p>Whether to support concurrent job submissions.</p> <code>False</code> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    window_size: int = 0,\n    concurrency: bool = False,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the optimizer.\n\n    Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n    for descriptions of other arguments.\n\n    Args:\n        window_size: Size of the window for the MAB (for drift handling).\n        concurrency: Whether to support concurrent job submissions.\n    \"\"\"\n    self.prior_mean = prior_mean\n    self.prior_precision = prior_precision\n    self.window_size = window_size\n    self.concurrency = concurrency\n    self.seed = seed\n    self.verbose = verbose\n\n    # One MAB for each job.\n    self.mabs: dict[Job, GaussianTS] = {}\n\n    # One PruningExplorationManager for each job.\n    self.exp_manager: dict[Job, PruningExploreManager] = {}\n\n    # Observation history (batch size, reward) for each job.\n    self.history: dict[Job, list[tuple[int, float]]] = {}\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.register_job","title":"register_job","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Register the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Register the job.\"\"\"\n    # Sanity checks.\n    if job.default_bs is None:\n        raise ValueError(f\"Default BS not specified for {job}.\")\n    if not batch_sizes:\n        raise ValueError(f\"Batch size list for {job} is empty.\")\n\n    # Set internal states.\n    self.exp_manager[job] = PruningExploreManager(\n        sorted(batch_sizes), job.default_bs\n    )\n    self.history[job] = []\n    if self.verbose:\n        self._log(f\"Registered {job}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.predict","title":"predict","text":"<pre><code>predict(job)\n</code></pre> <p>Return the batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job) -&gt; int:\n    \"\"\"Return the batch size to use for the job.\"\"\"\n    # Try to see if the exploration manager has something.\n    try:\n        batch_size = self.exp_manager[job].next_batch_size()\n        if self.verbose:\n            self._log(f\"{job} in pruning stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n    except StopIteration as exp:\n        # Pruning stage is over.\n        if job not in self.mabs:\n            self._construct_mab(job, exp.value)\n        batch_size = self.mabs[job].predict()\n        if self.verbose:\n            self._log(\n                f\"{job} in Thompson Sampling stage -&gt; \\033[31mBS = {batch_size}\\033[0m\"\n            )\n\n    return batch_size\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Learn from the cost of using the given batch size for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(\n    self, job: Job, batch_size: int, cost: float, converged: bool | None = None\n) -&gt; None:\n    \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n    # Add observation to history.\n    self.history[job].append((batch_size, -cost))\n\n    # We're in Thompson Sampling stage.\n    if job in self.mabs:\n        # Since we're learning the reward precision, we need to\n        # 1. re-compute the precision of this arm based on the reward history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all the reward history.\n        # Note that `arm_rewards` always has more than one entry (and hence a\n        # non-zero variance) because we've been through pruning exploration.\n        arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n        precision = np.reciprocal(np.var(arm_rewards))\n        mab = self.mabs[job]\n        mab.arm_reward_prec[batch_size] = precision\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n        if self.verbose:\n            arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n            self._log(\n                f\"{job} @ {batch_size}: \"\n                f\"arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\"\n            )\n\n    # We're in pruning stage.\n    else:\n        assert converged is not None\n        # Log before we potentially error out.\n        if self.verbose:\n            self._log(\n                f\"{job} in pruning stage, expecting BS {self.exp_manager[job].expecting}.\"\n                f\" Current BS {batch_size} that did {'not ' * converged}converge.\"\n            )\n\n        # If we don't support concurrency, we can just pass the results to the\n        # exploration manager, and the manager will err if the order of batch sizes\n        # is screwed up.\n        if not self.concurrency:\n            self.exp_manager[job].report_batch_size_result(\n                batch_size, cost, converged\n            )\n            return\n\n        # If we are supporting concurrency, there's a subtle issue.\n        # Pruning exploration demands a specific order of trying out a batch size\n        # and receiving the results (cost and whether reached). This breaks in the\n        # following situation, for example:\n        # 1. Job with BS 32 that is part of pruning exploration starts.\n        # 2. Concurrent job comes in, and we launch it with the best known BS 64.\n        # 3. Job with BS 64 finishes first, and calls bso.observe with BS 64.\n        # This breaks the observation order assumption of PruningExplorationManager.\n        # Thus we check whether the current batch size is the one expected by\n        # PruningExplorationManager, and then only if so, call bso.observe.\n        # Otherwise, we silently insert the cost observation into the bso's history\n        # (first line of this method) and don't touch the PruningExplorationManager.\n        if self.exp_manager[job].expecting == batch_size:\n            self.exp_manager[job].report_batch_size_result(\n                batch_size, cost, converged\n            )\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer._get_history_for_bs","title":"_get_history_for_bs","text":"<pre><code>_get_history_for_bs(job, batch_size)\n</code></pre> <p>Return the windowed history for the given job's batch size.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _get_history_for_bs(self, job: Job, batch_size: int) -&gt; list[float]:\n    \"\"\"Return the windowed history for the given job's batch size.\"\"\"\n    history = self.history[job]\n    rewards = []\n    # Collect rewards starting from the most recent ones and backwards.\n    for bs, reward in reversed(history):\n        if bs == batch_size:\n            rewards.append(reward)\n            if len(rewards) == self.window_size:\n                break\n    # There's no need to return this in time order, but just in case.\n    return list(reversed(rewards))\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer._construct_mab","title":"_construct_mab","text":"<pre><code>_construct_mab(job, batch_sizes)\n</code></pre> <p>When exploration is over, this method is called to construct and learn GTS.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _construct_mab(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"When exploration is over, this method is called to construct and learn GTS.\"\"\"\n    # Sanity check.\n    if not batch_sizes:\n        raise ValueError(\n            \"Empty batch size set when constructing MAB. \"\n            \"Probably all batch sizes have been pruned.\"\n        )\n\n    if self.verbose:\n        self._log(f\"Construct MAB for {job} with arms {batch_sizes}\")\n\n    mab = GaussianTS(\n        arms=batch_sizes,  # The MAB only has \"good\" arms.\n        reward_precision=0.0,\n        prior_mean=self.prior_mean,\n        prior_precision=self.prior_precision,\n        num_exploration=2,\n        seed=self.seed,\n        verbose=self.verbose,\n    )\n    # Fit the arm for each good batch size.\n    for batch_size in self.exp_manager[job].batch_sizes:\n        arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n        assert (\n            len(arm_rewards) &gt;= 2\n        ), f\"Number of observations for {batch_size} is {len(arm_rewards)}.\"\n        mab.arm_reward_prec[batch_size] = np.reciprocal(np.var(arm_rewards))\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n    # Save the MAB.\n    self.mabs[job] = mab\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer","title":"JITPowerLimitOptimizer","text":"<p>               Bases: <code>PowerLimitOptimizer</code></p> <p>Returns the best power limit to use for the job &amp; batch size.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class JITPowerLimitOptimizer(PowerLimitOptimizer):\n    \"\"\"Returns the best power limit to use for the job &amp; batch size.\"\"\"\n\n    def __init__(self, verbose: bool = True) -&gt; None:\n        \"\"\"Initialize the object.\"\"\"\n        self.verbose = verbose\n\n        self.best_pl: defaultdict[Job, dict[int, int]] = defaultdict(dict)\n        self.best_cost: defaultdict[Job, dict[int, float]] = defaultdict(dict)\n        self.observe_count: defaultdict[Job, defaultdict[int, int]] = defaultdict(\n            lambda: defaultdict(int)\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the power limit optimizer.\"\"\"\n        return \"JITPSO\"\n\n    def predict(self, job: Job, batch_size: int) -&gt; int | None:\n        \"\"\"Return the best power limit for the job, or None if unknown.\"\"\"\n        pred = self.best_pl[job].get(batch_size)\n        if self.verbose:\n            self._log(\n                f\"{job} @ {batch_size} -&gt; \\033[31mPL = \"\n                f\"{'needs profiling' if pred is None else str(pred) + 'W'}\\033[0m\"\n            )\n        return pred\n\n    def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n        \"\"\"Learn from the cost of using the given knobs for the job.\"\"\"\n        self.observe_count[job][batch_size] += 1\n        prev_best_cost = self.best_cost[job].get(batch_size)\n        if prev_best_cost is None or prev_best_cost &gt; cost:\n            self.best_pl[job][batch_size] = power_limit\n            self.best_cost[job][batch_size] = cost\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the power limit optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(verbose=True)\n</code></pre> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(self, verbose: bool = True) -&gt; None:\n    \"\"\"Initialize the object.\"\"\"\n    self.verbose = verbose\n\n    self.best_pl: defaultdict[Job, dict[int, int]] = defaultdict(dict)\n    self.best_cost: defaultdict[Job, dict[int, float]] = defaultdict(dict)\n    self.observe_count: defaultdict[Job, defaultdict[int, int]] = defaultdict(\n        lambda: defaultdict(int)\n    )\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.predict","title":"predict","text":"<pre><code>predict(job, batch_size)\n</code></pre> <p>Return the best power limit for the job, or None if unknown.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job, batch_size: int) -&gt; int | None:\n    \"\"\"Return the best power limit for the job, or None if unknown.\"\"\"\n    pred = self.best_pl[job].get(batch_size)\n    if self.verbose:\n        self._log(\n            f\"{job} @ {batch_size} -&gt; \\033[31mPL = \"\n            f\"{'needs profiling' if pred is None else str(pred) + 'W'}\\033[0m\"\n        )\n    return pred\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, power_limit, cost)\n</code></pre> <p>Learn from the cost of using the given knobs for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n    \"\"\"Learn from the cost of using the given knobs for the job.\"\"\"\n    self.observe_count[job][batch_size] += 1\n    prev_best_cost = self.best_cost[job].get(batch_size)\n    if prev_best_cost is None or prev_best_cost &gt; cost:\n        self.best_pl[job][batch_size] = power_limit\n        self.best_cost[job][batch_size] = cost\n</code></pre>"},{"location":"reference/device/","title":"device","text":""},{"location":"reference/device/#zeus.device","title":"zeus.device","text":"<p>Abstraction layer over devices like GPUs.</p>"},{"location":"reference/device/exception/","title":"exception","text":""},{"location":"reference/device/exception/#zeus.device.exception","title":"zeus.device.exception","text":"<p>Base Zeus GPU Exception Class.</p>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseGPUError","title":"ZeusBaseGPUError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Zeus base GPU exception class.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusBaseGPUError(ZeusBaseError):\n    \"\"\"Zeus base GPU exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseGPUError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/","title":"gpu","text":""},{"location":"reference/device/gpu/#zeus.device.gpu","title":"zeus.device.gpu","text":"<p>GPU device module for Zeus. Abstraction of GPU devices.</p> <p>The main function of this module is <code>get_gpus</code>, which returns a GPU Manager object specific to the platform. To instantiate a GPU Manager object, you can do the following:</p> <pre><code>from zeus.device import get_gpus\ngpus = get_gpus() # Returns NVIDIAGPUs() or AMDGPUs() depending on the platform.\n</code></pre> <p>There exists a 1:1 mapping between specific library functions and methods implemented in the GPU Manager object. For example, for NVIDIA systems, if you wanted to do:</p> <pre><code>handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\nconstraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(handle)\n</code></pre> <p>You can now do:</p> <pre><code>gpus = get_gpus() # returns a NVIDIAGPUs object\nconstraints =  gpus.getPowerManagementLimitConstraints(gpu_index)\n</code></pre> <p>Class hierarchy:</p> <ul> <li><code>GPUs</code>: Abstract class for GPU managers.<ul> <li><code>NVIDIAGPUs</code>: GPU manager for NVIDIA GPUs, initialize NVIDIAGPU objects.</li> <li><code>AMDGPUs</code>: GPU manager for AMD GPUs, initialize AMDGPU objects.</li> </ul> </li> <li><code>GPU</code>: Abstract class for GPU objects.<ul> <li><code>NVIDIAGPU</code>: GPU object for NVIDIA GPUs.</li> <li><code>AMDGPU</code>: GPU object for AMD GPUs.</li> </ul> </li> </ul> <p>The following exceptions are defined in this module:</p> <ul> <li><code>ZeusGPUInitError</code>: Base class for initialization errors.</li> <li><code>ZeusGPUInvalidArgError</code>: Error for invalid arguments.</li> <li><code>ZeusGPUNotSupportedError</code>: Error for unsupported GPUs.</li> <li><code>ZeusGPUNoPermissionError</code>: Error for permission issues.</li> <li><code>ZeusGPUAlreadyInitializedError</code>: Error for reinitialization.</li> <li><code>ZeusGPUNotFoundError</code>: Error for missing GPUs.</li> <li><code>ZeusGPUInsufficientSizeError</code>: Error for insufficient buffer size.</li> <li><code>ZeusGPUInsufficientPowerError</code>: Error for insufficient power.</li> <li><code>ZeusGPUDriverNotLoadedError</code>: Error for driver issues.</li> <li><code>ZeusGPUTimeoutError</code>: Error for timeout issues.</li> <li><code>ZeusGPUIRQError</code>: Error for IRQ issues.</li> <li><code>ZeusGPULibraryNotFoundError</code>: Error for missing libraries.</li> <li><code>ZeusGPUFunctionNotFoundError</code>: Error for missing functions.</li> <li><code>ZeusGPUCorruptedInfoROMError</code>: Error for corrupted info ROM.</li> <li><code>ZeusGPULostError</code>: Error for lost GPUs.</li> <li><code>ZeusGPUResetRequiredError</code>: Error for GPUs requiring reset.</li> <li><code>ZeusGPUOperatingSystemError</code>: Error for OS issues.</li> <li><code>ZeusGPULibRMVersionMismatchError</code>: Error for library version mismatch.</li> <li><code>ZeusGPUMemoryError</code>: Error for memory issues.</li> <li><code>ZeusGPUUnknownError</code>: Error for unknown issues.</li> </ul>"},{"location":"reference/device/gpu/#zeus.device.gpu.get_gpus","title":"get_gpus","text":"<pre><code>get_gpus(ensure_homogeneous=False)\n</code></pre> <p>Initialize and return a singleton GPU monitoring object for NVIDIA or AMD GPUs.</p> <p>The function returns a GPU management object that aims to abstract the underlying GPU monitoring libraries (pynvml for NVIDIA GPUs and amdsmi for AMD GPUs), and provides a 1:1 mapping between the methods in the object and related library functions.</p> <p>This function attempts to initialize GPU monitoring using the pynvml library for NVIDIA GPUs first. If pynvml is not available or fails to initialize, it then tries to use the amdsmi library for AMD GPUs. If both attempts fail, it raises a ZeusErrorInit exception.</p> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name. False by default.</p> <code>False</code> Source code in <code>zeus/device/gpu/__init__.py</code> <pre><code>def get_gpus(ensure_homogeneous: bool = False) -&gt; GPUs:\n    \"\"\"Initialize and return a singleton GPU monitoring object for NVIDIA or AMD GPUs.\n\n    The function returns a GPU management object that aims to abstract the underlying GPU monitoring libraries\n    (pynvml for NVIDIA GPUs and amdsmi for AMD GPUs), and provides a 1:1 mapping between the methods in the object and related library functions.\n\n    This function attempts to initialize GPU monitoring using the pynvml library for NVIDIA GPUs\n    first. If pynvml is not available or fails to initialize, it then tries to use the amdsmi\n    library for AMD GPUs. If both attempts fail, it raises a ZeusErrorInit exception.\n\n    Args:\n        ensure_homogeneous (bool, optional): If True, ensures that all tracked GPUs have the same name. False by default.\n    \"\"\"\n    global _gpus\n    if _gpus is not None:\n        return _gpus\n\n    if nvml_is_available():\n        _gpus = NVIDIAGPUs(ensure_homogeneous)\n        return _gpus\n    elif amdsmi_is_available():\n        _gpus = AMDGPUs(ensure_homogeneous)\n        return _gpus\n    else:\n        raise ZeusGPUInitError(\n            \"NVML and AMDSMI unavailable. Failed to initialize GPU management library.\"\n        )\n</code></pre>"},{"location":"reference/device/gpu/amd/","title":"amd","text":""},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd","title":"zeus.device.gpu.amd","text":"<p>AMD GPUs.</p>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.MockAMDSMI","title":"MockAMDSMI","text":"<p>Mock class for AMD SMI library.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class MockAMDSMI:\n    \"\"\"Mock class for AMD SMI library.\"\"\"\n\n    def __getattr__(self, name):\n        \"\"\"Raise an error if any method is called.\n\n        Since this class is only used when `amdsmi` is not available,\n        something has gone wrong if any method is called.\n        \"\"\"\n        raise RuntimeError(\n            f\"amdsmi is not available and amdsmi.{name} shouldn't have been called. \"\n            \"This is a bug.\"\n        )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.MockAMDSMI.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Raise an error if any method is called.</p> <p>Since this class is only used when <code>amdsmi</code> is not available, something has gone wrong if any method is called.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"Raise an error if any method is called.\n\n    Since this class is only used when `amdsmi` is not available,\n    something has gone wrong if any method is called.\n    \"\"\"\n    raise RuntimeError(\n        f\"amdsmi is not available and amdsmi.{name} shouldn't have been called. \"\n        \"This is a bug.\"\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU","title":"AMDGPU","text":"<p>               Bases: <code>GPU</code></p> <p>Control a Single AMD GPU.</p> <p>Uses amdsmi Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the amdsmi library functions. Zeus GPU Exceptions are raised when amdsmi errors occur. To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to amdsmi.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class AMDGPU(gpu_common.GPU):\n    \"\"\"Control a Single AMD GPU.\n\n    Uses amdsmi Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the amdsmi library functions.\n    Zeus GPU Exceptions are raised when amdsmi errors occur.\n    To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to amdsmi.\n    \"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initializes the AMDGPU object with a specified GPU index. Acquires a handle to the GPU using `amdsmi.amdsmi_get_processor_handles()`.\"\"\"\n        super().__init__(gpu_index)\n        self._get_handle()\n        self._supportsGetTotalEnergyConsumption = None\n\n    _exception_map = {\n        1: gpu_common.ZeusGPUInvalidArgError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INVAL\n        2: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_SUPPORTED\n        8: gpu_common.ZeusGPUTimeoutError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_TIMEOUT\n        10: gpu_common.ZeusGPUNoPermissionError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NO_PERM\n        15: gpu_common.ZeusGPUMemoryError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_OUT_OF_RESOURCES\n        18: gpu_common.ZeusGPUInitError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INIT_ERROR\n        31: gpu_common.ZeusGPUNotFoundError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_FOUND\n        32: gpu_common.ZeusGPUInitError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_INIT\n        34: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_DRIVER_NOT_LOADED\n        41: gpu_common.ZeusGPUInsufficientSizeError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INSUFFICIENT_SIZE\n        45: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_ENERGY_DRV\n        46: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_MSR_DRV\n        47: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_DRV\n        48: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_SUP\n        49: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_MSG_SUP\n        50: gpu_common.ZeusGPUTimeoutError,  # amdsmi.amdsmi_wrapper.AMDSMI_HSMP_TIMEOUT\n        51: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_DRV\n        52: gpu_common.ZeusGPULibraryNotFoundError,  # amdsmi.amdsmi_wrapper.AMDSMI_FILE_NOT_FOUND\n        53: gpu_common.ZeusGPUInvalidArgError,  # amdsmi.amdsmi_wrapper.AMDSMI_ARG_PTR_NULL\n        4294967295: gpu_common.ZeusGPUUnknownError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_UNKNOWN_ERROR\n    }\n\n    @_handle_amdsmi_errors\n    def _get_handle(self):\n        handles = amdsmi.amdsmi_get_processor_handles()\n        if len(handles) &lt;= self.gpu_index:\n            raise gpu_common.ZeusGPUNotFoundError(\n                f\"GPU with index {self.gpu_index} not found. Found {len(handles)} GPUs.\"\n            )\n        self.handle = amdsmi.amdsmi_get_processor_handles()[self.gpu_index]\n\n    @_handle_amdsmi_errors\n    def getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n        \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n        info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in W\n        return (info[\"min_power_cap\"] * 1000, info[\"max_power_cap\"] * 1000)\n\n    @_handle_amdsmi_errors\n    def setPersistenceMode(self, enable: bool) -&gt; None:\n        \"\"\"If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.\"\"\"\n        # N/A for AMD GPUs.\n        pass\n\n    @_handle_amdsmi_errors\n    def setPowerManagementLimit(self, value: int) -&gt; None:\n        \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n        amdsmi.amdsmi_set_power_cap(\n            self.handle, 0, int(value * 1000)\n        )  # Units for set_power_cap: microwatts\n\n    @_handle_amdsmi_errors\n    def resetPowerManagementLimit(self) -&gt; None:\n        \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n        info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in W\n        amdsmi.amdsmi_set_power_cap(\n            self.handle, 0, cap=int(info[\"default_power_cap\"] * 1e6)\n        )  # expects value in microwatts\n\n    @_handle_amdsmi_errors\n    def setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n        \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.\"\"\"\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            minMemClockMHz,\n            maxMemClockMHz,\n            clk_type=amdsmi.AmdSmiClkType.MEM,\n        )\n\n    @_handle_amdsmi_errors\n    def getSupportedMemoryClocks(self) -&gt; list[int]:\n        \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n        raise gpu_common.ZeusGPUNotSupportedError(\n            \"AMDSMI does not support querying memory frequencies\"\n        )\n\n    @_handle_amdsmi_errors\n    def getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n        \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n        raise gpu_common.ZeusGPUNotSupportedError(\n            \"AMDSMI does not support querying GFX frequencies given a memory frequency\"\n        )\n\n    @_handle_amdsmi_errors\n    def getName(self) -&gt; str:\n        \"\"\"Returns the name of the specified GPU.\"\"\"\n        info = amdsmi.amdsmi_get_gpu_asic_info(self.handle)\n        return info[\"market_name\"]\n\n    @_handle_amdsmi_errors\n    def setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n        \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies.  Units: MHz.\"\"\"\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            minGpuClockMHz,\n            maxGpuClockMHz,\n            clk_type=amdsmi.AmdSmiClkType.GFX,\n        )\n\n    @_handle_amdsmi_errors\n    def resetMemoryLockedClocks(self) -&gt; None:\n        \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n        # Get default MEM clock values\n        info = amdsmi.amdsmi_get_clock_info(\n            self.handle, amdsmi.AmdSmiClkType.MEM\n        )  # returns MHz\n\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            info[\"min_clk\"],\n            info[\"max_clk\"],\n            clk_type=amdsmi.AmdSmiClkType.MEM,\n        )  # expects MHz\n\n    @_handle_amdsmi_errors\n    def resetGpuLockedClocks(self) -&gt; None:\n        \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n        # Get default GPU clock values\n        info = amdsmi.amdsmi_get_clock_info(\n            self.handle, amdsmi.AmdSmiClkType.GFX\n        )  # returns MHz\n\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            info[\"min_clk\"],\n            info[\"max_clk\"],\n            clk_type=amdsmi.AmdSmiClkType.GFX,\n        )  # expects MHz\n\n    @_handle_amdsmi_errors\n    def getInstantPowerUsage(self) -&gt; int:\n        \"\"\"Returns the current power usage of the specified GPU. Units: mW.\"\"\"\n        # returns in W, convert to mW\n        return int(\n            amdsmi.amdsmi_get_power_info(self.handle)[\"average_socket_power\"] * 1000\n        )\n\n    @_handle_amdsmi_errors\n    def supportsGetTotalEnergyConsumption(self) -&gt; bool:\n        \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n        if self._supportsGetTotalEnergyConsumption is None:\n            try:\n                _ = amdsmi.amdsmi_get_energy_count(self.handle)\n                self._supportsGetTotalEnergyConsumption = True\n            except amdsmi.AmdSmiLibraryException as e:\n                if (\n                    e.get_error_code() == 2\n                ):  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_SUPPORTED\n                    self._supportsGetTotalEnergyConsumption = False\n                else:\n                    raise e\n\n        return self._supportsGetTotalEnergyConsumption\n\n    @_handle_amdsmi_errors\n    def getTotalEnergyConsumption(self) -&gt; int:\n        \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n        info = amdsmi.amdsmi_get_energy_count(self.handle)\n        return int(\n            info[\"power\"] / 1e3\n        )  # returns in micro Joules, convert to mili Joules\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initializes the AMDGPU object with a specified GPU index. Acquires a handle to the GPU using `amdsmi.amdsmi_get_processor_handles()`.\"\"\"\n    super().__init__(gpu_index)\n    self._get_handle()\n    self._supportsGetTotalEnergyConsumption = None\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getPowerManagementLimitConstraints","title":"getPowerManagementLimitConstraints","text":"<pre><code>getPowerManagementLimitConstraints()\n</code></pre> <p>Returns the minimum and maximum power management limits for the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n    \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in W\n    return (info[\"min_power_cap\"] * 1000, info[\"max_power_cap\"] * 1000)\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.setPersistenceMode","title":"setPersistenceMode","text":"<pre><code>setPersistenceMode(enable)\n</code></pre> <p>If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef setPersistenceMode(self, enable: bool) -&gt; None:\n    \"\"\"If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.\"\"\"\n    # N/A for AMD GPUs.\n    pass\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.setPowerManagementLimit","title":"setPowerManagementLimit","text":"<pre><code>setPowerManagementLimit(value)\n</code></pre> <p>Sets the power management limit for the specified GPU to the given value. Unit: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef setPowerManagementLimit(self, value: int) -&gt; None:\n    \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n    amdsmi.amdsmi_set_power_cap(\n        self.handle, 0, int(value * 1000)\n    )  # Units for set_power_cap: microwatts\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.resetPowerManagementLimit","title":"resetPowerManagementLimit","text":"<pre><code>resetPowerManagementLimit()\n</code></pre> <p>Resets the power management limit for the specified GPU to the default value.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef resetPowerManagementLimit(self) -&gt; None:\n    \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in W\n    amdsmi.amdsmi_set_power_cap(\n        self.handle, 0, cap=int(info[\"default_power_cap\"] * 1e6)\n    )  # expects value in microwatts\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.setMemoryLockedClocks","title":"setMemoryLockedClocks","text":"<pre><code>setMemoryLockedClocks(minMemClockMHz, maxMemClockMHz)\n</code></pre> <p>Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n    \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.\"\"\"\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        minMemClockMHz,\n        maxMemClockMHz,\n        clk_type=amdsmi.AmdSmiClkType.MEM,\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getSupportedMemoryClocks","title":"getSupportedMemoryClocks","text":"<pre><code>getSupportedMemoryClocks()\n</code></pre> <p>Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getSupportedMemoryClocks(self) -&gt; list[int]:\n    \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n    raise gpu_common.ZeusGPUNotSupportedError(\n        \"AMDSMI does not support querying memory frequencies\"\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getSupportedGraphicsClocks","title":"getSupportedGraphicsClocks","text":"<pre><code>getSupportedGraphicsClocks(freq)\n</code></pre> <p>Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n    \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n    raise gpu_common.ZeusGPUNotSupportedError(\n        \"AMDSMI does not support querying GFX frequencies given a memory frequency\"\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getName","title":"getName","text":"<pre><code>getName()\n</code></pre> <p>Returns the name of the specified GPU.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getName(self) -&gt; str:\n    \"\"\"Returns the name of the specified GPU.\"\"\"\n    info = amdsmi.amdsmi_get_gpu_asic_info(self.handle)\n    return info[\"market_name\"]\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.setGpuLockedClocks","title":"setGpuLockedClocks","text":"<pre><code>setGpuLockedClocks(minGpuClockMHz, maxGpuClockMHz)\n</code></pre> <p>Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies.  Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n    \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies.  Units: MHz.\"\"\"\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        minGpuClockMHz,\n        maxGpuClockMHz,\n        clk_type=amdsmi.AmdSmiClkType.GFX,\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.resetMemoryLockedClocks","title":"resetMemoryLockedClocks","text":"<pre><code>resetMemoryLockedClocks()\n</code></pre> <p>Resets the memory locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef resetMemoryLockedClocks(self) -&gt; None:\n    \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n    # Get default MEM clock values\n    info = amdsmi.amdsmi_get_clock_info(\n        self.handle, amdsmi.AmdSmiClkType.MEM\n    )  # returns MHz\n\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        info[\"min_clk\"],\n        info[\"max_clk\"],\n        clk_type=amdsmi.AmdSmiClkType.MEM,\n    )  # expects MHz\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.resetGpuLockedClocks","title":"resetGpuLockedClocks","text":"<pre><code>resetGpuLockedClocks()\n</code></pre> <p>Resets the GPU locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef resetGpuLockedClocks(self) -&gt; None:\n    \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n    # Get default GPU clock values\n    info = amdsmi.amdsmi_get_clock_info(\n        self.handle, amdsmi.AmdSmiClkType.GFX\n    )  # returns MHz\n\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        info[\"min_clk\"],\n        info[\"max_clk\"],\n        clk_type=amdsmi.AmdSmiClkType.GFX,\n    )  # expects MHz\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getInstantPowerUsage","title":"getInstantPowerUsage","text":"<pre><code>getInstantPowerUsage()\n</code></pre> <p>Returns the current power usage of the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getInstantPowerUsage(self) -&gt; int:\n    \"\"\"Returns the current power usage of the specified GPU. Units: mW.\"\"\"\n    # returns in W, convert to mW\n    return int(\n        amdsmi.amdsmi_get_power_info(self.handle)[\"average_socket_power\"] * 1000\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.supportsGetTotalEnergyConsumption","title":"supportsGetTotalEnergyConsumption","text":"<pre><code>supportsGetTotalEnergyConsumption()\n</code></pre> <p>Returns True if the specified GPU supports retrieving the total energy consumption.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef supportsGetTotalEnergyConsumption(self) -&gt; bool:\n    \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n    if self._supportsGetTotalEnergyConsumption is None:\n        try:\n            _ = amdsmi.amdsmi_get_energy_count(self.handle)\n            self._supportsGetTotalEnergyConsumption = True\n        except amdsmi.AmdSmiLibraryException as e:\n            if (\n                e.get_error_code() == 2\n            ):  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_SUPPORTED\n                self._supportsGetTotalEnergyConsumption = False\n            else:\n                raise e\n\n    return self._supportsGetTotalEnergyConsumption\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.getTotalEnergyConsumption","title":"getTotalEnergyConsumption","text":"<pre><code>getTotalEnergyConsumption()\n</code></pre> <p>Returns the total energy consumption of the specified GPU. Units: mJ.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef getTotalEnergyConsumption(self) -&gt; int:\n    \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n    info = amdsmi.amdsmi_get_energy_count(self.handle)\n    return int(\n        info[\"power\"] / 1e3\n    )  # returns in micro Joules, convert to mili Joules\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.UnprivilegedAMDGPU","title":"UnprivilegedAMDGPU","text":"<p>               Bases: <code>AMDGPU</code></p> <p>Control a Single AMD GPU with no SYS_ADMIN privileges.</p> <p>Uses amdsmi Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the amdsmi library functions. Zeus GPU Exceptions are raised when amdsmi errors occur. To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to amdsmi.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class UnprivilegedAMDGPU(AMDGPU):\n    \"\"\"Control a Single AMD GPU with no SYS_ADMIN privileges.\n\n    Uses amdsmi Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the amdsmi library functions.\n    Zeus GPU Exceptions are raised when amdsmi errors occur.\n    To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to amdsmi.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs","title":"AMDGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>AMD GPU Manager object, containing individual AMDGPU objects, abstracting amdsmi calls and handling related exceptions.</p> <p>Important</p> <p>Currently only ROCM 6.0 is supported.</p> <p>This class provides a high-level interface to interact with AMD GPUs. <code>HIP_VISIBLE_DEVICES</code> environment variable is respected if set. For example, if there are 4 GPUs and <code>HIP_VISIBLE_DEVICES=0,2</code>, only GPUs 0 and 2 are instantiated. In this case, to access GPU of HIP index 0, use the index 0, and for HIP index 2, use the index 1.</p> <p>This class provides a 1:1 mapping between the methods and AMDSMI library functions. For example, if you want to do the following:</p> <pre><code>handle = amdsmi.amdsmi_get_processor_handles()[gpu_index]\ninfo = amdsmi.amdsmi_get_power_cap_info(self.handle)\nconstraints = (info.min_power_cap, info.max_power_cap)\n</code></pre> <p>You can now do: <pre><code>gpus = get_gpus() # returns a AMDGPUs object\nconstraints =  gpus.getPowerManagementLimitConstraints(gpu_index)\n</code></pre></p> <p>Note: This class instantiates (grabs the handle, by calling <code>amdsmi.amdsmi_get_processor_handles()</code>) all GPUs that are visible to the system, as determined by the <code>HIP_VISIBLE_DEVICES</code> environment variable if set.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class AMDGPUs(gpu_common.GPUs):\n    \"\"\"AMD GPU Manager object, containing individual AMDGPU objects, abstracting amdsmi calls and handling related exceptions.\n\n    !!! Important\n        Currently only ROCM 6.0 is supported.\n\n    This class provides a high-level interface to interact with AMD GPUs. `HIP_VISIBLE_DEVICES` environment variable is respected if set. For example, if there are\n    4 GPUs and `HIP_VISIBLE_DEVICES=0,2`, only GPUs 0 and 2 are instantiated. In this case, to access\n    GPU of HIP index 0, use the index 0, and for HIP index 2, use the index 1.\n\n    This class provides a 1:1 mapping between the methods and AMDSMI library functions. For example, if you want to do the following:\n\n    ```python\n    handle = amdsmi.amdsmi_get_processor_handles()[gpu_index]\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)\n    constraints = (info.min_power_cap, info.max_power_cap)\n    ```\n\n    You can now do:\n    ```python\n    gpus = get_gpus() # returns a AMDGPUs object\n    constraints =  gpus.getPowerManagementLimitConstraints(gpu_index)\n    ```\n\n    Note: This class instantiates (grabs the handle, by calling `amdsmi.amdsmi_get_processor_handles()`) all GPUs that are visible to the system, as determined by the `HIP_VISIBLE_DEVICES` environment variable if set.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Instantiates NVIDIAGPUs object, setting up tracking for specified NVIDIA GPUs.\n\n        Args:\n            ensure_homogeneous (bool, optional): If True, ensures that all tracked GPUs have the same name (return value of amdsmi.amdsmi_get_gpu_asic_info(handle).market_name). False by default.\n        \"\"\"\n        try:\n            amdsmi.amdsmi_init()\n            self._init_gpus()\n            if ensure_homogeneous:\n                self._ensure_homogeneous()\n        except amdsmi.AmdSmiException as e:\n            exception_class = AMDGPU._exception_map.get(\n                e.value, gpu_common.ZeusBaseGPUError\n            )\n            raise exception_class(e.msg) from e\n\n    @property\n    def gpus(self) -&gt; Sequence[gpu_common.GPU]:\n        \"\"\"Returns a list of AMDGPU objects being tracked.\"\"\"\n        return self._gpus\n\n    def _init_gpus(self) -&gt; None:\n        # Must respect `HIP_VISIBLE_DEVICES` if set\n        if (visible_device := os.environ.get(\"HIP_VISIBLE_DEVICES\")) is not None:\n            self.visible_indices = [int(idx) for idx in visible_device.split(\",\")]\n        else:\n            self.visible_indices = list(\n                range(len(amdsmi.amdsmi_get_processor_handles()))\n            )\n\n        self._gpus = [AMDGPU(gpu_num) for gpu_num in self.visible_indices]\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the AMD GPU monitoring library to release resources and clean up.\"\"\"\n        with contextlib.suppress(amdsmi.AmdSmiException):\n            amdsmi.amdsmi_shut_down()  # Ignore error on shutdown. Neccessary for proper cleanup and test functionality\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Returns a list of AMDGPU objects being tracked.</p>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name (return value of amdsmi.amdsmi_get_gpu_asic_info(handle).market_name). False by default.</p> <code>False</code> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Instantiates NVIDIAGPUs object, setting up tracking for specified NVIDIA GPUs.\n\n    Args:\n        ensure_homogeneous (bool, optional): If True, ensures that all tracked GPUs have the same name (return value of amdsmi.amdsmi_get_gpu_asic_info(handle).market_name). False by default.\n    \"\"\"\n    try:\n        amdsmi.amdsmi_init()\n        self._init_gpus()\n        if ensure_homogeneous:\n            self._ensure_homogeneous()\n    except amdsmi.AmdSmiException as e:\n        exception_class = AMDGPU._exception_map.get(\n            e.value, gpu_common.ZeusBaseGPUError\n        )\n        raise exception_class(e.msg) from e\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the AMD GPU monitoring library to release resources and clean up.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the AMD GPU monitoring library to release resources and clean up.\"\"\"\n    with contextlib.suppress(amdsmi.AmdSmiException):\n        amdsmi.amdsmi_shut_down()  # Ignore error on shutdown. Neccessary for proper cleanup and test functionality\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.amdsmi_is_available","title":"amdsmi_is_available","text":"<pre><code>amdsmi_is_available()\n</code></pre> <p>Check if amdsmi is available.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def amdsmi_is_available() -&gt; bool:\n    \"\"\"Check if amdsmi is available.\"\"\"\n    try:\n        import amdsmi  # type: ignore\n    except ImportError:\n        logger.info(\"amdsmi is not available.\")\n        return False\n    try:\n        amdsmi.amdsmi_init()\n        logger.info(\"amdsmi is available and initialized\")\n        return True\n    except amdsmi.AmdSmiLibraryException:\n        logger.info(\"amdsmi is available but could not initialize.\")\n        return False\n</code></pre>"},{"location":"reference/device/gpu/common/","title":"common","text":""},{"location":"reference/device/gpu/common/#zeus.device.gpu.common","title":"zeus.device.gpu.common","text":"<p>Error wrappers and classes common to all GPU vendors.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInitError","title":"ZeusGPUInitError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Import error or GPU library initialization failures.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInitError(ZeusBaseGPUError):\n    \"\"\"Import error or GPU library initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInvalidArgError","title":"ZeusGPUInvalidArgError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Invalid Argument.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInvalidArgError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Invalid Argument.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInvalidArgError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotSupportedError","title":"ZeusGPUNotSupportedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Not Supported Operation on GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotSupportedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Not Supported Operation on GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotSupportedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNoPermissionError","title":"ZeusGPUNoPermissionError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for No Permission to perform GPU operation.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNoPermissionError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for No Permission to perform GPU operation.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNoPermissionError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUAlreadyInitializedError","title":"ZeusGPUAlreadyInitializedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Already Initialized GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUAlreadyInitializedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Already Initialized GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUAlreadyInitializedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotFoundError","title":"ZeusGPUNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Not Found GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Not Found GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientSizeError","title":"ZeusGPUInsufficientSizeError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Insufficient Size.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientSizeError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Insufficient Size.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientSizeError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientPowerError","title":"ZeusGPUInsufficientPowerError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Insufficient Power.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientPowerError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Insufficient Power.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientPowerError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUDriverNotLoadedError","title":"ZeusGPUDriverNotLoadedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Driver Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUDriverNotLoadedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Driver Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUDriverNotLoadedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUTimeoutError","title":"ZeusGPUTimeoutError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Timeout Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUTimeoutError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Timeout Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUTimeoutError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUIRQError","title":"ZeusGPUIRQError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for IRQ Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUIRQError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for IRQ Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUIRQError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibraryNotFoundError","title":"ZeusGPULibraryNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Library Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibraryNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Library Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibraryNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUFunctionNotFoundError","title":"ZeusGPUFunctionNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Function Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUFunctionNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Function Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUFunctionNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUCorruptedInfoROMError","title":"ZeusGPUCorruptedInfoROMError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Corrupted Info ROM Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUCorruptedInfoROMError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Corrupted Info ROM Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUCorruptedInfoROMError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULostError","title":"ZeusGPULostError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Lost GPU Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULostError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Lost GPU Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULostError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUResetRequiredError","title":"ZeusGPUResetRequiredError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Reset Required Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUResetRequiredError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Reset Required Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUResetRequiredError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUOperatingSystemError","title":"ZeusGPUOperatingSystemError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Operating System Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUOperatingSystemError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Operating System Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUOperatingSystemError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibRMVersionMismatchError","title":"ZeusGPULibRMVersionMismatchError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for LibRM Version Mismatch Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibRMVersionMismatchError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for LibRM Version Mismatch Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibRMVersionMismatchError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUMemoryError","title":"ZeusGPUMemoryError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Insufficient Memory Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUMemoryError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Insufficient Memory Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUMemoryError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUUnknownError","title":"ZeusGPUUnknownError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception class Wrapper for Unknown Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUUnknownError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception class Wrapper for Unknown Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUUnknownError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU","title":"GPU","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for GPU management.</p> <p>This class defines the interface for interacting with GPUs, subclasses should implement the methods to interact with specific GPU libraries (e.g., NVML for NVIDIA GPUs).</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class GPU(abc.ABC):\n    \"\"\"Abstract base class for GPU management.\n\n    This class defines the interface for interacting with GPUs, subclasses should implement the methods to interact with specific GPU libraries\n    (e.g., NVML for NVIDIA GPUs).\n    \"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initialize the GPU with a specified index.\"\"\"\n        self.gpu_index = gpu_index\n\n    @abc.abstractmethod\n    def getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits for the GPU. Units: mW.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def setPersistenceMode(self, enable: bool) -&gt; None:\n        \"\"\"Enable persistence mode for the GPU.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def setPowerManagementLimit(self, value: int) -&gt; None:\n        \"\"\"Set the power management limit for the GPU to a specified value or default. Unit: mW.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def resetPowerManagementLimit(self) -&gt; None:\n        \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def getSupportedMemoryClocks(self) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies for the GPU. Units: MHz.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies for a given memory frequency. Units: MHz.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def getName(self) -&gt; str:\n        \"\"\"Return the name of the GPU.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def resetMemoryLockedClocks(self) -&gt; None:\n        \"\"\"Reset the memory locked clocks to default values.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def resetGpuLockedClocks(self) -&gt; None:\n        \"\"\"Reset the GPU locked clocks to default values.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def getInstantPowerUsage(self) -&gt; int:\n        \"\"\"Returns the current power usage of the GPU. Units: mW.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def supportsGetTotalEnergyConsumption(self) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def getTotalEnergyConsumption(self) -&gt; int:\n        \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initialize the GPU with a specified index.\"\"\"\n    self.gpu_index = gpu_index\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getPowerManagementLimitConstraints","title":"getPowerManagementLimitConstraints  <code>abstractmethod</code>","text":"<pre><code>getPowerManagementLimitConstraints()\n</code></pre> <p>Return the minimum and maximum power management limits for the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits for the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.setPersistenceMode","title":"setPersistenceMode  <code>abstractmethod</code>","text":"<pre><code>setPersistenceMode(enable)\n</code></pre> <p>Enable persistence mode for the GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef setPersistenceMode(self, enable: bool) -&gt; None:\n    \"\"\"Enable persistence mode for the GPU.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.setPowerManagementLimit","title":"setPowerManagementLimit  <code>abstractmethod</code>","text":"<pre><code>setPowerManagementLimit(value)\n</code></pre> <p>Set the power management limit for the GPU to a specified value or default. Unit: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef setPowerManagementLimit(self, value: int) -&gt; None:\n    \"\"\"Set the power management limit for the GPU to a specified value or default. Unit: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.resetPowerManagementLimit","title":"resetPowerManagementLimit  <code>abstractmethod</code>","text":"<pre><code>resetPowerManagementLimit()\n</code></pre> <p>Resets the power management limit for the specified GPU to the default value.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef resetPowerManagementLimit(self) -&gt; None:\n    \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.setMemoryLockedClocks","title":"setMemoryLockedClocks  <code>abstractmethod</code>","text":"<pre><code>setMemoryLockedClocks(minMemClockMHz, maxMemClockMHz)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getSupportedMemoryClocks","title":"getSupportedMemoryClocks  <code>abstractmethod</code>","text":"<pre><code>getSupportedMemoryClocks()\n</code></pre> <p>Return a list of supported memory clock frequencies for the GPU. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getSupportedMemoryClocks(self) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies for the GPU. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getSupportedGraphicsClocks","title":"getSupportedGraphicsClocks  <code>abstractmethod</code>","text":"<pre><code>getSupportedGraphicsClocks(freq)\n</code></pre> <p>Return a list of supported graphics clock frequencies for a given memory frequency. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies for a given memory frequency. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getName","title":"getName  <code>abstractmethod</code>","text":"<pre><code>getName()\n</code></pre> <p>Return the name of the GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getName(self) -&gt; str:\n    \"\"\"Return the name of the GPU.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.setGpuLockedClocks","title":"setGpuLockedClocks  <code>abstractmethod</code>","text":"<pre><code>setGpuLockedClocks(minGpuClockMHz, maxGpuClockMHz)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.resetMemoryLockedClocks","title":"resetMemoryLockedClocks  <code>abstractmethod</code>","text":"<pre><code>resetMemoryLockedClocks()\n</code></pre> <p>Reset the memory locked clocks to default values.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef resetMemoryLockedClocks(self) -&gt; None:\n    \"\"\"Reset the memory locked clocks to default values.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.resetGpuLockedClocks","title":"resetGpuLockedClocks  <code>abstractmethod</code>","text":"<pre><code>resetGpuLockedClocks()\n</code></pre> <p>Reset the GPU locked clocks to default values.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef resetGpuLockedClocks(self) -&gt; None:\n    \"\"\"Reset the GPU locked clocks to default values.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getInstantPowerUsage","title":"getInstantPowerUsage  <code>abstractmethod</code>","text":"<pre><code>getInstantPowerUsage()\n</code></pre> <p>Returns the current power usage of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getInstantPowerUsage(self) -&gt; int:\n    \"\"\"Returns the current power usage of the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.supportsGetTotalEnergyConsumption","title":"supportsGetTotalEnergyConsumption  <code>abstractmethod</code>","text":"<pre><code>supportsGetTotalEnergyConsumption()\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef supportsGetTotalEnergyConsumption(self) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.getTotalEnergyConsumption","title":"getTotalEnergyConsumption  <code>abstractmethod</code>","text":"<pre><code>getTotalEnergyConsumption()\n</code></pre> <p>Return the total energy consumption of the GPU since driver load. Units: mJ.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef getTotalEnergyConsumption(self) -&gt; int:\n    \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs","title":"GPUs","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for GPU manager object.</p> <p>This class defines the essential interface and common functionality for GPU management, instantiating multiple <code>GPU</code> objects for each GPU being tracked. Forwards the call for a specific method to the corresponding GPU object.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class GPUs(abc.ABC):\n    \"\"\"An abstract base class for GPU manager object.\n\n    This class defines the essential interface and common functionality for GPU management, instantiating multiple `GPU` objects for each GPU being tracked.\n    Forwards the call for a specific method to the corresponding GPU object.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initializes the GPU management library to communicate with the GPU driver and sets up tracking for specified GPUs.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the GPU monitoring library to release resources and clean up.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def gpus(self) -&gt; Sequence[GPU]:\n        \"\"\"Returns a list of GPU objects being tracked.\"\"\"\n        pass\n\n    def _ensure_homogeneous(self) -&gt; None:\n        \"\"\"Ensures that all tracked GPUs are homogeneous in terms of name.\"\"\"\n        gpu_names = [gpu.getName() for gpu in self.gpus]\n        # Both zero (no GPUs found) and one are fine.\n        if len(set(gpu_names)) &gt; 1:\n            raise ZeusBaseGPUError(f\"Heterogeneous GPUs found: {gpu_names}\")\n\n    def getPowerManagementLimitConstraints(self, index: int) -&gt; tuple[int, int]:\n        \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n        return self.gpus[index].getPowerManagementLimitConstraints()\n\n    def setPersistenceMode(self, index: int, enable: bool) -&gt; None:\n        \"\"\"Enables persistence mode for the specified GPU.\"\"\"\n        self.gpus[index].setPersistenceMode(enable)\n\n    def setPowerManagementLimit(self, index: int, value: int) -&gt; None:\n        \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n        self.gpus[index].setPowerManagementLimit(value)\n\n    def resetPowerManagementLimit(self, index: int) -&gt; None:\n        \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n        self.gpus[index].resetPowerManagementLimit()\n\n    def setMemoryLockedClocks(\n        self, index: int, minMemClockMHz: int, maxMemClockMHz: int\n    ) -&gt; None:\n        \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.\"\"\"\n        self.gpus[index].setMemoryLockedClocks(minMemClockMHz, maxMemClockMHz)\n\n    def getSupportedMemoryClocks(self, index: int) -&gt; list[int]:\n        \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n        return self.gpus[index].getSupportedMemoryClocks()\n\n    def getSupportedGraphicsClocks(self, index: int, freq: int) -&gt; list[int]:\n        \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n        return self.gpus[index].getSupportedGraphicsClocks(freq)\n\n    def getName(self, index: int) -&gt; str:\n        \"\"\"Returns the name of the specified GPU.\"\"\"\n        return self.gpus[index].getName()\n\n    def setGpuLockedClocks(\n        self, index: int, minGpuClockMHz: int, maxGpuClockMHz: int\n    ) -&gt; None:\n        \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.\"\"\"\n        self.gpus[index].setGpuLockedClocks(minGpuClockMHz, maxGpuClockMHz)\n\n    def resetMemoryLockedClocks(self, index: int) -&gt; None:\n        \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n        self.gpus[index].resetMemoryLockedClocks()\n\n    def resetGpuLockedClocks(self, index: int) -&gt; None:\n        \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n        self.gpus[index].resetGpuLockedClocks()\n\n    def getInstantPowerUsage(self, index: int) -&gt; int:\n        \"\"\"Returns the power usage of the specified GPU. Units: mW.\"\"\"\n        return self.gpus[index].getInstantPowerUsage()\n\n    def supportsGetTotalEnergyConsumption(self, index: int) -&gt; bool:\n        \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n        return self.gpus[index].supportsGetTotalEnergyConsumption()\n\n    def getTotalEnergyConsumption(self, index: int) -&gt; int:\n        \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n        return self.gpus[index].getTotalEnergyConsumption()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of GPUs being tracked.\"\"\"\n        return len(self.gpus)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.gpus","title":"gpus  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Returns a list of GPU objects being tracked.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__init__","title":"__init__  <code>abstractmethod</code>","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initializes the GPU management library to communicate with the GPU driver and sets up tracking for specified GPUs.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__del__","title":"__del__  <code>abstractmethod</code>","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the GPU monitoring library to release resources and clean up.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __del__(self) -&gt; None:\n    \"\"\"Shuts down the GPU monitoring library to release resources and clean up.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs._ensure_homogeneous","title":"_ensure_homogeneous","text":"<pre><code>_ensure_homogeneous()\n</code></pre> <p>Ensures that all tracked GPUs are homogeneous in terms of name.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _ensure_homogeneous(self) -&gt; None:\n    \"\"\"Ensures that all tracked GPUs are homogeneous in terms of name.\"\"\"\n    gpu_names = [gpu.getName() for gpu in self.gpus]\n    # Both zero (no GPUs found) and one are fine.\n    if len(set(gpu_names)) &gt; 1:\n        raise ZeusBaseGPUError(f\"Heterogeneous GPUs found: {gpu_names}\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getPowerManagementLimitConstraints","title":"getPowerManagementLimitConstraints","text":"<pre><code>getPowerManagementLimitConstraints(index)\n</code></pre> <p>Returns the minimum and maximum power management limits for the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getPowerManagementLimitConstraints(self, index: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n    return self.gpus[index].getPowerManagementLimitConstraints()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.setPersistenceMode","title":"setPersistenceMode","text":"<pre><code>setPersistenceMode(index, enable)\n</code></pre> <p>Enables persistence mode for the specified GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def setPersistenceMode(self, index: int, enable: bool) -&gt; None:\n    \"\"\"Enables persistence mode for the specified GPU.\"\"\"\n    self.gpus[index].setPersistenceMode(enable)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.setPowerManagementLimit","title":"setPowerManagementLimit","text":"<pre><code>setPowerManagementLimit(index, value)\n</code></pre> <p>Sets the power management limit for the specified GPU to the given value. Unit: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def setPowerManagementLimit(self, index: int, value: int) -&gt; None:\n    \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n    self.gpus[index].setPowerManagementLimit(value)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.resetPowerManagementLimit","title":"resetPowerManagementLimit","text":"<pre><code>resetPowerManagementLimit(index)\n</code></pre> <p>Resets the power management limit for the specified GPU to the default value.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def resetPowerManagementLimit(self, index: int) -&gt; None:\n    \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n    self.gpus[index].resetPowerManagementLimit()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.setMemoryLockedClocks","title":"setMemoryLockedClocks","text":"<pre><code>setMemoryLockedClocks(\n    index, minMemClockMHz, maxMemClockMHz\n)\n</code></pre> <p>Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def setMemoryLockedClocks(\n    self, index: int, minMemClockMHz: int, maxMemClockMHz: int\n) -&gt; None:\n    \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies. Units: MHz.\"\"\"\n    self.gpus[index].setMemoryLockedClocks(minMemClockMHz, maxMemClockMHz)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getSupportedMemoryClocks","title":"getSupportedMemoryClocks","text":"<pre><code>getSupportedMemoryClocks(index)\n</code></pre> <p>Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getSupportedMemoryClocks(self, index: int) -&gt; list[int]:\n    \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n    return self.gpus[index].getSupportedMemoryClocks()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getSupportedGraphicsClocks","title":"getSupportedGraphicsClocks","text":"<pre><code>getSupportedGraphicsClocks(index, freq)\n</code></pre> <p>Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getSupportedGraphicsClocks(self, index: int, freq: int) -&gt; list[int]:\n    \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n    return self.gpus[index].getSupportedGraphicsClocks(freq)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getName","title":"getName","text":"<pre><code>getName(index)\n</code></pre> <p>Returns the name of the specified GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getName(self, index: int) -&gt; str:\n    \"\"\"Returns the name of the specified GPU.\"\"\"\n    return self.gpus[index].getName()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.setGpuLockedClocks","title":"setGpuLockedClocks","text":"<pre><code>setGpuLockedClocks(index, minGpuClockMHz, maxGpuClockMHz)\n</code></pre> <p>Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def setGpuLockedClocks(\n    self, index: int, minGpuClockMHz: int, maxGpuClockMHz: int\n) -&gt; None:\n    \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.\"\"\"\n    self.gpus[index].setGpuLockedClocks(minGpuClockMHz, maxGpuClockMHz)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.resetMemoryLockedClocks","title":"resetMemoryLockedClocks","text":"<pre><code>resetMemoryLockedClocks(index)\n</code></pre> <p>Resets the memory locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def resetMemoryLockedClocks(self, index: int) -&gt; None:\n    \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n    self.gpus[index].resetMemoryLockedClocks()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.resetGpuLockedClocks","title":"resetGpuLockedClocks","text":"<pre><code>resetGpuLockedClocks(index)\n</code></pre> <p>Resets the GPU locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def resetGpuLockedClocks(self, index: int) -&gt; None:\n    \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n    self.gpus[index].resetGpuLockedClocks()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getInstantPowerUsage","title":"getInstantPowerUsage","text":"<pre><code>getInstantPowerUsage(index)\n</code></pre> <p>Returns the power usage of the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getInstantPowerUsage(self, index: int) -&gt; int:\n    \"\"\"Returns the power usage of the specified GPU. Units: mW.\"\"\"\n    return self.gpus[index].getInstantPowerUsage()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.supportsGetTotalEnergyConsumption","title":"supportsGetTotalEnergyConsumption","text":"<pre><code>supportsGetTotalEnergyConsumption(index)\n</code></pre> <p>Returns True if the specified GPU supports retrieving the total energy consumption.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def supportsGetTotalEnergyConsumption(self, index: int) -&gt; bool:\n    \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n    return self.gpus[index].supportsGetTotalEnergyConsumption()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.getTotalEnergyConsumption","title":"getTotalEnergyConsumption","text":"<pre><code>getTotalEnergyConsumption(index)\n</code></pre> <p>Returns the total energy consumption of the specified GPU. Units: mJ.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def getTotalEnergyConsumption(self, index: int) -&gt; int:\n    \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n    return self.gpus[index].getTotalEnergyConsumption()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Returns the number of GPUs being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of GPUs being tracked.\"\"\"\n    return len(self.gpus)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/","title":"nvidia","text":""},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia","title":"zeus.device.gpu.nvidia","text":"<p>NVIDIA GPUs.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU","title":"NVIDIAGPU","text":"<p>               Bases: <code>GPU</code></p> <p>Control a Single NVIDIA GPU.</p> <p>Uses NVML Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the NVML library functions. Zeus GPU Exceptions are raised when NVML errors occur. To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to NVML.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class NVIDIAGPU(gpu_common.GPU):\n    \"\"\"Control a Single NVIDIA GPU.\n\n    Uses NVML Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the NVML library functions.\n    Zeus GPU Exceptions are raised when NVML errors occur.\n    To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to NVML.\n    \"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initializes the NVIDIAGPU object with a specified GPU index. Acquires a handle to the GPU using `pynvml.nvmlDeviceGetHandleByIndex`.\"\"\"\n        super().__init__(gpu_index)\n        self._get_handle()\n        self._supportsGetTotalEnergyConsumption = None\n\n    _exception_map = {\n        pynvml.NVML_ERROR_UNINITIALIZED: gpu_common.ZeusGPUInitError,\n        pynvml.NVML_ERROR_INVALID_ARGUMENT: gpu_common.ZeusGPUInvalidArgError,\n        pynvml.NVML_ERROR_NOT_SUPPORTED: gpu_common.ZeusGPUNotSupportedError,\n        pynvml.NVML_ERROR_NO_PERMISSION: gpu_common.ZeusGPUNoPermissionError,\n        pynvml.NVML_ERROR_ALREADY_INITIALIZED: gpu_common.ZeusGPUAlreadyInitializedError,\n        pynvml.NVML_ERROR_NOT_FOUND: gpu_common.ZeusGPUNotFoundError,\n        pynvml.NVML_ERROR_INSUFFICIENT_SIZE: gpu_common.ZeusGPUInsufficientSizeError,\n        pynvml.NVML_ERROR_INSUFFICIENT_POWER: gpu_common.ZeusGPUInsufficientPowerError,\n        pynvml.NVML_ERROR_DRIVER_NOT_LOADED: gpu_common.ZeusGPUDriverNotLoadedError,\n        pynvml.NVML_ERROR_TIMEOUT: gpu_common.ZeusGPUTimeoutError,\n        pynvml.NVML_ERROR_IRQ_ISSUE: gpu_common.ZeusGPUIRQError,\n        pynvml.NVML_ERROR_LIBRARY_NOT_FOUND: gpu_common.ZeusGPULibraryNotFoundError,\n        pynvml.NVML_ERROR_FUNCTION_NOT_FOUND: gpu_common.ZeusGPUFunctionNotFoundError,\n        pynvml.NVML_ERROR_CORRUPTED_INFOROM: gpu_common.ZeusGPUCorruptedInfoROMError,\n        pynvml.NVML_ERROR_GPU_IS_LOST: gpu_common.ZeusGPULostError,\n        pynvml.NVML_ERROR_RESET_REQUIRED: gpu_common.ZeusGPUResetRequiredError,\n        pynvml.NVML_ERROR_OPERATING_SYSTEM: gpu_common.ZeusGPUOperatingSystemError,\n        pynvml.NVML_ERROR_LIB_RM_VERSION_MISMATCH: gpu_common.ZeusGPULibRMVersionMismatchError,\n        pynvml.NVML_ERROR_MEMORY: gpu_common.ZeusGPUMemoryError,\n        pynvml.NVML_ERROR_UNKNOWN: gpu_common.ZeusGPUUnknownError,\n    }\n\n    @_handle_nvml_errors\n    def _get_handle(self):\n        self.handle = pynvml.nvmlDeviceGetHandleByIndex(self.gpu_index)\n\n    @_handle_nvml_errors\n    def getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n        \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n        min_, max_ = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(self.handle)\n        return (min_, max_)\n\n    @_handle_nvml_errors\n    def setPersistenceMode(self, enable: bool) -&gt; None:\n        \"\"\"If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.\"\"\"\n        if enable:\n            pynvml.nvmlDeviceSetPersistenceMode(\n                self.handle, pynvml.NVML_FEATURE_ENABLED\n            )\n        else:\n            pynvml.nvmlDeviceSetPersistenceMode(\n                self.handle, pynvml.NVML_FEATURE_DISABLED\n            )\n\n    @_handle_nvml_errors\n    def setPowerManagementLimit(self, value: int) -&gt; None:\n        \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n        pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, value)\n\n    @_handle_nvml_errors\n    def resetPowerManagementLimit(self) -&gt; None:\n        \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n        pynvml.nvmlDeviceSetPowerManagementLimit(\n            self.handle,\n            pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle),\n        )\n\n    @_handle_nvml_errors\n    def setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n        \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies.  Units: MHz.\"\"\"\n        pynvml.nvmlDeviceSetMemoryLockedClocks(\n            self.handle, minMemClockMHz, maxMemClockMHz\n        )\n\n    @_handle_nvml_errors\n    def getSupportedMemoryClocks(self) -&gt; list[int]:\n        \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n        return pynvml.nvmlDeviceGetSupportedMemoryClocks(self.handle)\n\n    @_handle_nvml_errors\n    def getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n        \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n        return pynvml.nvmlDeviceGetSupportedGraphicsClocks(self.handle, freq)\n\n    @_handle_nvml_errors\n    def getName(self) -&gt; str:\n        \"\"\"Returns the name of the specified GPU.\"\"\"\n        return pynvml.nvmlDeviceGetName(self.handle)\n\n    @_handle_nvml_errors\n    def setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n        \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.\"\"\"\n        pynvml.nvmlDeviceSetGpuLockedClocks(self.handle, minGpuClockMHz, maxGpuClockMHz)\n\n    @_handle_nvml_errors\n    def resetMemoryLockedClocks(self) -&gt; None:\n        \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n        pynvml.nvmlDeviceResetMemoryLockedClocks(self.handle)\n\n    @_handle_nvml_errors\n    def resetGpuLockedClocks(self) -&gt; None:\n        \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n        pynvml.nvmlDeviceResetGpuLockedClocks(self.handle)\n\n    @_handle_nvml_errors\n    def getInstantPowerUsage(self) -&gt; int:\n        \"\"\"Returns the current power usage of the specified GPU. Units: mW.\"\"\"\n        metric = pynvml.nvmlDeviceGetFieldValues(\n            self.handle, [pynvml.NVML_FI_DEV_POWER_INSTANT]\n        )[0]\n        if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n            raise pynvml.NVMLError(ret)\n        return metric.value.siVal\n\n    @_handle_nvml_errors\n    def supportsGetTotalEnergyConsumption(self) -&gt; bool:\n        \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n        # NVIDIA GPUs Volta or newer support this method\n        if self._supportsGetTotalEnergyConsumption is None:\n            self._supportsGetTotalEnergyConsumption = (\n                pynvml.nvmlDeviceGetArchitecture(self.handle)\n                &gt;= pynvml.NVML_DEVICE_ARCH_VOLTA\n            )\n\n        return self._supportsGetTotalEnergyConsumption\n\n    @_handle_nvml_errors\n    def getTotalEnergyConsumption(self) -&gt; int:\n        \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n        return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initializes the NVIDIAGPU object with a specified GPU index. Acquires a handle to the GPU using `pynvml.nvmlDeviceGetHandleByIndex`.\"\"\"\n    super().__init__(gpu_index)\n    self._get_handle()\n    self._supportsGetTotalEnergyConsumption = None\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getPowerManagementLimitConstraints","title":"getPowerManagementLimitConstraints","text":"<pre><code>getPowerManagementLimitConstraints()\n</code></pre> <p>Returns the minimum and maximum power management limits for the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getPowerManagementLimitConstraints(self) -&gt; tuple[int, int]:\n    \"\"\"Returns the minimum and maximum power management limits for the specified GPU. Units: mW.\"\"\"\n    min_, max_ = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(self.handle)\n    return (min_, max_)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.setPersistenceMode","title":"setPersistenceMode","text":"<pre><code>setPersistenceMode(enable)\n</code></pre> <p>If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef setPersistenceMode(self, enable: bool) -&gt; None:\n    \"\"\"If enable = True, enables persistence mode for the specified GPU. If enable = False, disables persistence mode.\"\"\"\n    if enable:\n        pynvml.nvmlDeviceSetPersistenceMode(\n            self.handle, pynvml.NVML_FEATURE_ENABLED\n        )\n    else:\n        pynvml.nvmlDeviceSetPersistenceMode(\n            self.handle, pynvml.NVML_FEATURE_DISABLED\n        )\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.setPowerManagementLimit","title":"setPowerManagementLimit","text":"<pre><code>setPowerManagementLimit(value)\n</code></pre> <p>Sets the power management limit for the specified GPU to the given value. Unit: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef setPowerManagementLimit(self, value: int) -&gt; None:\n    \"\"\"Sets the power management limit for the specified GPU to the given value. Unit: mW.\"\"\"\n    pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, value)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.resetPowerManagementLimit","title":"resetPowerManagementLimit","text":"<pre><code>resetPowerManagementLimit()\n</code></pre> <p>Resets the power management limit for the specified GPU to the default value.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef resetPowerManagementLimit(self) -&gt; None:\n    \"\"\"Resets the power management limit for the specified GPU to the default value.\"\"\"\n    pynvml.nvmlDeviceSetPowerManagementLimit(\n        self.handle,\n        pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle),\n    )\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.setMemoryLockedClocks","title":"setMemoryLockedClocks","text":"<pre><code>setMemoryLockedClocks(minMemClockMHz, maxMemClockMHz)\n</code></pre> <p>Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies.  Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef setMemoryLockedClocks(self, minMemClockMHz: int, maxMemClockMHz: int) -&gt; None:\n    \"\"\"Locks the memory clock of the specified GPU to a range defined by the minimum and maximum memory clock frequencies.  Units: MHz.\"\"\"\n    pynvml.nvmlDeviceSetMemoryLockedClocks(\n        self.handle, minMemClockMHz, maxMemClockMHz\n    )\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getSupportedMemoryClocks","title":"getSupportedMemoryClocks","text":"<pre><code>getSupportedMemoryClocks()\n</code></pre> <p>Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getSupportedMemoryClocks(self) -&gt; list[int]:\n    \"\"\"Returns a list of supported memory clock frequencies for the specified GPU. Units: MHz.\"\"\"\n    return pynvml.nvmlDeviceGetSupportedMemoryClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getSupportedGraphicsClocks","title":"getSupportedGraphicsClocks","text":"<pre><code>getSupportedGraphicsClocks(freq)\n</code></pre> <p>Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getSupportedGraphicsClocks(self, freq: int) -&gt; list[int]:\n    \"\"\"Returns a list of supported graphics clock frequencies for the specified GPU at a given frequency. Units: MHz.\"\"\"\n    return pynvml.nvmlDeviceGetSupportedGraphicsClocks(self.handle, freq)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getName","title":"getName","text":"<pre><code>getName()\n</code></pre> <p>Returns the name of the specified GPU.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getName(self) -&gt; str:\n    \"\"\"Returns the name of the specified GPU.\"\"\"\n    return pynvml.nvmlDeviceGetName(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.setGpuLockedClocks","title":"setGpuLockedClocks","text":"<pre><code>setGpuLockedClocks(minGpuClockMHz, maxGpuClockMHz)\n</code></pre> <p>Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef setGpuLockedClocks(self, minGpuClockMHz: int, maxGpuClockMHz: int) -&gt; None:\n    \"\"\"Locks the GPU clock of the specified GPU to a range defined by the minimum and maximum GPU clock frequencies. Units: MHz.\"\"\"\n    pynvml.nvmlDeviceSetGpuLockedClocks(self.handle, minGpuClockMHz, maxGpuClockMHz)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.resetMemoryLockedClocks","title":"resetMemoryLockedClocks","text":"<pre><code>resetMemoryLockedClocks()\n</code></pre> <p>Resets the memory locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef resetMemoryLockedClocks(self) -&gt; None:\n    \"\"\"Resets the memory locked clocks of the specified GPU to their default values.\"\"\"\n    pynvml.nvmlDeviceResetMemoryLockedClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.resetGpuLockedClocks","title":"resetGpuLockedClocks","text":"<pre><code>resetGpuLockedClocks()\n</code></pre> <p>Resets the GPU locked clocks of the specified GPU to their default values.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef resetGpuLockedClocks(self) -&gt; None:\n    \"\"\"Resets the GPU locked clocks of the specified GPU to their default values.\"\"\"\n    pynvml.nvmlDeviceResetGpuLockedClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getInstantPowerUsage","title":"getInstantPowerUsage","text":"<pre><code>getInstantPowerUsage()\n</code></pre> <p>Returns the current power usage of the specified GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getInstantPowerUsage(self) -&gt; int:\n    \"\"\"Returns the current power usage of the specified GPU. Units: mW.\"\"\"\n    metric = pynvml.nvmlDeviceGetFieldValues(\n        self.handle, [pynvml.NVML_FI_DEV_POWER_INSTANT]\n    )[0]\n    if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n        raise pynvml.NVMLError(ret)\n    return metric.value.siVal\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.supportsGetTotalEnergyConsumption","title":"supportsGetTotalEnergyConsumption","text":"<pre><code>supportsGetTotalEnergyConsumption()\n</code></pre> <p>Returns True if the specified GPU supports retrieving the total energy consumption.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef supportsGetTotalEnergyConsumption(self) -&gt; bool:\n    \"\"\"Returns True if the specified GPU supports retrieving the total energy consumption.\"\"\"\n    # NVIDIA GPUs Volta or newer support this method\n    if self._supportsGetTotalEnergyConsumption is None:\n        self._supportsGetTotalEnergyConsumption = (\n            pynvml.nvmlDeviceGetArchitecture(self.handle)\n            &gt;= pynvml.NVML_DEVICE_ARCH_VOLTA\n        )\n\n    return self._supportsGetTotalEnergyConsumption\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.getTotalEnergyConsumption","title":"getTotalEnergyConsumption","text":"<pre><code>getTotalEnergyConsumption()\n</code></pre> <p>Returns the total energy consumption of the specified GPU. Units: mJ.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef getTotalEnergyConsumption(self) -&gt; int:\n    \"\"\"Returns the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.UnprivilegedNVIDIAGPU","title":"UnprivilegedNVIDIAGPU","text":"<p>               Bases: <code>NVIDIAGPU</code></p> <p>Control a Single NVIDIA GPU with no SYS_ADMIN privileges.</p> <p>Uses NVML Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the NVML library functions. Zeus GPU Exceptions are raised when NVML errors occur. To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to NVML.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class UnprivilegedNVIDIAGPU(NVIDIAGPU):\n    \"\"\"Control a Single NVIDIA GPU with no SYS_ADMIN privileges.\n\n    Uses NVML Library to control and query GPU. There is a 1:1 mapping between the methods in this class and the NVML library functions.\n    Zeus GPU Exceptions are raised when NVML errors occur.\n    To ensure computational efficiency, this class utilizes caching (ex. saves the handle) to avoid repeated calls to NVML.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs","title":"NVIDIAGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>NVIDIA GPU Manager object, containing individual NVIDIAGPU objects, abstracting pyNVML calls and handling related exceptions.</p> <p>This class provides a high-level interface to interact with NVIDIA GPUs. <code>CUDA_VISIBLE_DEVICES</code> environment variable is respected if set. For example, if there are 4 GPUs and <code>CUDA_VISIBLE_DEVICES=0,2</code>, only GPUs 0 and 2 are instantiated. In this case, to access GPU of CUDA index 0, use the index 0, and for CUDA index 2, use the index 1.</p> <p>This class provides a 1:1 mapping between the methods and NVML library functions. For example, if you want to do the following:</p> <pre><code>handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\nconstraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(handle)\n</code></pre> <p>You can now do: <pre><code>gpus = get_gpus() # returns a NVIDIAGPUs object\nconstraints =  gpus.getPowerManagementLimitConstraints(gpu_index)\n</code></pre></p> <p>Note: This class instantiates (grabs the handle, by calling <code>pynvml.nvmlDeviceGetHandleByIndex</code>) all GPUs that are visible to the system, as determined by the <code>CUDA_VISIBLE_DEVICES</code> environment variable if set.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class NVIDIAGPUs(gpu_common.GPUs):\n    \"\"\"NVIDIA GPU Manager object, containing individual NVIDIAGPU objects, abstracting pyNVML calls and handling related exceptions.\n\n    This class provides a high-level interface to interact with NVIDIA GPUs. `CUDA_VISIBLE_DEVICES` environment variable is respected if set. For example, if there are\n    4 GPUs and `CUDA_VISIBLE_DEVICES=0,2`, only GPUs 0 and 2 are instantiated. In this case, to access\n    GPU of CUDA index 0, use the index 0, and for CUDA index 2, use the index 1.\n\n    This class provides a 1:1 mapping between the methods and NVML library functions. For example, if you want to do the following:\n\n    ```python\n    handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n    constraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(handle)\n    ```\n\n    You can now do:\n    ```python\n    gpus = get_gpus() # returns a NVIDIAGPUs object\n    constraints =  gpus.getPowerManagementLimitConstraints(gpu_index)\n    ```\n\n    Note: This class instantiates (grabs the handle, by calling `pynvml.nvmlDeviceGetHandleByIndex`) all GPUs that are visible to the system, as determined by the `CUDA_VISIBLE_DEVICES` environment variable if set.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Instantiates NVIDIAGPUs object, setting up tracking for specified NVIDIA GPUs.\n\n        Args:\n            ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name (return value of `nvmlDeviceGetName`). False by default.\n        \"\"\"\n        try:\n            pynvml.nvmlInit()\n            self._init_gpus()\n            if ensure_homogeneous:\n                self._ensure_homogeneous()\n        except pynvml.NVMLError as e:\n            exception_class = NVIDIAGPU._exception_map.get(\n                e.value,  # pyright: ignore[reportAttributeAccessIssue]\n                gpu_common.ZeusBaseGPUError,\n            )\n            raise exception_class(\n                e.msg  # pyright: ignore[reportAttributeAccessIssue]\n            ) from e\n\n    @property\n    def gpus(self) -&gt; Sequence[gpu_common.GPU]:\n        \"\"\"Returns a list of NVIDIAGPU objects being tracked.\"\"\"\n        return self._gpus\n\n    def _init_gpus(self) -&gt; None:\n        # Must respect `CUDA_VISIBLE_DEVICES` if set\n        if (visible_device := os.environ.get(\"CUDA_VISIBLE_DEVICES\")) is not None:\n            self.visible_indices = [int(idx) for idx in visible_device.split(\",\")]\n        else:\n            self.visible_indices = list(range(pynvml.nvmlDeviceGetCount()))\n\n        # initialize all GPUs\n        self._gpus = [NVIDIAGPU(gpu_num) for gpu_num in self.visible_indices]\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the NVIDIA GPU monitoring library to release resources and clean up.\"\"\"\n        with contextlib.suppress(pynvml.NVMLError):\n            pynvml.nvmlShutdown()\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Returns a list of NVIDIAGPU objects being tracked.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name (return value of <code>nvmlDeviceGetName</code>). False by default.</p> <code>False</code> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Instantiates NVIDIAGPUs object, setting up tracking for specified NVIDIA GPUs.\n\n    Args:\n        ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name (return value of `nvmlDeviceGetName`). False by default.\n    \"\"\"\n    try:\n        pynvml.nvmlInit()\n        self._init_gpus()\n        if ensure_homogeneous:\n            self._ensure_homogeneous()\n    except pynvml.NVMLError as e:\n        exception_class = NVIDIAGPU._exception_map.get(\n            e.value,  # pyright: ignore[reportAttributeAccessIssue]\n            gpu_common.ZeusBaseGPUError,\n        )\n        raise exception_class(\n            e.msg  # pyright: ignore[reportAttributeAccessIssue]\n        ) from e\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the NVIDIA GPU monitoring library to release resources and clean up.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the NVIDIA GPU monitoring library to release resources and clean up.\"\"\"\n    with contextlib.suppress(pynvml.NVMLError):\n        pynvml.nvmlShutdown()\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.nvml_is_available","title":"nvml_is_available","text":"<pre><code>nvml_is_available()\n</code></pre> <p>Check if NVML is available.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def nvml_is_available() -&gt; bool:\n    \"\"\"Check if NVML is available.\"\"\"\n    try:\n        import pynvml\n    except ImportError:\n        logger.info(\n            \"Failed to import `pynvml`. Make sure you have package `nvidia-ml-py` installed.\"\n        )\n        return False\n\n    # Detect unofficial pynvml packages.\n    # If detected, this should be a critical error.\n    if not hasattr(pynvml, \"_nvmlGetFunctionPointer\"):\n        logger.error(\"Unoffical pynvml package detected!\")\n        raise ImportError(\n            \"Unofficial pynvml package detected! \"\n            \"This causes conflicts with the official NVIDIA bindings. \"\n            \"Please remove with `pip uninstall pynvml` and instead use the official \"\n            \"bindings from NVIDIA: `nvidia-ml-py`. \"\n        )\n\n    try:\n        pynvml.nvmlInit()\n        logger.info(\"pynvml is available and initialized.\")\n        return True\n    except pynvml.NVMLError:\n        logger.info(\"pynvml is available but could not initialize.\")\n        return False\n</code></pre>"},{"location":"reference/monitor/","title":"monitor","text":""},{"location":"reference/monitor/#zeus.monitor","title":"zeus.monitor","text":"<p>Time, energy, and power monitors for Zeus.</p> <p>The main class of this module is <code>ZeusMonitor</code>.</p> <p>If users wish to monitor power consumption over time, the <code>power</code> module can come in handy.</p>"},{"location":"reference/monitor/energy/","title":"energy","text":""},{"location":"reference/monitor/energy/#zeus.monitor.energy","title":"zeus.monitor.energy","text":"<p>Measure the GPU time and energy consumption of a block of code.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.Measurement","title":"Measurement  <code>dataclass</code>","text":"<p>Measurement result of one window.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>Time elapsed (in seconds) during the measurement window.</p> <code>energy</code> <code>dict[int, float]</code> <p>Maps GPU indices to the energy consumed (in Joules) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>@dataclass\nclass Measurement:\n    \"\"\"Measurement result of one window.\n\n    Attributes:\n        time: Time elapsed (in seconds) during the measurement window.\n        energy: Maps GPU indices to the energy consumed (in Joules) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n    \"\"\"\n\n    time: float\n    energy: dict[int, float]\n\n    @cached_property\n    def total_energy(self) -&gt; float:\n        \"\"\"Total energy consumed (in Joules) during the measurement window.\"\"\"\n        return sum(self.energy.values())\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.Measurement.total_energy","title":"total_energy  <code>cached</code> <code>property</code>","text":"<pre><code>total_energy\n</code></pre> <p>Total energy consumed (in Joules) during the measurement window.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor","title":"ZeusMonitor","text":"<p>Measure the GPU energy and time consumption of a block of code.</p> <p>Works for multi-GPU and heterogeneous GPU types. Aware of <code>CUDA_VISIBLE_DEVICES</code>. For instance, if <code>CUDA_VISIBLE_DEVICES=2,3</code>, GPU index <code>1</code> passed into <code>gpu_indices</code> will be interpreted as CUDA device <code>3</code>.</p> <p>You can mark the beginning and end of a measurement window, during which the GPU energy and time consumed will be recorded. Multiple concurrent measurement windows are supported.</p> <p>For Volta or newer GPUs, energy consumption is measured very cheaply with the <code>nvmlDeviceGetTotalEnergyConsumption</code> API. On older architectures, this API is not supported, so a separate Python process is used to poll <code>nvmlDeviceGetPowerUsage</code> to get power samples over time, which are integrated to compute energy consumption.</p> <p>Warning</p> <p>Since the monitor may spawn a process to poll the power API on GPUs older than Volta, the monitor should not be instantiated as a global variable without guarding it with <code>if __name__ == \"__main__\"</code>. Refer to the \"Safe importing of main module\" section in the Python documentation for more details.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor--integration-example","title":"Integration Example","text":"<pre><code>import time\nfrom zeus.monitor import ZeusMonitor\n\ndef training():\n    # A dummy training function\n    time.sleep(5)\n\nif __name__ == \"__main__\":\n    # Time/Energy measurements for four GPUs will begin and end at the same time.\n    gpu_indices = [0, 1, 2, 3]\n    monitor = ZeusMonitor(gpu_indices)\n\n    # Mark the beginning of a measurement window. You can use any string\n    # as the window name, but make sure it's unique.\n    monitor.begin_window(\"entire_training\")\n\n    # Actual work\n    training()\n\n    # Mark the end of a measurement window and retrieve the measurment result.\n    result = monitor.end_window(\"entire_training\")\n\n    # Print the measurement result.\n    print(f\"Training took {result.time} seconds.\")\n    print(f\"Training consumed {result.total_energy} Joules.\")\n    for gpu_idx, gpu_energy in result.energy.items():\n        print(f\"GPU {gpu_idx} consumed {gpu_energy} Joules.\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>gpu_indices</code> <code>`list[int]`</code> <p>Indices of all the CUDA devices to monitor, from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>class ZeusMonitor:\n    \"\"\"Measure the GPU energy and time consumption of a block of code.\n\n    Works for multi-GPU and heterogeneous GPU types. Aware of `CUDA_VISIBLE_DEVICES`.\n    For instance, if `CUDA_VISIBLE_DEVICES=2,3`, GPU index `1` passed into `gpu_indices`\n    will be interpreted as CUDA device `3`.\n\n    You can mark the beginning and end of a measurement window, during which the GPU\n    energy and time consumed will be recorded. Multiple concurrent measurement windows\n    are supported.\n\n    For Volta or newer GPUs, energy consumption is measured very cheaply with the\n    `nvmlDeviceGetTotalEnergyConsumption` API. On older architectures, this API is\n    not supported, so a separate Python process is used to poll `nvmlDeviceGetPowerUsage`\n    to get power samples over time, which are integrated to compute energy consumption.\n\n    !!! Warning\n        Since the monitor may spawn a process to poll the power API on GPUs older than\n        Volta, **the monitor should not be instantiated as a global variable\n        without guarding it with `if __name__ == \"__main__\"`**.\n        Refer to the \"Safe importing of main module\" section in the\n        [Python documentation](https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods)\n        for more details.\n\n    ## Integration Example\n\n    ```python\n    import time\n    from zeus.monitor import ZeusMonitor\n\n    def training():\n        # A dummy training function\n        time.sleep(5)\n\n    if __name__ == \"__main__\":\n        # Time/Energy measurements for four GPUs will begin and end at the same time.\n        gpu_indices = [0, 1, 2, 3]\n        monitor = ZeusMonitor(gpu_indices)\n\n        # Mark the beginning of a measurement window. You can use any string\n        # as the window name, but make sure it's unique.\n        monitor.begin_window(\"entire_training\")\n\n        # Actual work\n        training()\n\n        # Mark the end of a measurement window and retrieve the measurment result.\n        result = monitor.end_window(\"entire_training\")\n\n        # Print the measurement result.\n        print(f\"Training took {result.time} seconds.\")\n        print(f\"Training consumed {result.total_energy} Joules.\")\n        for gpu_idx, gpu_energy in result.energy.items():\n            print(f\"GPU {gpu_idx} consumed {gpu_energy} Joules.\")\n    ```\n\n    Attributes:\n        gpu_indices (`list[int]`): Indices of all the CUDA devices to monitor, from the\n            DL framework's perspective after applying `CUDA_VISIBLE_DEVICES`.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        approx_instant_energy: bool = False,\n        log_file: str | Path | None = None,\n    ) -&gt; None:\n        \"\"\"Instantiate the monitor.\n\n        Args:\n            gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n                will begin and end at the same time for these GPUs (i.e., synchronized).\n                If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n                is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n                `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n                `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n            approx_instant_energy: When the execution time of a measurement window is\n                shorter than the NVML energy counter's update period, energy consumption may\n                be observed as zero. In this case, if `approx_instant_energy` is True, the\n                window's energy consumption will be approximated by multiplying the current\n                instantaneous power consumption with the window's execution time. This should\n                be a better estimate than zero, but it's still an approximation.\n            log_file: Path to the log CSV file. If `None`, logging will be disabled.\n        \"\"\"\n        # Save arguments.\n        self.approx_instant_energy = approx_instant_energy\n\n        # Get gpus\n        self.gpus = get_gpus()\n\n        # Get GPU indices:\n        self.gpu_indices = (\n            gpu_indices if gpu_indices is not None else list(range(len(self.gpus)))\n        )\n\n        # Initialize loggers.\n        if log_file is None:\n            self.log_file = None\n        else:\n            if dir := os.path.dirname(log_file):\n                os.makedirs(dir, exist_ok=True)\n            self.log_file = open(log_file, \"w\")\n            logger.info(\"Writing measurement logs to %s.\", log_file)\n            self.log_file.write(\n                f\"start_time,window_name,elapsed_time,{','.join(map(lambda i: f'gpu{i}_energy', self.gpu_indices))}\\n\",\n            )\n            self.log_file.flush()\n\n        logger.info(\"Monitoring GPU indices %s.\", self.gpu_indices)\n\n        # A dictionary that maps the string keys of active measurement windows to\n        # the state of the measurement window. Each element in the dictionary is a tuple of:\n        #     1) Time elapsed at the beginning of this window.\n        #     2) Total energy consumed by each &gt;= Volta GPU at the beginning of\n        #        this window (`None` for older GPUs).\n        self.measurement_states: dict[str, tuple[float, dict[int, float]]] = {}\n\n        # Initialize power monitors for older architecture GPUs.\n        old_gpu_indices = [\n            gpu_index\n            for gpu_index in self.gpu_indices\n            if not self.gpus.supportsGetTotalEnergyConsumption(gpu_index)\n        ]\n        if old_gpu_indices:\n            self.power_monitor = PowerMonitor(\n                gpu_indices=old_gpu_indices, update_period=None\n            )\n        else:\n            self.power_monitor = None\n\n    def _get_instant_power(self) -&gt; tuple[dict[int, float], float]:\n        \"\"\"Measure the power consumption of all GPUs at the current time.\"\"\"\n        power_measurement_start_time: float = time()\n        power = {\n            i: self.gpus.getInstantPowerUsage(i) / 1000.0 for i in self.gpu_indices\n        }\n        power_measurement_time = time() - power_measurement_start_time\n        return power, power_measurement_time\n\n    def begin_window(self, key: str, sync_cuda: bool = True) -&gt; None:\n        \"\"\"Begin a new measurement window.\n\n        Args:\n            key: Unique name of the measurement window.\n            sync_cuda: Whether to synchronize CUDA before starting the measurement window.\n                (Default: `True`)\n        \"\"\"\n        # Make sure the key is unique.\n        if key in self.measurement_states:\n            raise ValueError(f\"Measurement window '{key}' already exists\")\n\n        # Call cudaSynchronize to make sure we freeze at the right time.\n        if sync_cuda:\n            for gpu_index in self.gpu_indices:\n                cuda_sync(gpu_index)\n\n        # Freeze the start time of the profiling window.\n        timestamp: float = time()\n        energy_state: dict[int, float] = {}\n        for gpu_index in self.gpu_indices:\n            # Query energy directly if the GPU has newer architecture.\n            # Otherwise, the Zeus power monitor is running in the background to\n            # collect power consumption, so we just need to read the log file later.\n            if self.gpus.supportsGetTotalEnergyConsumption(gpu_index):\n                energy_state[gpu_index] = (\n                    self.gpus.getTotalEnergyConsumption(gpu_index) / 1000.0\n                )\n\n        # Add measurement state to dictionary.\n        self.measurement_states[key] = (timestamp, energy_state)\n        logger.debug(\"Measurement window '%s' started.\", key)\n\n    def end_window(\n        self, key: str, sync_cuda: bool = True, cancel: bool = False\n    ) -&gt; Measurement:\n        \"\"\"End a measurement window and return the time and energy consumption.\n\n        Args:\n            key: Name of an active measurement window.\n            sync_cuda: Whether to synchronize CUDA before ending the measurement window.\n                (default: `True`)\n            cancel: Whether to cancel the measurement window. If `True`, the measurement\n                window is assumed to be cancelled and discarded. Thus, an empty Measurement\n                object will be returned and the measurement window will not be recorded in\n                the log file either. `sync_cuda` is still respected.\n        \"\"\"\n        # Retrieve the start time and energy consumption of this window.\n        try:\n            start_time, start_energy = self.measurement_states.pop(key)\n        except KeyError:\n            raise ValueError(f\"Measurement window '{key}' does not exist\") from None\n\n        # Take instant power consumption measurements.\n        # This, in theory, is introducing extra NVMLs call in the critical path\n        # even if computation time is not so short. However, it is reasonable to\n        # expect that computation time would be short if the user explicitly\n        # turned on the `approx_instant_energy` option. Calling this function\n        # as early as possible will lead to more accurate energy approximation.\n        power, power_measurement_time = (\n            self._get_instant_power() if self.approx_instant_energy else ({}, 0.0)\n        )\n\n        # Call cudaSynchronize to make sure we freeze at the right time.\n        if sync_cuda:\n            for gpu_index in self.gpu_indices:\n                cuda_sync(gpu_index)\n\n        # If the measurement window is cancelled, return an empty Measurement object.\n        if cancel:\n            logger.debug(\"Measurement window '%s' cancelled.\", key)\n            return Measurement(time=0.0, energy={gpu: 0.0 for gpu in self.gpu_indices})\n\n        end_time: float = time()\n        time_consumption: float = end_time - start_time\n        energy_consumption: dict[int, float] = {}\n        for gpu_index in self.gpu_indices:\n            # Query energy directly if the GPU has newer architecture.\n            if self.gpus.supportsGetTotalEnergyConsumption(gpu_index):\n                end_energy = self.gpus.getTotalEnergyConsumption(gpu_index) / 1000.0\n                energy_consumption[gpu_index] = end_energy - start_energy[gpu_index]\n\n        # If there are older GPU architectures, the PowerMonitor will take care of those.\n        if self.power_monitor is not None:\n            energy = self.power_monitor.get_energy(start_time, end_time)\n            # Fallback to the instant power measurement if the PowerMonitor does not\n            # have the power samples.\n            if energy is None:\n                energy = {gpu: 0.0 for gpu in self.power_monitor.gpu_indices}\n            energy_consumption |= energy\n\n        # Approximate energy consumption if the measurement window is too short.\n        if self.approx_instant_energy:\n            for gpu_index in self.gpu_indices:\n                if energy_consumption[gpu_index] == 0.0:\n                    energy_consumption[gpu_index] = power[gpu_index] * (\n                        time_consumption - power_measurement_time\n                    )\n\n        logger.debug(\"Measurement window '%s' ended.\", key)\n\n        # Add to log file.\n        if self.log_file is not None:\n            self.log_file.write(\n                f\"{start_time},{key},{time_consumption},\"\n                + \",\".join(str(energy_consumption[gpu]) for gpu in self.gpu_indices)\n                + \"\\n\"\n            )\n            self.log_file.flush()\n\n        return Measurement(time_consumption, energy_consumption)\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.__init__","title":"__init__","text":"<pre><code>__init__(\n    gpu_indices=None,\n    approx_instant_energy=False,\n    log_file=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. Time/Energy measurements will begin and end at the same time for these GPUs (i.e., synchronized). If None, all the GPUs available will be used. <code>CUDA_VISIBLE_DEVICES</code> is respected if set, e.g., GPU index <code>1</code> passed into <code>gpu_indices</code> when <code>CUDA_VISIBLE_DEVICES=2,3</code> will be interpreted as CUDA device <code>3</code>. <code>CUDA_VISIBLE_DEVICES</code>s formatted with comma-separated indices are supported.</p> <code>None</code> <code>approx_instant_energy</code> <code>bool</code> <p>When the execution time of a measurement window is shorter than the NVML energy counter's update period, energy consumption may be observed as zero. In this case, if <code>approx_instant_energy</code> is True, the window's energy consumption will be approximated by multiplying the current instantaneous power consumption with the window's execution time. This should be a better estimate than zero, but it's still an approximation.</p> <code>False</code> <code>log_file</code> <code>str | Path | None</code> <p>Path to the log CSV file. If <code>None</code>, logging will be disabled.</p> <code>None</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    approx_instant_energy: bool = False,\n    log_file: str | Path | None = None,\n) -&gt; None:\n    \"\"\"Instantiate the monitor.\n\n    Args:\n        gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n            will begin and end at the same time for these GPUs (i.e., synchronized).\n            If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n            is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n            `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n            `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n        approx_instant_energy: When the execution time of a measurement window is\n            shorter than the NVML energy counter's update period, energy consumption may\n            be observed as zero. In this case, if `approx_instant_energy` is True, the\n            window's energy consumption will be approximated by multiplying the current\n            instantaneous power consumption with the window's execution time. This should\n            be a better estimate than zero, but it's still an approximation.\n        log_file: Path to the log CSV file. If `None`, logging will be disabled.\n    \"\"\"\n    # Save arguments.\n    self.approx_instant_energy = approx_instant_energy\n\n    # Get gpus\n    self.gpus = get_gpus()\n\n    # Get GPU indices:\n    self.gpu_indices = (\n        gpu_indices if gpu_indices is not None else list(range(len(self.gpus)))\n    )\n\n    # Initialize loggers.\n    if log_file is None:\n        self.log_file = None\n    else:\n        if dir := os.path.dirname(log_file):\n            os.makedirs(dir, exist_ok=True)\n        self.log_file = open(log_file, \"w\")\n        logger.info(\"Writing measurement logs to %s.\", log_file)\n        self.log_file.write(\n            f\"start_time,window_name,elapsed_time,{','.join(map(lambda i: f'gpu{i}_energy', self.gpu_indices))}\\n\",\n        )\n        self.log_file.flush()\n\n    logger.info(\"Monitoring GPU indices %s.\", self.gpu_indices)\n\n    # A dictionary that maps the string keys of active measurement windows to\n    # the state of the measurement window. Each element in the dictionary is a tuple of:\n    #     1) Time elapsed at the beginning of this window.\n    #     2) Total energy consumed by each &gt;= Volta GPU at the beginning of\n    #        this window (`None` for older GPUs).\n    self.measurement_states: dict[str, tuple[float, dict[int, float]]] = {}\n\n    # Initialize power monitors for older architecture GPUs.\n    old_gpu_indices = [\n        gpu_index\n        for gpu_index in self.gpu_indices\n        if not self.gpus.supportsGetTotalEnergyConsumption(gpu_index)\n    ]\n    if old_gpu_indices:\n        self.power_monitor = PowerMonitor(\n            gpu_indices=old_gpu_indices, update_period=None\n        )\n    else:\n        self.power_monitor = None\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor._get_instant_power","title":"_get_instant_power","text":"<pre><code>_get_instant_power()\n</code></pre> <p>Measure the power consumption of all GPUs at the current time.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def _get_instant_power(self) -&gt; tuple[dict[int, float], float]:\n    \"\"\"Measure the power consumption of all GPUs at the current time.\"\"\"\n    power_measurement_start_time: float = time()\n    power = {\n        i: self.gpus.getInstantPowerUsage(i) / 1000.0 for i in self.gpu_indices\n    }\n    power_measurement_time = time() - power_measurement_start_time\n    return power, power_measurement_time\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_cuda=True)\n</code></pre> <p>Begin a new measurement window.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Unique name of the measurement window.</p> required <code>sync_cuda</code> <code>bool</code> <p>Whether to synchronize CUDA before starting the measurement window. (Default: <code>True</code>)</p> <code>True</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def begin_window(self, key: str, sync_cuda: bool = True) -&gt; None:\n    \"\"\"Begin a new measurement window.\n\n    Args:\n        key: Unique name of the measurement window.\n        sync_cuda: Whether to synchronize CUDA before starting the measurement window.\n            (Default: `True`)\n    \"\"\"\n    # Make sure the key is unique.\n    if key in self.measurement_states:\n        raise ValueError(f\"Measurement window '{key}' already exists\")\n\n    # Call cudaSynchronize to make sure we freeze at the right time.\n    if sync_cuda:\n        for gpu_index in self.gpu_indices:\n            cuda_sync(gpu_index)\n\n    # Freeze the start time of the profiling window.\n    timestamp: float = time()\n    energy_state: dict[int, float] = {}\n    for gpu_index in self.gpu_indices:\n        # Query energy directly if the GPU has newer architecture.\n        # Otherwise, the Zeus power monitor is running in the background to\n        # collect power consumption, so we just need to read the log file later.\n        if self.gpus.supportsGetTotalEnergyConsumption(gpu_index):\n            energy_state[gpu_index] = (\n                self.gpus.getTotalEnergyConsumption(gpu_index) / 1000.0\n            )\n\n    # Add measurement state to dictionary.\n    self.measurement_states[key] = (timestamp, energy_state)\n    logger.debug(\"Measurement window '%s' started.\", key)\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_cuda=True, cancel=False)\n</code></pre> <p>End a measurement window and return the time and energy consumption.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of an active measurement window.</p> required <code>sync_cuda</code> <code>bool</code> <p>Whether to synchronize CUDA before ending the measurement window. (default: <code>True</code>)</p> <code>True</code> <code>cancel</code> <code>bool</code> <p>Whether to cancel the measurement window. If <code>True</code>, the measurement window is assumed to be cancelled and discarded. Thus, an empty Measurement object will be returned and the measurement window will not be recorded in the log file either. <code>sync_cuda</code> is still respected.</p> <code>False</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def end_window(\n    self, key: str, sync_cuda: bool = True, cancel: bool = False\n) -&gt; Measurement:\n    \"\"\"End a measurement window and return the time and energy consumption.\n\n    Args:\n        key: Name of an active measurement window.\n        sync_cuda: Whether to synchronize CUDA before ending the measurement window.\n            (default: `True`)\n        cancel: Whether to cancel the measurement window. If `True`, the measurement\n            window is assumed to be cancelled and discarded. Thus, an empty Measurement\n            object will be returned and the measurement window will not be recorded in\n            the log file either. `sync_cuda` is still respected.\n    \"\"\"\n    # Retrieve the start time and energy consumption of this window.\n    try:\n        start_time, start_energy = self.measurement_states.pop(key)\n    except KeyError:\n        raise ValueError(f\"Measurement window '{key}' does not exist\") from None\n\n    # Take instant power consumption measurements.\n    # This, in theory, is introducing extra NVMLs call in the critical path\n    # even if computation time is not so short. However, it is reasonable to\n    # expect that computation time would be short if the user explicitly\n    # turned on the `approx_instant_energy` option. Calling this function\n    # as early as possible will lead to more accurate energy approximation.\n    power, power_measurement_time = (\n        self._get_instant_power() if self.approx_instant_energy else ({}, 0.0)\n    )\n\n    # Call cudaSynchronize to make sure we freeze at the right time.\n    if sync_cuda:\n        for gpu_index in self.gpu_indices:\n            cuda_sync(gpu_index)\n\n    # If the measurement window is cancelled, return an empty Measurement object.\n    if cancel:\n        logger.debug(\"Measurement window '%s' cancelled.\", key)\n        return Measurement(time=0.0, energy={gpu: 0.0 for gpu in self.gpu_indices})\n\n    end_time: float = time()\n    time_consumption: float = end_time - start_time\n    energy_consumption: dict[int, float] = {}\n    for gpu_index in self.gpu_indices:\n        # Query energy directly if the GPU has newer architecture.\n        if self.gpus.supportsGetTotalEnergyConsumption(gpu_index):\n            end_energy = self.gpus.getTotalEnergyConsumption(gpu_index) / 1000.0\n            energy_consumption[gpu_index] = end_energy - start_energy[gpu_index]\n\n    # If there are older GPU architectures, the PowerMonitor will take care of those.\n    if self.power_monitor is not None:\n        energy = self.power_monitor.get_energy(start_time, end_time)\n        # Fallback to the instant power measurement if the PowerMonitor does not\n        # have the power samples.\n        if energy is None:\n            energy = {gpu: 0.0 for gpu in self.power_monitor.gpu_indices}\n        energy_consumption |= energy\n\n    # Approximate energy consumption if the measurement window is too short.\n    if self.approx_instant_energy:\n        for gpu_index in self.gpu_indices:\n            if energy_consumption[gpu_index] == 0.0:\n                energy_consumption[gpu_index] = power[gpu_index] * (\n                    time_consumption - power_measurement_time\n                )\n\n    logger.debug(\"Measurement window '%s' ended.\", key)\n\n    # Add to log file.\n    if self.log_file is not None:\n        self.log_file.write(\n            f\"{start_time},{key},{time_consumption},\"\n            + \",\".join(str(energy_consumption[gpu]) for gpu in self.gpu_indices)\n            + \"\\n\"\n        )\n        self.log_file.flush()\n\n    return Measurement(time_consumption, energy_consumption)\n</code></pre>"},{"location":"reference/monitor/power/","title":"power","text":""},{"location":"reference/monitor/power/#zeus.monitor.power","title":"zeus.monitor.power","text":"<p>Monitor the power usage of GPUs.</p>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor","title":"PowerMonitor","text":"<p>Monitor power usage from GPUs.</p> <p>This class acts as a lower level wrapper around a Python process that polls the power consumption of GPUs. This is primarily used by <code>ZeusMonitor</code> for older architecture GPUs that do not support the nvmlDeviceGetTotalEnergyConsumption API.</p> <p>Warning</p> <p>Since the monitor spawns a child process, it should not be instantiated as a global variable. Python puts a protection to prevent creating a process in global scope. Refer to the \"Safe importing of main module\" section in the Python documentation for more details.</p> <p>Attributes:</p> Name Type Description <code>gpu_indices</code> <code>list[int]</code> <p>Indices of the GPUs to monitor.</p> <code>update_period</code> <code>int</code> <p>Update period of the power monitor in seconds. Holds inferred update period if <code>update_period</code> was given as <code>None</code>.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>class PowerMonitor:\n    \"\"\"Monitor power usage from GPUs.\n\n    This class acts as a lower level wrapper around a Python process that polls\n    the power consumption of GPUs. This is primarily used by\n    [`ZeusMonitor`][zeus.monitor.ZeusMonitor] for older architecture GPUs that\n    do not support the nvmlDeviceGetTotalEnergyConsumption API.\n\n    !!! Warning\n        Since the monitor spawns a child process, **it should not be instantiated as a global variable**.\n        Python puts a protection to prevent creating a process in global scope.\n        Refer to the \"Safe importing of main module\" section in the\n        [Python documentation](https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods)\n        for more details.\n\n    Attributes:\n        gpu_indices (list[int]): Indices of the GPUs to monitor.\n        update_period (int): Update period of the power monitor in seconds.\n            Holds inferred update period if `update_period` was given as `None`.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        update_period: float | None = None,\n        power_csv_path: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the power monitor.\n\n        Args:\n            gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n            update_period: Update period of the power monitor in seconds. If None,\n                infer the update period by max speed polling the power counter for\n                each GPU model.\n            power_csv_path: If given, the power polling process will write measurements\n                to this path. Otherwise, a temporary file will be used.\n        \"\"\"\n        if gpu_indices is not None and not gpu_indices:\n            raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n        # Get GPUs\n        gpus = get_gpus()\n\n        # Set up logging.\n        self.logger = get_logger(type(self).__name__)\n\n        # Get GPUs\n        self.gpu_indices = (\n            gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n        )\n        self.logger.info(\"Monitoring power usage of GPUs %s\", self.gpu_indices)\n\n        # Infer the update period if necessary.\n        if update_period is None:\n            update_period = infer_counter_update_period(self.gpu_indices)\n        self.update_period = update_period\n\n        # Create and open the CSV to record power measurements.\n        if power_csv_path is None:\n            power_csv_path = tempfile.mkstemp(suffix=\".csv\", text=True)[1]\n        open(power_csv_path, \"w\").close()\n        self.power_f = open(power_csv_path)\n        self.power_df_columns = [\"time\"] + [f\"power{i}\" for i in self.gpu_indices]\n        self.power_df = pd.DataFrame(columns=self.power_df_columns)\n\n        # Spawn the power polling process.\n        atexit.register(self._stop)\n        self.process = mp.get_context(\"spawn\").Process(\n            target=_polling_process,\n            args=(self.gpu_indices, power_csv_path, update_period),\n        )\n        self.process.start()\n\n    def _stop(self) -&gt; None:\n        \"\"\"Stop monitoring power usage.\"\"\"\n        if self.process is not None:\n            self.process.terminate()\n            self.process.join(timeout=1.0)\n            self.process.kill()\n            self.process = None\n\n    def _update_df(self) -&gt; None:\n        \"\"\"Add rows to the power dataframe from the CSV file.\"\"\"\n        try:\n            additional_df = typing.cast(\n                pd.DataFrame,\n                pd.read_csv(self.power_f, header=None, names=self.power_df_columns),\n            )\n        except pd.errors.EmptyDataError:\n            return\n\n        if additional_df.empty:\n            return\n\n        if self.power_df.empty:\n            self.power_df = additional_df\n        else:\n            self.power_df = pd.concat(\n                [self.power_df, additional_df],\n                axis=0,\n                ignore_index=True,\n                copy=False,\n            )\n\n    def get_energy(self, start_time: float, end_time: float) -&gt; dict[int, float] | None:\n        \"\"\"Get the energy used by the GPUs between two times.\n\n        Args:\n            start_time: Start time of the interval, from time.time().\n            end_time: End time of the interval, from time.time().\n\n        Returns:\n            A dictionary mapping GPU indices to the energy used by the GPU between the\n            two times. GPU indices are from the DL framework's perspective after\n            applying `CUDA_VISIBLE_DEVICES`.\n            If there are no power readings, return None.\n        \"\"\"\n        self._update_df()\n\n        if self.power_df.empty:\n            return None\n\n        df = typing.cast(\n            pd.DataFrame, self.power_df.query(f\"{start_time} &lt;= time &lt;= {end_time}\")\n        )\n\n        try:\n            return {\n                i: float(auc(df[\"time\"], df[f\"power{i}\"])) for i in self.gpu_indices\n            }\n        except ValueError:\n            return None\n\n    def get_power(self, time: float | None = None) -&gt; dict[int, float] | None:\n        \"\"\"Get the power usage of the GPUs at a specific time point.\n\n        Args:\n            time: Time point to get the power usage at. If None, get the power usage\n                at the last recorded time point.\n\n        Returns:\n            A dictionary mapping GPU indices to the power usage of the GPU at the\n            specified time point. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n            If there are no power readings (e.g., future timestamps), return None.\n        \"\"\"\n        self._update_df()\n\n        if self.power_df.empty:\n            return None\n\n        if time is None:\n            row = self.power_df.iloc[-1]\n        else:\n            ind = self.power_df.time.searchsorted(time)\n            try:\n                row = self.power_df.iloc[ind]\n            except IndexError:\n                # This means that the time is after the last recorded power reading.\n                row = self.power_df.iloc[-1]\n\n        return {i: float(row[f\"power{i}\"]) for i in self.gpu_indices}\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.__init__","title":"__init__","text":"<pre><code>__init__(\n    gpu_indices=None,\n    update_period=None,\n    power_csv_path=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of the GPUs to monitor. If None, monitor all GPUs.</p> <code>None</code> <code>update_period</code> <code>float | None</code> <p>Update period of the power monitor in seconds. If None, infer the update period by max speed polling the power counter for each GPU model.</p> <code>None</code> <code>power_csv_path</code> <code>str | None</code> <p>If given, the power polling process will write measurements to this path. Otherwise, a temporary file will be used.</p> <code>None</code> Source code in <code>zeus/monitor/power.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    update_period: float | None = None,\n    power_csv_path: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the power monitor.\n\n    Args:\n        gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n        update_period: Update period of the power monitor in seconds. If None,\n            infer the update period by max speed polling the power counter for\n            each GPU model.\n        power_csv_path: If given, the power polling process will write measurements\n            to this path. Otherwise, a temporary file will be used.\n    \"\"\"\n    if gpu_indices is not None and not gpu_indices:\n        raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n    # Get GPUs\n    gpus = get_gpus()\n\n    # Set up logging.\n    self.logger = get_logger(type(self).__name__)\n\n    # Get GPUs\n    self.gpu_indices = (\n        gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n    )\n    self.logger.info(\"Monitoring power usage of GPUs %s\", self.gpu_indices)\n\n    # Infer the update period if necessary.\n    if update_period is None:\n        update_period = infer_counter_update_period(self.gpu_indices)\n    self.update_period = update_period\n\n    # Create and open the CSV to record power measurements.\n    if power_csv_path is None:\n        power_csv_path = tempfile.mkstemp(suffix=\".csv\", text=True)[1]\n    open(power_csv_path, \"w\").close()\n    self.power_f = open(power_csv_path)\n    self.power_df_columns = [\"time\"] + [f\"power{i}\" for i in self.gpu_indices]\n    self.power_df = pd.DataFrame(columns=self.power_df_columns)\n\n    # Spawn the power polling process.\n    atexit.register(self._stop)\n    self.process = mp.get_context(\"spawn\").Process(\n        target=_polling_process,\n        args=(self.gpu_indices, power_csv_path, update_period),\n    )\n    self.process.start()\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor._stop","title":"_stop","text":"<pre><code>_stop()\n</code></pre> <p>Stop monitoring power usage.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _stop(self) -&gt; None:\n    \"\"\"Stop monitoring power usage.\"\"\"\n    if self.process is not None:\n        self.process.terminate()\n        self.process.join(timeout=1.0)\n        self.process.kill()\n        self.process = None\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor._update_df","title":"_update_df","text":"<pre><code>_update_df()\n</code></pre> <p>Add rows to the power dataframe from the CSV file.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _update_df(self) -&gt; None:\n    \"\"\"Add rows to the power dataframe from the CSV file.\"\"\"\n    try:\n        additional_df = typing.cast(\n            pd.DataFrame,\n            pd.read_csv(self.power_f, header=None, names=self.power_df_columns),\n        )\n    except pd.errors.EmptyDataError:\n        return\n\n    if additional_df.empty:\n        return\n\n    if self.power_df.empty:\n        self.power_df = additional_df\n    else:\n        self.power_df = pd.concat(\n            [self.power_df, additional_df],\n            axis=0,\n            ignore_index=True,\n            copy=False,\n        )\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_energy","title":"get_energy","text":"<pre><code>get_energy(start_time, end_time)\n</code></pre> <p>Get the energy used by the GPUs between two times.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Start time of the interval, from time.time().</p> required <code>end_time</code> <code>float</code> <p>End time of the interval, from time.time().</p> required <p>Returns:</p> Type Description <code>dict[int, float] | None</code> <p>A dictionary mapping GPU indices to the energy used by the GPU between the</p> <code>dict[int, float] | None</code> <p>two times. GPU indices are from the DL framework's perspective after</p> <code>dict[int, float] | None</code> <p>applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>dict[int, float] | None</code> <p>If there are no power readings, return None.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_energy(self, start_time: float, end_time: float) -&gt; dict[int, float] | None:\n    \"\"\"Get the energy used by the GPUs between two times.\n\n    Args:\n        start_time: Start time of the interval, from time.time().\n        end_time: End time of the interval, from time.time().\n\n    Returns:\n        A dictionary mapping GPU indices to the energy used by the GPU between the\n        two times. GPU indices are from the DL framework's perspective after\n        applying `CUDA_VISIBLE_DEVICES`.\n        If there are no power readings, return None.\n    \"\"\"\n    self._update_df()\n\n    if self.power_df.empty:\n        return None\n\n    df = typing.cast(\n        pd.DataFrame, self.power_df.query(f\"{start_time} &lt;= time &lt;= {end_time}\")\n    )\n\n    try:\n        return {\n            i: float(auc(df[\"time\"], df[f\"power{i}\"])) for i in self.gpu_indices\n        }\n    except ValueError:\n        return None\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_power","title":"get_power","text":"<pre><code>get_power(time=None)\n</code></pre> <p>Get the power usage of the GPUs at a specific time point.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>float | None</code> <p>Time point to get the power usage at. If None, get the power usage at the last recorded time point.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, float] | None</code> <p>A dictionary mapping GPU indices to the power usage of the GPU at the</p> <code>dict[int, float] | None</code> <p>specified time point. GPU indices are from the DL framework's perspective</p> <code>dict[int, float] | None</code> <p>after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>dict[int, float] | None</code> <p>If there are no power readings (e.g., future timestamps), return None.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_power(self, time: float | None = None) -&gt; dict[int, float] | None:\n    \"\"\"Get the power usage of the GPUs at a specific time point.\n\n    Args:\n        time: Time point to get the power usage at. If None, get the power usage\n            at the last recorded time point.\n\n    Returns:\n        A dictionary mapping GPU indices to the power usage of the GPU at the\n        specified time point. GPU indices are from the DL framework's perspective\n        after applying `CUDA_VISIBLE_DEVICES`.\n        If there are no power readings (e.g., future timestamps), return None.\n    \"\"\"\n    self._update_df()\n\n    if self.power_df.empty:\n        return None\n\n    if time is None:\n        row = self.power_df.iloc[-1]\n    else:\n        ind = self.power_df.time.searchsorted(time)\n        try:\n            row = self.power_df.iloc[ind]\n        except IndexError:\n            # This means that the time is after the last recorded power reading.\n            row = self.power_df.iloc[-1]\n\n    return {i: float(row[f\"power{i}\"]) for i in self.gpu_indices}\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.infer_counter_update_period","title":"infer_counter_update_period","text":"<pre><code>infer_counter_update_period(gpu_indicies)\n</code></pre> <p>Infer the update period of the NVML power counter.</p> <p>NVML counters can update as slow as 10 Hz depending on the GPU model, so there's no need to poll them too faster than that. This function infers the update period for each unique GPU model and selects the fastest-updating period detected. Then, it returns half the period to ensure that the counter is polled at least twice per update period.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def infer_counter_update_period(gpu_indicies: list[int]) -&gt; float:\n    \"\"\"Infer the update period of the NVML power counter.\n\n    NVML counters can update as slow as 10 Hz depending on the GPU model, so\n    there's no need to poll them too faster than that. This function infers the\n    update period for each unique GPU model and selects the fastest-updating\n    period detected. Then, it returns half the period to ensure that the\n    counter is polled at least twice per update period.\n    \"\"\"\n    logger = get_logger(__name__)\n\n    # get gpus\n    gpus = get_gpus()\n\n    # For each unique GPU model, infer the update period.\n    update_period = 0.0\n    gpu_models_covered = set()\n    for index in gpu_indicies:\n        if (model := gpus.getName(index)) not in gpu_models_covered:\n            logger.info(\n                \"Detected %s, inferring NVML power counter update period.\", model\n            )\n            gpu_models_covered.add(model)\n            detected_period = _infer_counter_update_period_single(index)\n            logger.info(\n                \"Counter update period for %s is %.2f s\",\n                model,\n                detected_period,\n            )\n            if update_period &gt; detected_period:\n                update_period = detected_period\n\n    # Target half the update period to ensure that the counter is enough.\n    update_period /= 2.0\n\n    # Anything less than ten times a second is probably too slow.\n    if update_period &gt; 0.1:\n        logger.warning(\n            \"Inferred update period (%.2f s) is too long. Using 0.1 s instead.\",\n            update_period,\n        )\n        update_period = 0.1\n    return update_period\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power._infer_counter_update_period_single","title":"_infer_counter_update_period_single","text":"<pre><code>_infer_counter_update_period_single(gpu_index)\n</code></pre> <p>Infer the update period of the NVML power counter for a single GPU.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _infer_counter_update_period_single(gpu_index: int) -&gt; float:\n    \"\"\"Infer the update period of the NVML power counter for a single GPU.\"\"\"\n    # get gpus\n    gpus = get_gpus()\n    # Collect 1000 samples of the power counter with timestamps.\n    time_power_samples: list[tuple[float, int]] = [(0.0, 0) for _ in range(1000)]\n    for i in range(len(time_power_samples)):\n        time_power_samples[i] = (\n            time(),\n            gpus.getInstantPowerUsage(gpu_index),\n        )\n\n    # Find the timestamps when the power readings changed.\n    time_power_samples = time_power_samples[10:]\n    changed_times = []\n    prev_power = time_power_samples[0][1]\n    for t, p in time_power_samples:\n        if p != prev_power:\n            changed_times.append(t)\n            prev_power = p\n\n    # Compute the minimum time difference between power change timestamps.\n    intervals = [\n        time2 - time1 for time1, time2 in zip(changed_times, changed_times[1:])\n    ]\n    if len(intervals) == 0:\n        return 0.1\n    return min(intervals)\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power._polling_process","title":"_polling_process","text":"<pre><code>_polling_process(\n    gpu_indices, power_csv_path, update_period\n)\n</code></pre> <p>Run the power monitor.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _polling_process(\n    gpu_indices: list[int],\n    power_csv_path: str,\n    update_period: float,\n) -&gt; None:\n    \"\"\"Run the power monitor.\"\"\"\n    try:\n        # Get GPUs\n        gpus = get_gpus()\n\n        # Use line buffering.\n        with open(power_csv_path, \"w\", buffering=1) as power_f:\n            while True:\n                power: list[float] = []\n                now = time()\n                for index in gpu_indices:\n                    power.append(gpus.getInstantPowerUsage(index))\n                power_str = \",\".join(map(lambda p: str(p / 1000), power))\n                power_f.write(f\"{now},{power_str}\\n\")\n                if (sleep_time := update_period - (time() - now)) &gt; 0:\n                    sleep(sleep_time)\n    except KeyboardInterrupt:\n        return\n</code></pre>"},{"location":"reference/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/#zeus.optimizer","title":"zeus.optimizer","text":"<p>Zeus energy optimizers.</p>"},{"location":"reference/optimizer/power_limit/","title":"power_limit","text":""},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit","title":"zeus.optimizer.power_limit","text":"<p>Optimizers that select the optimum power limit.</p> <p>This module contains the following pieces:</p> <ul> <li><code>GlobalPowerLimitOptimizer</code>   is the main class that implements the state machine   and the logic for profiling power limits and selecting   the optimum power limit.</li> <li><code>PowerLimitMeasurement</code> and various   state classes are helpers that support the state machine.</li> <li><code>OptimumSelector</code>   is an abstract base class for selecting the optimum power limit   from a list of power limit profiling results. There are concrete classes   that implement different selection strategies, like   minimizing energy,   minimizing time,   minimizing the Zeus time-energy cost,   or selecting the lowest power limit that meets the given maximum training time slowdown factor.</li> <li><code>HFGlobalPowerLimitOptimizer</code>   is a wrapper for the Hugging Face <code>TrainerCallback</code> class that uses <code>GlobalPowerLimitOptimizer</code>.</li> </ul>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.OptimumSelector","title":"OptimumSelector","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for optimum power limit selectors.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class OptimumSelector(ABC):\n    \"\"\"Base class for optimum power limit selectors.\"\"\"\n\n    @abstractmethod\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.OptimumSelector.select","title":"select  <code>abstractmethod</code>","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>@abstractmethod\ndef select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy","title":"Energy","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes energy consumption.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Energy(OptimumSelector):\n    \"\"\"Selects the power limit that minimizes energy consumption.\"\"\"\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        return min(measurements, key=lambda x: x.energy).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    return min(measurements, key=lambda x: x.energy).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time","title":"Time","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes training time.</p> <p>This may not necessarily choose the maximum power limit, as time profiling results can be slightly noisy. However, we believe that's actually better because it means that training time is very similar among higher power limits, but lower power limit will consume less power.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Time(OptimumSelector):\n    \"\"\"Selects the power limit that minimizes training time.\n\n    This may not necessarily choose the maximum power limit, as time profiling\n    results can be slightly noisy. However, we believe that's actually better\n    because it means that training time is very similar among higher power limits,\n    but lower power limit will consume less power.\n    \"\"\"\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        return min(measurements, key=lambda x: x.time).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    return min(measurements, key=lambda x: x.time).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost","title":"ZeusCost","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes a linear Zeus time-energy cost function.</p> <p>Cost function is \\(\\eta \\cdot \\text{Energy} + (1 - \\eta) \\cdot \\text{MaxPower} \\cdot \\text{Time}\\).</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class ZeusCost(OptimumSelector):\n    r\"\"\"Selects the power limit that minimizes a linear Zeus time-energy cost function.\n\n    Cost function is $\\eta \\cdot \\text{Energy} + (1 - \\eta) \\cdot \\text{MaxPower} \\cdot \\text{Time}$.\n    \"\"\"\n\n    def __init__(self, eta_knob: float, world_size: int = 1) -&gt; None:\n        r\"\"\"Initialize the selector.\n\n        Args:\n            eta_knob: The $0 \\le \\eta \\le 1$ knob for the Zeus time-energy cost function.\n            world_size: The number of GPUs in the training job. Defaults to 1.\n        \"\"\"\n        if eta_knob &lt; 0 or eta_knob &gt; 1:\n            raise ValueError(\"eta_knob must be between 0 and 1, inclusive both sides.\")\n        if world_size &lt; 1:\n            raise ValueError(\"world_size must be greater than or equal to 1.\")\n\n        self.eta_knob = eta_knob\n        self.world_size = world_size\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        max_power = (\n            max(measurement.power_limit for measurement in measurements)\n            * self.world_size\n        )\n        zeus_cost_map = {\n            measurement.power_limit: zeus_cost(\n                energy=measurement.energy,\n                time=measurement.time,\n                eta_knob=self.eta_knob,\n                max_power=max_power,\n            )\n            for measurement in measurements\n        }\n        return min(zeus_cost_map, key=lambda x: zeus_cost_map[x])\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost.__init__","title":"__init__","text":"<pre><code>__init__(eta_knob, world_size=1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>eta_knob</code> <code>float</code> <p>The \\(0 \\le \\eta \\le 1\\) knob for the Zeus time-energy cost function.</p> required <code>world_size</code> <code>int</code> <p>The number of GPUs in the training job. Defaults to 1.</p> <code>1</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(self, eta_knob: float, world_size: int = 1) -&gt; None:\n    r\"\"\"Initialize the selector.\n\n    Args:\n        eta_knob: The $0 \\le \\eta \\le 1$ knob for the Zeus time-energy cost function.\n        world_size: The number of GPUs in the training job. Defaults to 1.\n    \"\"\"\n    if eta_knob &lt; 0 or eta_knob &gt; 1:\n        raise ValueError(\"eta_knob must be between 0 and 1, inclusive both sides.\")\n    if world_size &lt; 1:\n        raise ValueError(\"world_size must be greater than or equal to 1.\")\n\n    self.eta_knob = eta_knob\n    self.world_size = world_size\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    max_power = (\n        max(measurement.power_limit for measurement in measurements)\n        * self.world_size\n    )\n    zeus_cost_map = {\n        measurement.power_limit: zeus_cost(\n            energy=measurement.energy,\n            time=measurement.time,\n            eta_knob=self.eta_knob,\n            max_power=max_power,\n        )\n        for measurement in measurements\n    }\n    return min(zeus_cost_map, key=lambda x: zeus_cost_map[x])\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint","title":"MaxSlowdownConstraint","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the minumum power limit that does not slow down training by more than the given factor.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class MaxSlowdownConstraint(OptimumSelector):\n    \"\"\"Selects the minumum power limit that does not slow down training by more than the given factor.\"\"\"\n\n    def __init__(self, factor: float) -&gt; None:\n        \"\"\"Initialize the selector.\n\n        Args:\n            factor: The maximum allowed slowdown factor. Greater than or equal to 1.0.\n        \"\"\"\n        if factor &lt; 1.0:\n            raise ValueError(\n                f\"max_slowdown_factor must be greater than or equal to 1.0. Got {factor}.\",\n            )\n\n        self.factor = factor\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        feasible_power_limits = []\n        max_power = max(measurement.power_limit for measurement in measurements)\n        shortest_time = next(\n            measurement.time\n            for measurement in measurements\n            if measurement.power_limit == max_power\n        )\n        for measurement in measurements:\n            if measurement.time &lt;= self.factor * shortest_time:\n                feasible_power_limits.append(measurement.power_limit)\n        return min(feasible_power_limits)\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint.__init__","title":"__init__","text":"<pre><code>__init__(factor)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>float</code> <p>The maximum allowed slowdown factor. Greater than or equal to 1.0.</p> required Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(self, factor: float) -&gt; None:\n    \"\"\"Initialize the selector.\n\n    Args:\n        factor: The maximum allowed slowdown factor. Greater than or equal to 1.0.\n    \"\"\"\n    if factor &lt; 1.0:\n        raise ValueError(\n            f\"max_slowdown_factor must be greater than or equal to 1.0. Got {factor}.\",\n        )\n\n    self.factor = factor\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    feasible_power_limits = []\n    max_power = max(measurement.power_limit for measurement in measurements)\n    shortest_time = next(\n        measurement.time\n        for measurement in measurements\n        if measurement.power_limit == max_power\n    )\n    for measurement in measurements:\n        if measurement.time &lt;= self.factor * shortest_time:\n            feasible_power_limits.append(measurement.power_limit)\n    return min(feasible_power_limits)\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Ready","title":"Ready","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are ready to start measuring the next power limit.</p> <p>Initial state of the state machine if no previous profiling results were given. <code>Ready</code> -&gt; <code>Warmup</code> after <code>step</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Ready(BaseModel):\n    \"\"\"State for when we are ready to start measuring the next power limit.\n\n    Initial state of the state machine if no previous profiling results were given.\n    `Ready` -&gt; `Warmup` after `step`'th `on_step_begin`.\n    \"\"\"\n\n    next_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Warmup","title":"Warmup","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are warming up for a power limit.</p> <p><code>Warmup</code> -&gt; <code>Profiling</code> on the <code>steps</code>'th <code>on_step_begin</code>. <code>Warmup</code> -&gt; <code>Ready</code> on <code>on_epoch_end</code> before <code>steps</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Warmup(BaseModel):\n    \"\"\"State for when we are warming up for a power limit.\n\n    `Warmup` -&gt; `Profiling` on the `steps`'th `on_step_begin`.\n    `Warmup` -&gt; `Ready` on `on_epoch_end` before `steps`'th `on_step_begin`.\n    \"\"\"\n\n    current_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Profiling","title":"Profiling","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are profiling a power limit.</p> <p><code>Profiling</code> -&gt; <code>Warmup</code> after <code>steps</code>'th <code>on_step_begin</code> and     there are still power limits left to profile. <code>Profiling</code> -&gt; <code>Done</code> after <code>steps</code>'th <code>on_step_begin</code> and     there are no more power limits left to profile. <code>Profiling</code> -&gt; <code>Ready</code> on <code>on_epoch_end</code> before <code>steps</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Profiling(BaseModel):\n    \"\"\"State for when we are profiling a power limit.\n\n    `Profiling` -&gt; `Warmup` after `steps`'th `on_step_begin` and\n        there are still power limits left to profile.\n    `Profiling` -&gt; `Done` after `steps`'th `on_step_begin` and\n        there are no more power limits left to profile.\n    `Profiling` -&gt; `Ready` on `on_epoch_end` before `steps`'th `on_step_begin`.\n    \"\"\"\n\n    current_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Done","title":"Done","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are done profiling all power limits.</p> <p>Initial state of the state machine if previous profiling results were given. Final state of the state machine in any case.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Done(BaseModel):\n    \"\"\"State for when we are done profiling all power limits.\n\n    Initial state of the state machine if previous profiling results were given.\n    Final state of the state machine in any case.\n    \"\"\"\n\n    optimal_power_limit: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.PowerLimitMeasurement","title":"PowerLimitMeasurement","text":"<p>               Bases: <code>BaseModel</code></p> <p>POD for GPU energy and time measurements for one power limit (W).</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class PowerLimitMeasurement(BaseModel):\n    \"\"\"POD for GPU energy and time measurements for one power limit (W).\"\"\"\n\n    power_limit: PositiveInt  # In Watts.\n    energy: PositiveFloat\n    time: PositiveFloat\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit._PowerLimitMeasurementList","title":"_PowerLimitMeasurementList","text":"<p>               Bases: <code>BaseModel</code></p> <p>Proxy class to save and load a list of <code>PowerLimitMeasurement</code>s.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class _PowerLimitMeasurementList(BaseModel):\n    \"\"\"Proxy class to save and load a list of `PowerLimitMeasurement`s.\"\"\"\n\n    measurements: list[PowerLimitMeasurement]\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer","title":"GlobalPowerLimitOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Optimizer for the power limit knob.</p> <p>This optimizer uses the JIT profiling log to determine the optimal power limit.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class GlobalPowerLimitOptimizer(Callback):\n    \"\"\"Optimizer for the power limit knob.\n\n    This optimizer uses the JIT profiling log to determine the optimal power limit.\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor: ZeusMonitor,\n        optimum_selector: OptimumSelector | None = None,\n        wait_steps: int = 1,\n        warmup_steps: int = 10,\n        profile_steps: int = 40,\n        pl_step: int = 25,\n        profile_path: str | Path | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the optimizer.\n\n        GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n        Args:\n            monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n            optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n            wait_steps: Number of steps to pass by before doing anything at the beginning.\n                Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n                because the first iteration won't be representative of the rest of the iterations.\n            warmup_steps: Number of warmup iterations for each power limit.\n            profile_steps: Number of profie iterations for each power limit.\n            pl_step: The stride between power limits to explore, in unites of Watts.\n            profile_path: If the path points to an existing file, load the profile from the file\n                and do not run any profiling. If the path points to a non-existing file, profile\n                and save the profile to the file. If `None`, do not save or load any profile.\n        \"\"\"\n        # Sanity checks.\n        if wait_steps &lt; 0:\n            raise ValueError(\"wait_steps must be non-negative.\")\n        if warmup_steps &lt; 0:\n            raise ValueError(\"warmup_steps must be non-negative.\")\n        if profile_steps &lt;= 0:\n            raise ValueError(\"profile_steps must be positive.\")\n        if pl_step &lt;= 0:\n            raise ValueError(\"pl_step must be positive.\")\n\n        self.monitor = monitor\n        self.optimum_selector = optimum_selector or ZeusCost(\n            eta_knob=0.5,\n            world_size=len(monitor.gpu_indices),\n        )\n        self.warmup_steps = warmup_steps\n        self.profile_steps = profile_steps\n        self.pl_step = pl_step * 1000  # Internally, we use milliWatts.\n        self.profile_path = (\n            Path(profile_path) if isinstance(profile_path, str) else profile_path\n        )\n\n        # Setup logging.\n        self.logger = get_logger(type(self).__name__)\n\n        # Set the range of power limits to explore.\n        # Assert that supported power limits ranges are uniform across GPUs.\n        gpus = get_gpus(ensure_homogeneous=True)\n        pls = []\n        for index in monitor.gpu_indices:\n            pls.append(gpus.getPowerManagementLimitConstraints(index))\n        if not all(pls[0] == pl for pl in pls):\n            raise ValueError(\"Power limits ranges are not uniform across GPUs.\")\n        self.power_limits = list(\n            range(pls[0][1], pls[0][0] - self.pl_step, -self.pl_step)\n        )\n\n        # Turn on persistence mode and set to the highest power limit.\n        try:\n            for index in monitor.gpu_indices:\n                gpus.setPersistenceMode(index, enable=True)\n        except ZeusGPUNoPermissionError as ze:\n            raise RuntimeError(\n                \"SYS_ADMIN capability is required to modify GPU power limits. \"\n                \"Using --cap-add SYS_ADMIN when running the Docker container \"\n                \"is the easiest way to do this.\"\n            ) from ze\n        self.current_power_limit = 0\n\n        # Store `Measurement` objects in a list, one for each power limit.\n        self.measurements: list[PowerLimitMeasurement] = []\n\n        # State for the profiler state machine.\n        self.state: Ready | Warmup | Profiling | Done\n\n        # Initialize JIT profiling states.\n        if self.profile_path is None:\n            self.logger.info(\"JIT profiling enabled.\")\n            self.logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n            self.state = Ready(\n                next_power_limit=self.power_limits[0], steps=wait_steps + 1\n            )\n            self.logger.info(\"Set power limit to the maximum before starting.\")\n            self._set_power_limit(max(self.power_limits))\n        elif not self.profile_path.exists():\n            self.logger.info(\n                \"JIT Profiling enabled. Profile will be saved to '%s'.\",\n                str(self.profile_path),\n            )\n            self.logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n            self.state = Ready(\n                next_power_limit=self.power_limits[0], steps=wait_steps + 1\n            )\n            self.logger.info(\"Set power limit to the maximum before starting.\")\n            self._set_power_limit(max(self.power_limits))\n        else:\n            self.measurements = _PowerLimitMeasurementList.parse_file(\n                self.profile_path,\n            ).measurements\n            # self.measurements = _PowerLimitMeasurementList.model_validate_json(\n            #     open(self.profile_path).read(),\n            #     strict=True,\n            # ).measurements\n            self.logger.info(\n                \"Loaded previous profiling results from '%s'.\", str(self.profile_path)\n            )\n            optimal_power_limit = self._compute_optimal_power_limit()\n            self.logger.info(\n                \"Optimal power limit is %d W.\", optimal_power_limit // 1000\n            )\n            self.state = Done(optimal_power_limit=optimal_power_limit)\n            self._set_power_limit(self.state.optimal_power_limit)\n\n        # Restore all GPUs back to their maximum power limit on exit.\n        atexit.register(lambda: self._set_power_limit(max(self.power_limits)))\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Mark the end of a training epoch.\"\"\"\n        if isinstance(self.state, Ready):\n            pass\n\n        elif isinstance(self.state, (Warmup, Profiling)):\n            # Warmup/Profiling stage interrupted by the end of an epoch.\n            self.logger.info(\n                \"%s phase for %d W interrupted by the end of a training epoch.\",\n                type(self.state).__name__,\n                self.state.current_power_limit // 1000,\n            )\n            if isinstance(self.state, Profiling):\n                self.monitor.end_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                    cancel=True,\n                )\n            self.state = Ready(next_power_limit=self.state.current_power_limit, steps=1)\n            self._set_power_limit(max(self.power_limits))\n\n        elif isinstance(self.state, Done):\n            pass\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Mark the beginning of a training step.\"\"\"\n        if isinstance(self.state, Ready):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                self.logger.info(\n                    \"Starting warmup for power limit %d W.\",\n                    self.state.next_power_limit // 1000,\n                )\n                self._set_power_limit(self.state.next_power_limit)\n                self.state = Warmup(\n                    current_power_limit=self.state.next_power_limit,\n                    steps=self.warmup_steps,\n                )\n\n        elif isinstance(self.state, Warmup):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                self.logger.info(\n                    \"Starting actual profiling for power limit %d W.\",\n                    self.state.current_power_limit // 1000,\n                )\n                self.state = Profiling(\n                    current_power_limit=self.state.current_power_limit,\n                    steps=self.profile_steps,\n                )\n                self.monitor.begin_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                )\n\n        elif isinstance(self.state, Profiling):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                measurement = self.monitor.end_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                )\n                self.logger.info(\n                    \"Finished profiling for power limit %d W.\",\n                    self.state.current_power_limit // 1000,\n                )\n                self.measurements.append(\n                    PowerLimitMeasurement(\n                        power_limit=self.state.current_power_limit // 1000,\n                        energy=measurement.total_energy,\n                        time=measurement.time,\n                    )\n                )\n                # If we're done profiling all power limits, compute the optimal\n                # power limit and transition to the Done state. Otherwise, move\n                # on to the Warmup phase for the next power limit.\n                current_power_limit_index = self.power_limits.index(\n                    self.state.current_power_limit\n                )\n                if current_power_limit_index == len(self.power_limits) - 1:\n                    self.state = Done(\n                        optimal_power_limit=self._compute_optimal_power_limit(),\n                    )\n                    self._set_power_limit(self.state.optimal_power_limit)\n                    self._save_profile()\n                else:\n                    next_power_limit = self.power_limits[current_power_limit_index + 1]\n                    self.logger.info(\n                        \"Starting warmup for power limit %d W.\",\n                        next_power_limit // 1000,\n                    )\n                    self._set_power_limit(next_power_limit)\n                    self.state = Warmup(\n                        current_power_limit=next_power_limit,\n                        steps=self.warmup_steps,\n                    )\n\n        elif isinstance(self.state, Done):\n            pass\n\n    def _set_power_limit(self, power_limit: int) -&gt; None:\n        \"\"\"Set the power limit for all GPUs.\n\n        Args:\n            power_limit: The power limit to set, in milliWatts.\n        \"\"\"\n        gpus = get_gpus()\n        self.logger.info(\"Setting power limit to %d W.\", power_limit // 1000)\n        if self.current_power_limit == power_limit:\n            return\n        for index in self.monitor.gpu_indices:\n            gpus.setPowerManagementLimit(index, power_limit)\n        self.current_power_limit = power_limit\n\n    def _compute_optimal_power_limit(self) -&gt; int:\n        \"\"\"Compute the optimal power limit in milliWatts.\"\"\"\n        optimal_power_limit = self.optimum_selector.select(self.measurements) * 1000\n        self.logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n        return optimal_power_limit\n\n    def _save_profile(self) -&gt; None:\n        \"\"\"Save JIT profiling results and the optimal power limit to a JSON file.\"\"\"\n        if self.profile_path is None:\n            return\n\n        assert isinstance(self.state, Done)\n        with self.profile_path.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(\n                _PowerLimitMeasurementList(measurements=self.measurements).json(\n                    indent=4\n                ),\n            )\n        self.logger.info(\"JIT profiling results saved to '%s'.\", str(self.profile_path))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    monitor,\n    optimum_selector=None,\n    wait_steps=1,\n    warmup_steps=10,\n    profile_steps=40,\n    pl_step=25,\n    profile_path=None,\n)\n</code></pre> <p>GPU indices to profile and optimize for are taken from <code>monitor.gpu_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance used to profile GPU time and energy consumption.</p> required <code>optimum_selector</code> <code>OptimumSelector | None</code> <p>The optimum selector to use. If not given, use <code>ZeusCost</code> with \\eta=0.5.</p> <code>None</code> <code>wait_steps</code> <code>int</code> <p>Number of steps to pass by before doing anything at the beginning. Useful if you have something like <code>torch.backends.cudnn.benchmark=True</code>, because the first iteration won't be representative of the rest of the iterations.</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup iterations for each power limit.</p> <code>10</code> <code>profile_steps</code> <code>int</code> <p>Number of profie iterations for each power limit.</p> <code>40</code> <code>pl_step</code> <code>int</code> <p>The stride between power limits to explore, in unites of Watts.</p> <code>25</code> <code>profile_path</code> <code>str | Path | None</code> <p>If the path points to an existing file, load the profile from the file and do not run any profiling. If the path points to a non-existing file, profile and save the profile to the file. If <code>None</code>, do not save or load any profile.</p> <code>None</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(\n    self,\n    monitor: ZeusMonitor,\n    optimum_selector: OptimumSelector | None = None,\n    wait_steps: int = 1,\n    warmup_steps: int = 10,\n    profile_steps: int = 40,\n    pl_step: int = 25,\n    profile_path: str | Path | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the optimizer.\n\n    GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n    Args:\n        monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n        optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n        wait_steps: Number of steps to pass by before doing anything at the beginning.\n            Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n            because the first iteration won't be representative of the rest of the iterations.\n        warmup_steps: Number of warmup iterations for each power limit.\n        profile_steps: Number of profie iterations for each power limit.\n        pl_step: The stride between power limits to explore, in unites of Watts.\n        profile_path: If the path points to an existing file, load the profile from the file\n            and do not run any profiling. If the path points to a non-existing file, profile\n            and save the profile to the file. If `None`, do not save or load any profile.\n    \"\"\"\n    # Sanity checks.\n    if wait_steps &lt; 0:\n        raise ValueError(\"wait_steps must be non-negative.\")\n    if warmup_steps &lt; 0:\n        raise ValueError(\"warmup_steps must be non-negative.\")\n    if profile_steps &lt;= 0:\n        raise ValueError(\"profile_steps must be positive.\")\n    if pl_step &lt;= 0:\n        raise ValueError(\"pl_step must be positive.\")\n\n    self.monitor = monitor\n    self.optimum_selector = optimum_selector or ZeusCost(\n        eta_knob=0.5,\n        world_size=len(monitor.gpu_indices),\n    )\n    self.warmup_steps = warmup_steps\n    self.profile_steps = profile_steps\n    self.pl_step = pl_step * 1000  # Internally, we use milliWatts.\n    self.profile_path = (\n        Path(profile_path) if isinstance(profile_path, str) else profile_path\n    )\n\n    # Setup logging.\n    self.logger = get_logger(type(self).__name__)\n\n    # Set the range of power limits to explore.\n    # Assert that supported power limits ranges are uniform across GPUs.\n    gpus = get_gpus(ensure_homogeneous=True)\n    pls = []\n    for index in monitor.gpu_indices:\n        pls.append(gpus.getPowerManagementLimitConstraints(index))\n    if not all(pls[0] == pl for pl in pls):\n        raise ValueError(\"Power limits ranges are not uniform across GPUs.\")\n    self.power_limits = list(\n        range(pls[0][1], pls[0][0] - self.pl_step, -self.pl_step)\n    )\n\n    # Turn on persistence mode and set to the highest power limit.\n    try:\n        for index in monitor.gpu_indices:\n            gpus.setPersistenceMode(index, enable=True)\n    except ZeusGPUNoPermissionError as ze:\n        raise RuntimeError(\n            \"SYS_ADMIN capability is required to modify GPU power limits. \"\n            \"Using --cap-add SYS_ADMIN when running the Docker container \"\n            \"is the easiest way to do this.\"\n        ) from ze\n    self.current_power_limit = 0\n\n    # Store `Measurement` objects in a list, one for each power limit.\n    self.measurements: list[PowerLimitMeasurement] = []\n\n    # State for the profiler state machine.\n    self.state: Ready | Warmup | Profiling | Done\n\n    # Initialize JIT profiling states.\n    if self.profile_path is None:\n        self.logger.info(\"JIT profiling enabled.\")\n        self.logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n        self.state = Ready(\n            next_power_limit=self.power_limits[0], steps=wait_steps + 1\n        )\n        self.logger.info(\"Set power limit to the maximum before starting.\")\n        self._set_power_limit(max(self.power_limits))\n    elif not self.profile_path.exists():\n        self.logger.info(\n            \"JIT Profiling enabled. Profile will be saved to '%s'.\",\n            str(self.profile_path),\n        )\n        self.logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n        self.state = Ready(\n            next_power_limit=self.power_limits[0], steps=wait_steps + 1\n        )\n        self.logger.info(\"Set power limit to the maximum before starting.\")\n        self._set_power_limit(max(self.power_limits))\n    else:\n        self.measurements = _PowerLimitMeasurementList.parse_file(\n            self.profile_path,\n        ).measurements\n        # self.measurements = _PowerLimitMeasurementList.model_validate_json(\n        #     open(self.profile_path).read(),\n        #     strict=True,\n        # ).measurements\n        self.logger.info(\n            \"Loaded previous profiling results from '%s'.\", str(self.profile_path)\n        )\n        optimal_power_limit = self._compute_optimal_power_limit()\n        self.logger.info(\n            \"Optimal power limit is %d W.\", optimal_power_limit // 1000\n        )\n        self.state = Done(optimal_power_limit=optimal_power_limit)\n        self._set_power_limit(self.state.optimal_power_limit)\n\n    # Restore all GPUs back to their maximum power limit on exit.\n    atexit.register(lambda: self._set_power_limit(max(self.power_limits)))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Mark the end of a training epoch.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Mark the end of a training epoch.\"\"\"\n    if isinstance(self.state, Ready):\n        pass\n\n    elif isinstance(self.state, (Warmup, Profiling)):\n        # Warmup/Profiling stage interrupted by the end of an epoch.\n        self.logger.info(\n            \"%s phase for %d W interrupted by the end of a training epoch.\",\n            type(self.state).__name__,\n            self.state.current_power_limit // 1000,\n        )\n        if isinstance(self.state, Profiling):\n            self.monitor.end_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                cancel=True,\n            )\n        self.state = Ready(next_power_limit=self.state.current_power_limit, steps=1)\n        self._set_power_limit(max(self.power_limits))\n\n    elif isinstance(self.state, Done):\n        pass\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Mark the beginning of a training step.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Mark the beginning of a training step.\"\"\"\n    if isinstance(self.state, Ready):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            self.logger.info(\n                \"Starting warmup for power limit %d W.\",\n                self.state.next_power_limit // 1000,\n            )\n            self._set_power_limit(self.state.next_power_limit)\n            self.state = Warmup(\n                current_power_limit=self.state.next_power_limit,\n                steps=self.warmup_steps,\n            )\n\n    elif isinstance(self.state, Warmup):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            self.logger.info(\n                \"Starting actual profiling for power limit %d W.\",\n                self.state.current_power_limit // 1000,\n            )\n            self.state = Profiling(\n                current_power_limit=self.state.current_power_limit,\n                steps=self.profile_steps,\n            )\n            self.monitor.begin_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n            )\n\n    elif isinstance(self.state, Profiling):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            measurement = self.monitor.end_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n            )\n            self.logger.info(\n                \"Finished profiling for power limit %d W.\",\n                self.state.current_power_limit // 1000,\n            )\n            self.measurements.append(\n                PowerLimitMeasurement(\n                    power_limit=self.state.current_power_limit // 1000,\n                    energy=measurement.total_energy,\n                    time=measurement.time,\n                )\n            )\n            # If we're done profiling all power limits, compute the optimal\n            # power limit and transition to the Done state. Otherwise, move\n            # on to the Warmup phase for the next power limit.\n            current_power_limit_index = self.power_limits.index(\n                self.state.current_power_limit\n            )\n            if current_power_limit_index == len(self.power_limits) - 1:\n                self.state = Done(\n                    optimal_power_limit=self._compute_optimal_power_limit(),\n                )\n                self._set_power_limit(self.state.optimal_power_limit)\n                self._save_profile()\n            else:\n                next_power_limit = self.power_limits[current_power_limit_index + 1]\n                self.logger.info(\n                    \"Starting warmup for power limit %d W.\",\n                    next_power_limit // 1000,\n                )\n                self._set_power_limit(next_power_limit)\n                self.state = Warmup(\n                    current_power_limit=next_power_limit,\n                    steps=self.warmup_steps,\n                )\n\n    elif isinstance(self.state, Done):\n        pass\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._set_power_limit","title":"_set_power_limit","text":"<pre><code>_set_power_limit(power_limit)\n</code></pre> <p>Set the power limit for all GPUs.</p> <p>Parameters:</p> Name Type Description Default <code>power_limit</code> <code>int</code> <p>The power limit to set, in milliWatts.</p> required Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _set_power_limit(self, power_limit: int) -&gt; None:\n    \"\"\"Set the power limit for all GPUs.\n\n    Args:\n        power_limit: The power limit to set, in milliWatts.\n    \"\"\"\n    gpus = get_gpus()\n    self.logger.info(\"Setting power limit to %d W.\", power_limit // 1000)\n    if self.current_power_limit == power_limit:\n        return\n    for index in self.monitor.gpu_indices:\n        gpus.setPowerManagementLimit(index, power_limit)\n    self.current_power_limit = power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._compute_optimal_power_limit","title":"_compute_optimal_power_limit","text":"<pre><code>_compute_optimal_power_limit()\n</code></pre> <p>Compute the optimal power limit in milliWatts.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _compute_optimal_power_limit(self) -&gt; int:\n    \"\"\"Compute the optimal power limit in milliWatts.\"\"\"\n    optimal_power_limit = self.optimum_selector.select(self.measurements) * 1000\n    self.logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n    return optimal_power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._save_profile","title":"_save_profile","text":"<pre><code>_save_profile()\n</code></pre> <p>Save JIT profiling results and the optimal power limit to a JSON file.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _save_profile(self) -&gt; None:\n    \"\"\"Save JIT profiling results and the optimal power limit to a JSON file.\"\"\"\n    if self.profile_path is None:\n        return\n\n    assert isinstance(self.state, Done)\n    with self.profile_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(\n            _PowerLimitMeasurementList(measurements=self.measurements).json(\n                indent=4\n            ),\n        )\n    self.logger.info(\"JIT profiling results saved to '%s'.\", str(self.profile_path))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer","title":"HFGlobalPowerLimitOptimizer","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>[Wrapped for Hugging Face Trainer Callback] Optimizer for the power limit knob.</p> <p>This optimizer uses the JIT profiling log to determine the optimal power limit. See <code>GlobalPowerLimitOptimizer</code> for the underlying optimizer implementation.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class HFGlobalPowerLimitOptimizer(TrainerCallback):  # type: ignore\n    \"\"\"[Wrapped for Hugging Face Trainer Callback] Optimizer for the power limit knob.\n\n    This optimizer uses the JIT profiling log to determine the optimal power limit.\n    See [`GlobalPowerLimitOptimizer`][zeus.optimizer.power_limit.GlobalPowerLimitOptimizer]\n    for the underlying optimizer implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor: ZeusMonitor,\n        optimum_selector: OptimumSelector | None = None,\n        wait_steps: int = 1,\n        warmup_steps: int = 10,\n        profile_steps: int = 40,\n        pl_step: int = 25,\n        profile_path: str | Path | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the optimizer.\n\n        GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n        Args:\n            monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n            optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n            wait_steps: Number of steps to pass by before doing anything at the beginning.\n                Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n                because the first iteration won't be representative of the rest of the iterations.\n            warmup_steps: Number of warmup iterations for each power limit.\n            profile_steps: Number of profie iterations for each power limit.\n            pl_step: The stride between power limits to explore, in unites of Watts.\n            profile_path: If the path points to an existing file, load the profile from the file\n                and do not run any profiling. If the path points to a non-existing file, profile\n                and save the profile to the file. If `None`, do not save or load any profile.\n        \"\"\"\n        if not transformers_available:\n            raise ImportError(\n                \"The transformers package is not installed. Please install it to use the HFGlobalPowerLimitOptimizer.\"\n            )\n\n        self.optimizer = GlobalPowerLimitOptimizer(\n            monitor=monitor,\n            optimum_selector=optimum_selector,\n            wait_steps=wait_steps,\n            warmup_steps=warmup_steps,\n            profile_steps=profile_steps,\n            pl_step=pl_step,\n            profile_path=profile_path,\n        )\n\n    def on_epoch_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        model: PreTrainedModel,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Mark the end of a training epoch.\"\"\"\n        self.optimizer.on_epoch_end()\n\n    def on_step_begin(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        model: PreTrainedModel,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Mark the beginning of a training step.\"\"\"\n        self.optimizer.on_step_begin()\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    monitor,\n    optimum_selector=None,\n    wait_steps=1,\n    warmup_steps=10,\n    profile_steps=40,\n    pl_step=25,\n    profile_path=None,\n)\n</code></pre> <p>GPU indices to profile and optimize for are taken from <code>monitor.gpu_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance used to profile GPU time and energy consumption.</p> required <code>optimum_selector</code> <code>OptimumSelector | None</code> <p>The optimum selector to use. If not given, use <code>ZeusCost</code> with \\eta=0.5.</p> <code>None</code> <code>wait_steps</code> <code>int</code> <p>Number of steps to pass by before doing anything at the beginning. Useful if you have something like <code>torch.backends.cudnn.benchmark=True</code>, because the first iteration won't be representative of the rest of the iterations.</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup iterations for each power limit.</p> <code>10</code> <code>profile_steps</code> <code>int</code> <p>Number of profie iterations for each power limit.</p> <code>40</code> <code>pl_step</code> <code>int</code> <p>The stride between power limits to explore, in unites of Watts.</p> <code>25</code> <code>profile_path</code> <code>str | Path | None</code> <p>If the path points to an existing file, load the profile from the file and do not run any profiling. If the path points to a non-existing file, profile and save the profile to the file. If <code>None</code>, do not save or load any profile.</p> <code>None</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(\n    self,\n    monitor: ZeusMonitor,\n    optimum_selector: OptimumSelector | None = None,\n    wait_steps: int = 1,\n    warmup_steps: int = 10,\n    profile_steps: int = 40,\n    pl_step: int = 25,\n    profile_path: str | Path | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the optimizer.\n\n    GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n    Args:\n        monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n        optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n        wait_steps: Number of steps to pass by before doing anything at the beginning.\n            Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n            because the first iteration won't be representative of the rest of the iterations.\n        warmup_steps: Number of warmup iterations for each power limit.\n        profile_steps: Number of profie iterations for each power limit.\n        pl_step: The stride between power limits to explore, in unites of Watts.\n        profile_path: If the path points to an existing file, load the profile from the file\n            and do not run any profiling. If the path points to a non-existing file, profile\n            and save the profile to the file. If `None`, do not save or load any profile.\n    \"\"\"\n    if not transformers_available:\n        raise ImportError(\n            \"The transformers package is not installed. Please install it to use the HFGlobalPowerLimitOptimizer.\"\n        )\n\n    self.optimizer = GlobalPowerLimitOptimizer(\n        monitor=monitor,\n        optimum_selector=optimum_selector,\n        wait_steps=wait_steps,\n        warmup_steps=warmup_steps,\n        profile_steps=profile_steps,\n        pl_step=pl_step,\n        profile_path=profile_path,\n    )\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end(args, state, control, model, **kwargs)\n</code></pre> <p>Mark the end of a training epoch.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_epoch_end(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    model: PreTrainedModel,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Mark the end of a training epoch.\"\"\"\n    self.optimizer.on_epoch_end()\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin(args, state, control, model, **kwargs)\n</code></pre> <p>Mark the beginning of a training step.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_step_begin(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    model: PreTrainedModel,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Mark the beginning of a training step.\"\"\"\n    self.optimizer.on_step_begin()\n</code></pre>"},{"location":"reference/optimizer/batch_size/","title":"batch_size","text":""},{"location":"reference/optimizer/batch_size/#zeus.optimizer.batch_size","title":"zeus.optimizer.batch_size","text":"<p>Batch size optimizer server and client.</p>"},{"location":"reference/optimizer/batch_size/client/","title":"client","text":""},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client","title":"zeus.optimizer.batch_size.client","text":"<p>Zeus batch size optimizer client that communicates with server.</p>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer","title":"BatchSizeOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Batch size optimizer client that talks to server.</p> <p>The following methods must be called in order inside the training script:</p> <ul> <li><code>get_batch_size</code>: At the beginning of the script.</li> <li><code>on_train_begin</code>: Before running any epochs.</li> <li><code>on_evaluate</code>: After each epoch when the validation metric is available.</li> </ul> <p>One batch size optimizer per one training session of the job. The set of GPUs to be used for training should be homogeneous, and will be inferred from the <code>ZeusMonitor</code> instance passed into the constructor.</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>class BatchSizeOptimizer(Callback):\n    \"\"\"Batch size optimizer client that talks to server.\n\n    The following methods must be called in order inside the training script:\n\n    - `get_batch_size`: At the beginning of the script.\n    - `on_train_begin`: Before running any epochs.\n    - `on_evaluate`: After each epoch when the validation metric is available.\n\n    One batch size optimizer per one training session of the job.\n    The set of GPUs to be used for training should be homogeneous, and will be inferred\n    from the `ZeusMonitor` instance passed into the constructor.\n    \"\"\"\n\n    def __init__(\n        self, monitor: ZeusMonitor, server_url: str, job: JobSpec, rank: int = 0\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer, and register the job to the server.\n\n        If job is already registered, check if the job configuration is identical with previously registered config.\n\n        Args:\n            monitor: `ZeusMonitor` instance configured to measure the energy of all and only the GPUs used for training.\n            server_url: url of batch size optimizer server\n            job: job specification. Refer to `JobSpec` for job specifcatio parameters.\n            rank: rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.\n        \"\"\"\n        self.monitor = monitor\n        self.server_url = server_url\n        self.cur_epoch = 0  # 0-indexed\n        self.running_time = 0.0\n        self.consumed_energy = 0.0\n        self.training_finished = False\n        self.trial_number = 0\n        self.rank = rank\n\n        # Currently, the BSO only supports homogeneous GPU training.\n        gpus = get_gpus(ensure_homogeneous=True)\n        if len(gpus) == 0:\n            raise ZeusBSOConfigError(\"No GPUs detected.\")\n\n        # set gpu configurations(max_power, number of gpus, and gpu model)\n        self.job_spec = JobSpecFromClient(\n            **job.dict(),\n            max_power=gpus.getPowerManagementLimitConstraints(0)[1]\n            // 1000\n            * len(monitor.gpu_indices),\n            number_of_gpus=len(monitor.gpu_indices),\n            gpu_model=gpus.getName(0),\n        )\n\n        # Track the batch size of current job\n        self.current_batch_size = 0\n\n        # Register job\n        res = httpx.post(\n            self.server_url + REGISTER_JOB_URL, content=self.job_spec.json()\n        )\n        self._handle_response(res)\n\n        self.job = CreatedJob.parse_obj(res.json())\n\n        logger.critical(\n            \"Job is registered with job_id: \\x1b[31;1m%s\\x1b[0m\", self.job.job_id\n        )\n        logger.info(\"Job is registered: %s\", str(self.job))\n\n    def get_batch_size(self) -&gt; int:\n        \"\"\"Get batch size to use from the BSO server.\n\n        Returns:\n            return a batch size to use for current job\n\n        Raises:\n            `ZeusBSORuntimError`: if the batch size we receive is invalid\n        \"\"\"\n        if self.rank != 0:\n            raise ZeusBSOBadOperationError(\"Only rank 0 gpu can ask for a batch size.\")\n\n        if self.current_batch_size != 0:\n            # If we already got the batch size, return\n            return self.current_batch_size\n\n        self.cur_epoch = 0\n        res = httpx.get(\n            self.server_url + GET_NEXT_BATCH_SIZE_URL,\n            params={\"job_id\": self.job.job_id},\n        )\n        self._handle_response(res)\n        trial_id = TrialId.parse_obj(res.json())\n\n        if trial_id.batch_size not in self.job.batch_sizes:\n            raise ZeusBSORuntimError(\n                f\"Zeus server returned a strange batch_size: {trial_id.batch_size}\"\n            )\n\n        self.current_batch_size = trial_id.batch_size\n        self.trial_number = trial_id.trial_number\n\n        logger.info(\"[BatchSizeOptimizer] Chosen batch size: %s\", trial_id.batch_size)\n\n        def report_end() -&gt; None:\n            httpx.patch(self.server_url + REPORT_END_URL, content=trial_id.json())\n\n        atexit.register(report_end)\n        return trial_id.batch_size\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Start the monitor window and mark training is started.\"\"\"\n        self.training_finished = False\n        self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n\n    def on_evaluate(\n        self,\n        metric: float,\n    ) -&gt; None:\n        \"\"\"Determine whether or not to stop training after evaluation.\n\n        Training stops when\n        - `max_epochs` was reached, or\n        - the target metric was reached. or\n        - Cost exceeded the early stop threshold\n\n        Args:\n            metric: Validation metric of this epoch. See also `higher_metric_is_better` in [`JobParams`][zeus.optimizer.batch_size.common.JobParams].\n\n        Raises:\n            `ZeusBSOOperationOrderError`: When `get_batch_size` was not called first.\n            `ZeusBSOTrainFailError`: When train failed for a chosen batch size and should be stopped.\n                                    This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size\n            `ZeusBSORuntimError`: When the server returns an error\n        \"\"\"\n        if self.current_batch_size == 0:\n            raise ZeusBSOOperationOrderError(\n                \"Call get_batch_size to set the batch size first\"\n            )\n\n        if self.training_finished:\n            return\n\n        self.cur_epoch += 1\n        measurement = self.monitor.end_window(\"BatciSizeOptimizerClient\")\n\n        # Accumulate time and energy\n        self.running_time += measurement.time\n        self.consumed_energy += measurement.total_energy\n\n        training_result = TrainingResult(\n            job_id=self.job.job_id,\n            batch_size=self.current_batch_size,\n            trial_number=self.trial_number,\n            time=self.running_time,\n            energy=self.consumed_energy,\n            metric=metric,\n            current_epoch=self.cur_epoch,\n        )\n\n        # report to the server about the result of this training\n        res = httpx.post(\n            self.server_url + REPORT_RESULT_URL, content=training_result.json()\n        )\n        self._handle_response(res)\n\n        parsed_response = ReportResponse.parse_obj(res.json())\n\n        if not parsed_response.stop_train:\n            # Should keep training. Re-open the window\n            self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n        else:\n            # Train is over. If not converged, raise an error\n            self.training_finished = True\n            if not parsed_response.converged:\n                raise ZeusBSOTrainFailError(\n                    f\"Train failed: {parsed_response.message} This batch size will not be selected again. Please re-launch the training\"\n                )\n\n    def _handle_response(self, res: httpx.Response) -&gt; None:\n        \"\"\"Check if the response is success. Otherwise raise an error with error message from the server.\n\n        Args:\n            res: response from the server\n        \"\"\"\n        if not (200 &lt;= (code := res.status_code) &lt; 300):\n            raise ZeusBSORuntimError(\n                f\"Zeus server returned status code {code}: {res.text}\"\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(monitor, server_url, job, rank=0)\n</code></pre> <p>If job is already registered, check if the job configuration is identical with previously registered config.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance configured to measure the energy of all and only the GPUs used for training.</p> required <code>server_url</code> <code>str</code> <p>url of batch size optimizer server</p> required <code>job</code> <code>JobSpec</code> <p>job specification. Refer to <code>JobSpec</code> for job specifcatio parameters.</p> required <code>rank</code> <code>int</code> <p>rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.</p> <code>0</code> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def __init__(\n    self, monitor: ZeusMonitor, server_url: str, job: JobSpec, rank: int = 0\n) -&gt; None:\n    \"\"\"Initialize the optimizer, and register the job to the server.\n\n    If job is already registered, check if the job configuration is identical with previously registered config.\n\n    Args:\n        monitor: `ZeusMonitor` instance configured to measure the energy of all and only the GPUs used for training.\n        server_url: url of batch size optimizer server\n        job: job specification. Refer to `JobSpec` for job specifcatio parameters.\n        rank: rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.\n    \"\"\"\n    self.monitor = monitor\n    self.server_url = server_url\n    self.cur_epoch = 0  # 0-indexed\n    self.running_time = 0.0\n    self.consumed_energy = 0.0\n    self.training_finished = False\n    self.trial_number = 0\n    self.rank = rank\n\n    # Currently, the BSO only supports homogeneous GPU training.\n    gpus = get_gpus(ensure_homogeneous=True)\n    if len(gpus) == 0:\n        raise ZeusBSOConfigError(\"No GPUs detected.\")\n\n    # set gpu configurations(max_power, number of gpus, and gpu model)\n    self.job_spec = JobSpecFromClient(\n        **job.dict(),\n        max_power=gpus.getPowerManagementLimitConstraints(0)[1]\n        // 1000\n        * len(monitor.gpu_indices),\n        number_of_gpus=len(monitor.gpu_indices),\n        gpu_model=gpus.getName(0),\n    )\n\n    # Track the batch size of current job\n    self.current_batch_size = 0\n\n    # Register job\n    res = httpx.post(\n        self.server_url + REGISTER_JOB_URL, content=self.job_spec.json()\n    )\n    self._handle_response(res)\n\n    self.job = CreatedJob.parse_obj(res.json())\n\n    logger.critical(\n        \"Job is registered with job_id: \\x1b[31;1m%s\\x1b[0m\", self.job.job_id\n    )\n    logger.info(\"Job is registered: %s\", str(self.job))\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.get_batch_size","title":"get_batch_size","text":"<pre><code>get_batch_size()\n</code></pre> <p>Get batch size to use from the BSO server.</p> <p>Returns:</p> Type Description <code>int</code> <p>return a batch size to use for current job</p> <p>Raises:</p> Type Description <code>`ZeusBSORuntimError`</code> <p>if the batch size we receive is invalid</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def get_batch_size(self) -&gt; int:\n    \"\"\"Get batch size to use from the BSO server.\n\n    Returns:\n        return a batch size to use for current job\n\n    Raises:\n        `ZeusBSORuntimError`: if the batch size we receive is invalid\n    \"\"\"\n    if self.rank != 0:\n        raise ZeusBSOBadOperationError(\"Only rank 0 gpu can ask for a batch size.\")\n\n    if self.current_batch_size != 0:\n        # If we already got the batch size, return\n        return self.current_batch_size\n\n    self.cur_epoch = 0\n    res = httpx.get(\n        self.server_url + GET_NEXT_BATCH_SIZE_URL,\n        params={\"job_id\": self.job.job_id},\n    )\n    self._handle_response(res)\n    trial_id = TrialId.parse_obj(res.json())\n\n    if trial_id.batch_size not in self.job.batch_sizes:\n        raise ZeusBSORuntimError(\n            f\"Zeus server returned a strange batch_size: {trial_id.batch_size}\"\n        )\n\n    self.current_batch_size = trial_id.batch_size\n    self.trial_number = trial_id.trial_number\n\n    logger.info(\"[BatchSizeOptimizer] Chosen batch size: %s\", trial_id.batch_size)\n\n    def report_end() -&gt; None:\n        httpx.patch(self.server_url + REPORT_END_URL, content=trial_id.json())\n\n    atexit.register(report_end)\n    return trial_id.batch_size\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Start the monitor window and mark training is started.</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Start the monitor window and mark training is started.\"\"\"\n    self.training_finished = False\n    self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Determine whether or not to stop training after evaluation.</p> <p>Training stops when - <code>max_epochs</code> was reached, or - the target metric was reached. or - Cost exceeded the early stop threshold</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>float</code> <p>Validation metric of this epoch. See also <code>higher_metric_is_better</code> in <code>JobParams</code>.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOOperationOrderError`</code> <p>When <code>get_batch_size</code> was not called first.</p> <code>`ZeusBSOTrainFailError`</code> <p>When train failed for a chosen batch size and should be stopped.                     This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size</p> <code>`ZeusBSORuntimError`</code> <p>When the server returns an error</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def on_evaluate(\n    self,\n    metric: float,\n) -&gt; None:\n    \"\"\"Determine whether or not to stop training after evaluation.\n\n    Training stops when\n    - `max_epochs` was reached, or\n    - the target metric was reached. or\n    - Cost exceeded the early stop threshold\n\n    Args:\n        metric: Validation metric of this epoch. See also `higher_metric_is_better` in [`JobParams`][zeus.optimizer.batch_size.common.JobParams].\n\n    Raises:\n        `ZeusBSOOperationOrderError`: When `get_batch_size` was not called first.\n        `ZeusBSOTrainFailError`: When train failed for a chosen batch size and should be stopped.\n                                This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size\n        `ZeusBSORuntimError`: When the server returns an error\n    \"\"\"\n    if self.current_batch_size == 0:\n        raise ZeusBSOOperationOrderError(\n            \"Call get_batch_size to set the batch size first\"\n        )\n\n    if self.training_finished:\n        return\n\n    self.cur_epoch += 1\n    measurement = self.monitor.end_window(\"BatciSizeOptimizerClient\")\n\n    # Accumulate time and energy\n    self.running_time += measurement.time\n    self.consumed_energy += measurement.total_energy\n\n    training_result = TrainingResult(\n        job_id=self.job.job_id,\n        batch_size=self.current_batch_size,\n        trial_number=self.trial_number,\n        time=self.running_time,\n        energy=self.consumed_energy,\n        metric=metric,\n        current_epoch=self.cur_epoch,\n    )\n\n    # report to the server about the result of this training\n    res = httpx.post(\n        self.server_url + REPORT_RESULT_URL, content=training_result.json()\n    )\n    self._handle_response(res)\n\n    parsed_response = ReportResponse.parse_obj(res.json())\n\n    if not parsed_response.stop_train:\n        # Should keep training. Re-open the window\n        self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n    else:\n        # Train is over. If not converged, raise an error\n        self.training_finished = True\n        if not parsed_response.converged:\n            raise ZeusBSOTrainFailError(\n                f\"Train failed: {parsed_response.message} This batch size will not be selected again. Please re-launch the training\"\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer._handle_response","title":"_handle_response","text":"<pre><code>_handle_response(res)\n</code></pre> <p>Check if the response is success. Otherwise raise an error with error message from the server.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Response</code> <p>response from the server</p> required Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def _handle_response(self, res: httpx.Response) -&gt; None:\n    \"\"\"Check if the response is success. Otherwise raise an error with error message from the server.\n\n    Args:\n        res: response from the server\n    \"\"\"\n    if not (200 &lt;= (code := res.status_code) &lt; 300):\n        raise ZeusBSORuntimError(\n            f\"Zeus server returned status code {code}: {res.text}\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/","title":"common","text":""},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common","title":"zeus.optimizer.batch_size.common","text":"<p>Shared model definitions for the server and client.</p>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobParams","title":"JobParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Job parameters.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>unique ID for the job</p> <code>batch_sizes</code> <code>list[int]</code> <p>list of batch sizes to try</p> <code>default_batch_size</code> <code>int</code> <p>first batch size to try</p> <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) parameter for computing <code>zeus_cost</code></p> <code>beta_knob</code> <code>Optional[float]</code> <p>beta for early stopping. If min_cost*beta_knob &lt; current_cost, job will be stopped by bso server.         To disable, set it to None.</p> <code>target_metric</code> <code>float</code> <p>target metric to achieve for training.</p> <code>higher_is_better_metric</code> <code>bool</code> <p>if the goal of training is achieving higher metric than <code>target_metric</code></p> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs for a training run.</p> <code>num_pruning_rounds</code> <code>int</code> <p>Number of rounds we are trying for pruning stage</p> <code>window_size</code> <code>int</code> <p>For MAB, how many recent measurements to fetch for computing the arm states. If set to 0, fetch all measurements.</p> <code>mab_prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>mab_prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>mab_num_explorations</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> <code>mab_seed</code> <code>Optional[int]</code> <p>The random seed to use.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobParams(BaseModel):\n    r\"\"\"Job parameters.\n\n    Attributes:\n        job_id: unique ID for the job\n        batch_sizes: list of batch sizes to try\n        default_batch_size: first batch size to try\n        eta_knob: $\\eta$ parameter for computing `zeus_cost`\n        beta_knob: beta for early stopping. If min_cost*beta_knob &lt; current_cost, job will be stopped by bso server.\n                    To disable, set it to None.\n        target_metric: target metric to achieve for training.\n        higher_is_better_metric: if the goal of training is achieving higher metric than `target_metric`\n        max_epochs: Maximum number of epochs for a training run.\n        num_pruning_rounds: Number of rounds we are trying for pruning stage\n        window_size: For MAB, how many recent measurements to fetch for computing the arm states. If set to 0, fetch all measurements.\n\n        mab_prior_mean: Mean of the belief prior distribution.\n        mab_prior_precision: Precision of the belief prior distribution.\n        mab_num_explorations: How many static explorations to run when no observations are available.\n        mab_seed: The random seed to use.\n    \"\"\"\n\n    job_id: str\n    job_id_prefix: str\n    batch_sizes: list[int]\n    default_batch_size: int = Field(gt=0)\n    eta_knob: float = 0.5\n    beta_knob: Optional[float] = 2.0\n    target_metric: float = 0.50\n    higher_is_better_metric: bool = True\n    max_epochs: int = Field(default=100, gt=0)\n    num_pruning_rounds: int = Field(default=2, ge=0)\n    window_size: int = 0\n\n    mab_prior_mean: float = 0.0\n    mab_prior_precision: float = 0.0\n    mab_num_explorations: int = Field(default=2, ge=0)\n    mab_seed: Optional[int] = None\n\n    @validator(\"batch_sizes\")\n    def _validate_batch_sizes(cls, bs: list[int]) -&gt; list[int]:\n        if bs is not None and len(bs) &gt; 0:\n            bs.sort()\n            return bs\n        else:\n            raise ValueError(f\"Batch Sizes = {bs} is empty\")\n\n    @validator(\"eta_knob\")\n    def _validate_eta_knob(cls, v: float) -&gt; float:\n        if v &lt; 0 or v &gt; 1:\n            raise ValueError(\"eta_knob should be in range [0,1]\")\n        return v\n\n    @validator(\"beta_knob\")\n    def _validate_beta_knob(cls, v: float) -&gt; float:\n        if v is None or v &gt; 0:\n            return v\n        else:\n            raise ValueError(\n                f\"Invalid beta_knob({v}). To disable early stop, set beta_knob = None to disable or positive value.\"\n            )\n\n    @root_validator(skip_on_failure=True)\n    def _check_default_batch_size(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        bs = values[\"default_batch_size\"]\n        bss = values[\"batch_sizes\"]\n        if bs not in bss:\n            raise ValueError(f\"Default BS({bs}) not in batch_sizes({bss}).\")\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.GpuConfig","title":"GpuConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gpu configuration of current training.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class GpuConfig(BaseModel):\n    \"\"\"Gpu configuration of current training.\"\"\"\n\n    max_power: float = Field(gt=0)\n    number_of_gpus: int = Field(gt=0)\n    gpu_model: str\n\n    @validator(\"gpu_model\")\n    def _validate_gpu_model(cls, v: str) -&gt; str:\n        if v is None or v == \"\":\n            raise ValueError(f\"Invalid gpu_model({v}). Shouldn't be empty.\")\n        else:\n            return v\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobSpec","title":"JobSpec","text":"<p>               Bases: <code>JobParams</code></p> <p>Job specification that user inputs.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>Optional[str]</code> <p>ID of job. If none is provided, will be created by server.</p> <p>Refer to <code>JobParams</code> for other attributes.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobSpec(JobParams):\n    \"\"\"Job specification that user inputs.\n\n    Attributes:\n        job_id: ID of job. If none is provided, will be created by server.\n\n    Refer to [`JobParams`][zeus.optimizer.batch_size.common.JobParams] for other attributes.\n    \"\"\"\n\n    job_id: Optional[str]  # pyright: ignore[reportIncompatibleVariableOverride]\n\n    @root_validator(skip_on_failure=True)\n    def _check_job_id(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        job_id: str | None = values.get(\"job_id\")\n        prefix: str = values[\"job_id_prefix\"]\n\n        if job_id is not None and not job_id.startswith(prefix):\n            raise ValueError(f\"Job_id({job_id}) does not start with prefix({prefix}).\")\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobSpecFromClient","title":"JobSpecFromClient","text":"<p>               Bases: <code>JobSpec</code>, <code>GpuConfig</code></p> <p>Internal job configuration including gpu settings. Job Id is optional here.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobSpecFromClient(JobSpec, GpuConfig):\n    \"\"\"Internal job configuration including gpu settings. Job Id is optional here.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.CreatedJob","title":"CreatedJob","text":"<p>               Bases: <code>JobParams</code>, <code>GpuConfig</code></p> <p>Job configuration from the server. Job Id is required.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class CreatedJob(JobParams, GpuConfig):\n    \"\"\"Job configuration from the server. Job Id is required.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.TrialId","title":"TrialId","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response format from the server for getting a batch size to use, which is an unique idnetifier of trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>ID of job</p> <code>batch_size</code> <code>int</code> <p>batch size to use.</p> <code>trial_number</code> <code>int</code> <p>trial number of current training.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class TrialId(BaseModel):\n    \"\"\"Response format from the server for getting a batch size to use, which is an unique idnetifier of trial.\n\n    Attributes:\n        job_id: ID of job\n        batch_size: batch size to use.\n        trial_number: trial number of current training.\n    \"\"\"\n\n    job_id: str\n    batch_size: int\n    trial_number: int\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.TrainingResult","title":"TrainingResult","text":"<p>               Bases: <code>TrialId</code></p> <p>Result of training for that job &amp; batch size.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>total time consumption so far</p> <code>energy</code> <code>float</code> <p>total energy consumption so far</p> <code>metric</code> <code>float</code> <p>current metric value after <code>current_epoch</code></p> <code>current_epoch</code> <code>int</code> <p>current epoch of training. Server can check if the train reached the <code>max_epochs</code></p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class TrainingResult(TrialId):\n    \"\"\"Result of training for that job &amp; batch size.\n\n    Attributes:\n        time: total time consumption so far\n        energy: total energy consumption so far\n        metric: current metric value after `current_epoch`\n        current_epoch: current epoch of training. Server can check if the train reached the `max_epochs`\n    \"\"\"\n\n    time: float\n    energy: float\n    metric: float\n    current_epoch: int\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.ReportResponse","title":"ReportResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response format from the server for client's training result report.</p> <p>Attributes:</p> Name Type Description <code>stop_train</code> <code>bool</code> <p>Whether we should stop training or not.</p> <code>converged</code> <code>bool</code> <p>Whether the target metric has been reached.</p> <code>message</code> <code>str</code> <p>message from the server regarding training. ex) why train should be stopped.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class ReportResponse(BaseModel):\n    \"\"\"Response format from the server for client's training result report.\n\n    Attributes:\n        stop_train: Whether we should stop training or not.\n        converged: Whether the target metric has been reached.\n        message: message from the server regarding training. ex) why train should be stopped.\n    \"\"\"\n\n    stop_train: bool\n    converged: bool\n    message: str\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/","title":"exceptions","text":""},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions","title":"zeus.optimizer.batch_size.exceptions","text":"<p>Zeus batch size optimizer client exceptions.</p>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSORuntimError","title":"ZeusBSORuntimError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Bso server failed to process the request correctly.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSORuntimError(ZeusBaseError):\n    \"\"\"Bso server failed to process the request correctly.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOTrainFailError","title":"ZeusBSOTrainFailError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Training failed for the chosen batch_size.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOTrainFailError(ZeusBaseError):\n    \"\"\"Training failed for the chosen batch_size.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOConfigError","title":"ZeusBSOConfigError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Configuration of training doesn't meet the requirements. ex) heterogeneous GPU.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOConfigError(ZeusBaseError):\n    \"\"\"Configuration of training doesn't meet the requirements. ex) heterogeneous GPU.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOOperationOrderError","title":"ZeusBSOOperationOrderError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Order of calling methods of BatchSizeOptimizer is wrong.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOOperationOrderError(ZeusBaseError):\n    \"\"\"Order of calling methods of BatchSizeOptimizer is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOBadOperationError","title":"ZeusBSOBadOperationError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>The usage of operations is wrong.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOBadOperationError(ZeusBaseError):\n    \"\"\"The usage of operations is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/","title":"server","text":""},{"location":"reference/optimizer/batch_size/server/#zeus.optimizer.batch_size.server","title":"zeus.optimizer.batch_size.server","text":"<p>Zeus batch size optimizer server.</p> <p>[Description]</p> <p>Batch size optimizer is in composed of server and client. The reason for server-client architecture is that we need to maintain the states accross all trainings. Therefore, we need a central place that is not limited by a scope of one training. </p> <p>Server's role is maintaining and updating the state of the job based on client's report. There are three types of states.  </p> <ul> <li>Job related states: <code>JobState</code></li> <li>Trial related states: <code>Trial</code></li> <li>MAB related states: <code>GaussianTsArmState</code></li> </ul> <p>Client's role is letting the server know that the user started a new training and report the result of training. </p> <p>[Structure of code]</p> <p>Each domain (batch_size_state and job) is composed of repository, commands, and models. </p> <ul> <li>Repository is the lowest layer that modifies the DB. It provides CRUD operations, and performs corresponding sql operation for a given request.</li> <li>Commands are the command (collection of arguments) to use each method in repository. It is mainly used to validate the request.</li> <li>Models are used to safely perform operations on objects. All ORM objects can be converted into these models and we also have some helper models. </li> </ul> <p>In services directory, we have a single service, <code>ZeusService</code> which performs one or more operations of repositories. It performs business logics, and provides more complicated operations to application layer. It also has commands that validates requests of using service's method.</p> <p>[Hierarchy of program]</p> <pre><code>                            | Application layer     | Business logic | DB operation               | Storage\n                            |                       |                |                            |\nClient request -&gt; Router -&gt; | Optimizer -&gt; Explorer | -&gt; ZeusService | -&gt; JobStateRepository      | &lt;-&gt; DB\n                            |           -&gt; Mab      |                | -&gt; BatchSizeStateRepository|\n                            |                       |                |                            |\n</code></pre> <p>[Database Transaction]</p> <p>Each session represent a single transaction. When Fastapi receives the request, it creates a single session. Then, at the end of the request, it commits every operations to Database.</p>"},{"location":"reference/optimizer/batch_size/server/config/","title":"config","text":""},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config","title":"zeus.optimizer.batch_size.server.config","text":"<p>Server global configurations.</p>"},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config.ZeusBsoSettings","title":"ZeusBsoSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>App setting.</p> <p>Attributes:</p> Name Type Description <code>database_url</code> <code>str</code> <p>url of database for the server</p> <code>echo_sql</code> <code>Union[bool, str]</code> <p>log sql statements it executes</p> <code>log_level</code> <code>str</code> <p>level of log</p> Source code in <code>zeus/optimizer/batch_size/server/config.py</code> <pre><code>class ZeusBsoSettings(BaseSettings):\n    \"\"\"App setting.\n\n    Attributes:\n        database_url: url of database for the server\n        echo_sql: log sql statements it executes\n        log_level: level of log\n    \"\"\"\n\n    database_url: str\n    echo_sql: Union[bool, str] = False  # To prevent conversion error for empty string\n    log_level: str = \"INFO\"\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Set how to find the env variables and how to parse it.\n        \"\"\"\n\n        env_prefix = \"ZEUS_BSO_\"\n        env_file = find_dotenv(filename=\".env\")\n        env_file_encoding = \"utf-8\"\n\n    @validator(\"echo_sql\")\n    def _validate_echo_sql(cls, v) -&gt; bool:\n        if v is not None and isinstance(v, bool):\n            return v\n        elif v is not None and isinstance(v, str):\n            if v.lower() == \"false\":\n                return False\n            elif v.lower() == \"true\":\n                return True\n        return False\n\n    @validator(\"log_level\")\n    def _validate_log_level(cls, v) -&gt; str:\n        if v is None or v not in {\n            \"NOTSET\",\n            \"DEBUG\",\n            \"INFO\",\n            \"WARN\",\n            \"ERROR\",\n            \"CRITICAL\",\n        }:\n            # Default log level\n            return \"INFO\"\n        return v\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config.ZeusBsoSettings.Config","title":"Config","text":"<p>Model configuration.</p> <p>Set how to find the env variables and how to parse it.</p> Source code in <code>zeus/optimizer/batch_size/server/config.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Set how to find the env variables and how to parse it.\n    \"\"\"\n\n    env_prefix = \"ZEUS_BSO_\"\n    env_file = find_dotenv(filename=\".env\")\n    env_file_encoding = \"utf-8\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/","title":"exceptions","text":""},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions","title":"zeus.optimizer.batch_size.server.exceptions","text":"<p>Zeus server exceptions.</p>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerBaseError","title":"ZeusBSOServerBaseError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Base error class for BSO server.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerBaseError(ZeusBaseError):\n    \"\"\"Base error class for BSO server.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerBaseError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError","title":"ZeusBSOJobConfigMismatchError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the job configuration doesn't align for the same job_id.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOJobConfigMismatchError(ZeusBSOServerBaseError):\n    \"\"\"When the job configuration doesn't align for the same job_id.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 409\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 409\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError","title":"ZeusBSOValueError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the certain value is invalid.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOValueError(ZeusBSOServerBaseError):\n    \"\"\"When the certain value is invalid.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFoundError","title":"ZeusBSOServerNotFoundError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>Resource we are looking for is not found.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerNotFoundError(ZeusBSOServerBaseError):\n    \"\"\"Resource we are looking for is not found.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 404\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 404\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError","title":"ZeusBSOServiceBadOperationError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the operation doesn't meet requirements. ex) fetching measurements before fetching a job.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServiceBadOperationError(ZeusBSOServerBaseError):\n    \"\"\"When the operation doesn't meet requirements. ex) fetching measurements before fetching a job.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerRuntimeError","title":"ZeusBSOServerRuntimeError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>Initialization or other errors during runtime.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerRuntimeError(ZeusBSOServerBaseError):\n    \"\"\"Initialization or other errors during runtime.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerRuntimeError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/","title":"explorer","text":""},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer","title":"zeus.optimizer.batch_size.server.explorer","text":"<p>Provides report/next_batch_size during pruning stage.</p>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager","title":"PruningExploreManager","text":"<p>Pruning manager that manges the batch size states in pruning stage.</p> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>class PruningExploreManager:\n    \"\"\"Pruning manager that manges the batch size states in pruning stage.\"\"\"\n\n    def __init__(self, service: ZeusService):\n        \"\"\"Set up zeus service.\"\"\"\n        self.service = service\n\n    async def next_batch_size(\n        self,\n        job: JobState,\n        exploration_history: ExplorationsPerJob,\n    ) -&gt; ReadTrial | list[int]:\n        \"\"\"Find the next batch size to explore.\n\n        Three cases possible.\n        1. Pruninig Stage : There is a batch size that has not explored during the round.\n        2. Concurrent job : There is an exploration with \"Dispatched\" state.\n        3. Mab stage : All batch sizes have been explored and round is over.\n\n        Args:\n            job: state of the job\n            exploration_history: all \"succeeded\" explorations that we have done for that job\n\n        Returns:\n            Return the batch size to use during Pruning stage.\n            If Pruning stage was over, return None.\n\n        Raises:\n            `ZeusBSOValueError`: If the value is invalid. EX) default batch size is not in the converged batch size list.\n        \"\"\"\n        batch_sizes = job.batch_sizes\n        exp_default_bs = job.default_batch_size\n\n        for round in range(job.num_pruning_rounds):\n            converged_bs_list = []\n\n            min_cost_of_round = float(\"inf\")\n            min_batch_size_of_round = 0\n\n            batch_sizes.sort()\n            idx = batch_sizes.index(exp_default_bs)\n            down = sorted(batch_sizes[: idx + 1], reverse=True)\n            up = sorted(batch_sizes[idx + 1 :])\n\n            for bs_list in [down, up]:\n                for bs in bs_list:\n                    if (\n                        bs in exploration_history.explorations_per_bs\n                        and len(exploration_history.explorations_per_bs[bs]) &gt; round\n                    ):\n                        # Already explored at this round\n                        if (\n                            exploration_history.explorations_per_bs[bs][round].status\n                            == TrialStatus.Dispatched\n                        ):\n                            # We are waiting for the result of this exploration -&gt; Concurrent job!\n                            return await self.service.create_trial(\n                                CreateConcurrentTrial(\n                                    job_id=job.job_id,\n                                    batch_size=job.min_cost_batch_size,\n                                )\n                            )\n\n                        if not exploration_history.explorations_per_bs[bs][\n                            round\n                        ].converged:\n                            # Failed to converge -&gt; Go to next list or round\n                            break\n                        else:\n                            # Training converged.\n                            converged_bs_list.append(bs)\n\n                            m = exploration_history.explorations_per_bs[bs][round]\n                            if m.energy is None or m.time is None:\n                                raise ZeusBSOValueError(\n                                    \"Energy or time is not available for the exploration.\"\n                                )\n                            cost = zeus_cost(\n                                m.energy, m.time, job.eta_knob, job.max_power\n                            )\n                            if cost &lt; min_cost_of_round:\n                                min_cost_of_round = cost\n                                min_batch_size_of_round = bs\n\n                    else:\n                        # Did not explore this round. Should explore!\n                        return await self.service.create_trial(\n                            CreateExplorationTrial(\n                                job_id=job.job_id,\n                                batch_size=bs,\n                            )\n                        )\n\n            # We should go to next round. Update exp_default_bs and batch sizes!\n            exp_default_bs = min_batch_size_of_round\n            batch_sizes = converged_bs_list\n\n            logger.info(\n                \"[PruningExploreManager] go to next round(%d) new default bs = %d converged bs list = %s\",\n                round,\n                exp_default_bs,\n                batch_sizes,\n            )\n\n            if len(batch_sizes) == 0:\n                raise ZeusBSOServerRuntimeError(\n                    \"No converged batch sizes has observed. Reconfigure batch_sizes and re-launch the job.\"\n                )\n        # After going through pruning rounds, we couldn't find the bs. Should go to MAB stage, so return good batch_sizes.\n        return sorted(batch_sizes)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>def __init__(self, service: ZeusService):\n    \"\"\"Set up zeus service.\"\"\"\n    self.service = service\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager.next_batch_size","title":"next_batch_size  <code>async</code>","text":"<pre><code>next_batch_size(job, exploration_history)\n</code></pre> <p>Find the next batch size to explore.</p> <p>Three cases possible. 1. Pruninig Stage : There is a batch size that has not explored during the round. 2. Concurrent job : There is an exploration with \"Dispatched\" state. 3. Mab stage : All batch sizes have been explored and round is over.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of the job</p> required <code>exploration_history</code> <code>ExplorationsPerJob</code> <p>all \"succeeded\" explorations that we have done for that job</p> required <p>Returns:</p> Type Description <code>ReadTrial | list[int]</code> <p>Return the batch size to use during Pruning stage.</p> <code>ReadTrial | list[int]</code> <p>If Pruning stage was over, return None.</p> <p>Raises:</p> Type Description <code>`ZeusBSOValueError`</code> <p>If the value is invalid. EX) default batch size is not in the converged batch size list.</p> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>async def next_batch_size(\n    self,\n    job: JobState,\n    exploration_history: ExplorationsPerJob,\n) -&gt; ReadTrial | list[int]:\n    \"\"\"Find the next batch size to explore.\n\n    Three cases possible.\n    1. Pruninig Stage : There is a batch size that has not explored during the round.\n    2. Concurrent job : There is an exploration with \"Dispatched\" state.\n    3. Mab stage : All batch sizes have been explored and round is over.\n\n    Args:\n        job: state of the job\n        exploration_history: all \"succeeded\" explorations that we have done for that job\n\n    Returns:\n        Return the batch size to use during Pruning stage.\n        If Pruning stage was over, return None.\n\n    Raises:\n        `ZeusBSOValueError`: If the value is invalid. EX) default batch size is not in the converged batch size list.\n    \"\"\"\n    batch_sizes = job.batch_sizes\n    exp_default_bs = job.default_batch_size\n\n    for round in range(job.num_pruning_rounds):\n        converged_bs_list = []\n\n        min_cost_of_round = float(\"inf\")\n        min_batch_size_of_round = 0\n\n        batch_sizes.sort()\n        idx = batch_sizes.index(exp_default_bs)\n        down = sorted(batch_sizes[: idx + 1], reverse=True)\n        up = sorted(batch_sizes[idx + 1 :])\n\n        for bs_list in [down, up]:\n            for bs in bs_list:\n                if (\n                    bs in exploration_history.explorations_per_bs\n                    and len(exploration_history.explorations_per_bs[bs]) &gt; round\n                ):\n                    # Already explored at this round\n                    if (\n                        exploration_history.explorations_per_bs[bs][round].status\n                        == TrialStatus.Dispatched\n                    ):\n                        # We are waiting for the result of this exploration -&gt; Concurrent job!\n                        return await self.service.create_trial(\n                            CreateConcurrentTrial(\n                                job_id=job.job_id,\n                                batch_size=job.min_cost_batch_size,\n                            )\n                        )\n\n                    if not exploration_history.explorations_per_bs[bs][\n                        round\n                    ].converged:\n                        # Failed to converge -&gt; Go to next list or round\n                        break\n                    else:\n                        # Training converged.\n                        converged_bs_list.append(bs)\n\n                        m = exploration_history.explorations_per_bs[bs][round]\n                        if m.energy is None or m.time is None:\n                            raise ZeusBSOValueError(\n                                \"Energy or time is not available for the exploration.\"\n                            )\n                        cost = zeus_cost(\n                            m.energy, m.time, job.eta_knob, job.max_power\n                        )\n                        if cost &lt; min_cost_of_round:\n                            min_cost_of_round = cost\n                            min_batch_size_of_round = bs\n\n                else:\n                    # Did not explore this round. Should explore!\n                    return await self.service.create_trial(\n                        CreateExplorationTrial(\n                            job_id=job.job_id,\n                            batch_size=bs,\n                        )\n                    )\n\n        # We should go to next round. Update exp_default_bs and batch sizes!\n        exp_default_bs = min_batch_size_of_round\n        batch_sizes = converged_bs_list\n\n        logger.info(\n            \"[PruningExploreManager] go to next round(%d) new default bs = %d converged bs list = %s\",\n            round,\n            exp_default_bs,\n            batch_sizes,\n        )\n\n        if len(batch_sizes) == 0:\n            raise ZeusBSOServerRuntimeError(\n                \"No converged batch sizes has observed. Reconfigure batch_sizes and re-launch the job.\"\n            )\n    # After going through pruning rounds, we couldn't find the bs. Should go to MAB stage, so return good batch_sizes.\n    return sorted(batch_sizes)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/","title":"mab","text":""},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab","title":"zeus.optimizer.batch_size.server.mab","text":"<p>Thompson Sampling policy for Gaussian bandits. MAB related logic is implented here.</p>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS","title":"GaussianTS","text":"<p>Thompson Sampling policy for Gaussian bandits.</p> <p>For each arm, the reward is modeled as a Gaussian distribution with known precision. The conjugate priors are also Gaussian distributions.</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>class GaussianTS:\n    \"\"\"Thompson Sampling policy for Gaussian bandits.\n\n    For each arm, the reward is modeled as a Gaussian distribution with\n    known precision. The conjugate priors are also Gaussian distributions.\n    \"\"\"\n\n    def __init__(self, service: ZeusService):\n        \"\"\"Set up zeus service to interact with database.\"\"\"\n        self.service = service\n        self.name = \"GaussianTS\"\n\n    def _fit_arm(\n        self,\n        bs_base: BatchSizeBase,\n        prior_mean: float,\n        prior_precision: float,\n        rewards: np.ndarray,\n    ) -&gt; GaussianTsArmState:\n        \"\"\"Update the parameter distribution for one arm.\n\n        Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n        Args:\n            bs_base: job id and batch size tha represents this arm\n            prior_mean: Mean of the belief prior distribution.\n            prior_precision: Precision of the belief prior distribution.\n            rewards: Array of rewards observed by pulling that arm.\n\n        Returns:\n            Updated arm state\n        \"\"\"\n        if len(rewards) == 0:\n            raise ZeusBSOValueError(\"No rewards to fit the arm.\")\n\n        variance = np.var(rewards)\n        reward_prec = np.inf if variance == 0.0 else np.reciprocal(variance)\n\n        # Reset to priors\n        mean = prior_mean\n        prec = prior_precision\n\n        # Compute the parameters of the posterior distribution.\n        # The reward distribution's precision is given as infinite only when we\n        # have exactly one observation for the arm, s.t. sampling yields that\n        # exact observation.\n        if reward_prec == np.inf:\n            new_prec = np.inf\n            new_mean = rewards.mean()\n        else:\n            new_prec = prec + len(rewards) * reward_prec\n            new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n        # Updated state.\n        return GaussianTsArmState(\n            job_id=bs_base.job_id,\n            batch_size=bs_base.batch_size,\n            param_mean=new_mean,\n            param_precision=new_prec,\n            reward_precision=reward_prec,\n            num_observations=len(rewards),\n        )\n\n    def predict(\n        self,\n        job_id: str,\n        prior_precision: float,\n        num_exploration: int,\n        arms: list[GaussianTsArmState],\n    ) -&gt; int:\n        \"\"\"Return the arm with the largest sampled expected reward.\n\n        Args:\n            job_id: job id\n            prior_precision: Precision of the belief prior distribution.\n            num_exploration: How many static explorations to run when no observations are available.\n            arms: list of arms\n\n        Returns:\n            batch size to use\n        \"\"\"\n        arm_dict = {arm.batch_size: arm for arm in arms}\n\n        # Exploration-only phase.\n        # Order is random considering concurrent bandit scenarios.\n        choices = self.service.get_random_choices(\n            GetRandomChoices(job_id=job_id, choices=[arm.batch_size for arm in arms])\n        )\n\n        for arm in choices:\n            if arm_dict[arm].num_observations &lt; num_exploration:\n                logger.info(\"[%s] Explore arm %s.\", self.name, str(arm))\n                return arm\n\n        # Thomopson Sampling phase.\n        # Sample the expected reward for each arm.\n        # Assumes that each arm has been explored at least once. Otherwise,\n        # a value will be sampled from the prior.\n\n        expectations = {}  # A mapping from every arm to their sampled expected reward.\n        for arm in arms:\n            if arm.param_precision == prior_precision:\n                logger.warning(\n                    \"predict_expectations called when arm '%d' is cold.\",\n                    arm.batch_size,\n                    stacklevel=1,\n                )\n            expectations[arm.batch_size] = self.service.get_normal(\n                GetNormal(\n                    job_id=job_id,\n                    loc=arm.param_mean,\n                    scale=np.sqrt(np.reciprocal(arm.param_precision)),\n                )\n            )\n\n        logger.info(\"[%s] Sampled mean rewards:\", self.name)\n        for arm, sample in expectations.items():\n            logger.info(\n                \"[%s] Arm %d: mu ~ N(%.2f, %.2f) -&gt; %.2f\",\n                self.name,\n                arm,\n                arm_dict[arm].param_mean,\n                1 / arm_dict[arm].param_precision,\n                sample,\n            )\n\n        bs = max(expectations, key=expectations.get)  # type: ignore\n        logger.info(\"%s in Thompson Sampling stage -&gt; BS = %d\", job_id, bs)\n        return bs\n\n    async def construct_mab(\n        self, job: JobState, evidence: ExplorationsPerJob, good_bs: list[int]\n    ) -&gt; list[GaussianTsArmState]:\n        \"\"\"Construct arms and initialize them.\n\n        Args:\n            job: state of job.\n            evidence: Completed explorations. We create arms based on the explorations we have done during pruning stage.\n            good_bs: Converged batch size list.\n\n        Returns:\n            list of arms that we created\n\n        Raises:\n            `ValueError`: If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)\n            `ZeusBSOValueError`: No converged batch sizes from pruning stage.\n        \"\"\"\n        if job.job_id != evidence.job_id:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Job Id is not consistent: job({job.job_id}) != explorations({evidence.job_id})\"\n            )\n\n        if len(good_bs) == 0:\n            raise ZeusBSOValueError(\"While creating arms, no batch size is selected\")\n\n        logger.info(\n            \"Construct MAB for %s with arms %s\",\n            job.job_id,\n            str(good_bs),\n        )\n\n        new_arms: list[GaussianTsArmState] = []\n\n        # Fit the arm for each good batch size.\n        for _, bs in enumerate(good_bs):\n            rewards = []\n            # Collect rewards starting from the most recent ones and backwards.\n            for trial in evidence.explorations_per_bs[bs]:\n                if trial.energy is None or trial.time is None:\n                    raise ZeusBSOValueError(\n                        f\"Trial {trial.trial_number} has no energy or time set.\"\n                    )\n                rewards.append(\n                    -zeus_cost(trial.energy, trial.time, job.eta_knob, job.max_power)\n                )\n\n            new_arms.append(\n                # create an arm\n                self._fit_arm(\n                    BatchSizeBase(job_id=job.job_id, batch_size=bs),\n                    job.mab_prior_mean,\n                    job.mab_prior_precision,\n                    np.array(rewards),\n                )\n            )\n\n        # submit new arms to db\n        self.service.create_arms(new_arms)\n        # update job stage from pruning to mab since we created arms\n        self.service.update_job_stage(\n            UpdateJobStage(job_id=job.job_id, stage=Stage.MAB)\n        )\n        return new_arms\n\n    async def report(self, job: JobState, trial_result: UpdateTrial) -&gt; None:\n        \"\"\"Based on the measurement, update the arm state.\n\n        Args:\n            job: state of the job\n            trial_result: result of training (job id, batch_size, trial_number)\n\n        Raises:\n            `ZeusBSOValueError`: When the arm (job id, batch_size) doesn't exist\n        \"\"\"\n        if trial_result.energy is None or trial_result.time is None:\n            raise ZeusBSOValueError(\n                f\"Trial {trial_result.trial_number} has no energy or time set.\"\n            )\n\n        # Since we're learning the reward precision, we need to\n        # 1. re-compute the precision of this arm based on the reward history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all the reward history.\n        # Note that `arm_rewards` always has more than one entry (and hence a\n        # non-zero variance) because we've been through pruning exploration.\n        batch_size_key = BatchSizeBase(\n            job_id=job.job_id, batch_size=trial_result.batch_size\n        )\n\n        # Get measurements of this bs in descending order. At most window_size length\n        history = await self.service.get_trial_results_of_bs(batch_size_key)\n\n        if len(history.results) &gt;= job.window_size and job.window_size &gt; 0:\n            # if the history is already above the window size, pop the last one to leave the spot for the current measurement.\n            history.results.pop()\n            history.results.reverse()  # Now ascending order.\n\n        costs = [\n            -zeus_cost(m.energy, m.time, job.eta_knob, job.max_power)\n            for m in history.results\n        ]\n        # Add current measurement to the costs\n        costs.append(\n            -zeus_cost(\n                trial_result.energy, trial_result.time, job.eta_knob, job.max_power\n            )\n        )\n        arm_rewards = np.array(costs)\n\n        logger.info(\"Arm_rewards: %s\", str(arm_rewards))\n\n        # Get current arm.\n        arm = await self.service.get_arm(batch_size_key)\n\n        if arm is None:\n            raise ZeusBSOValueError(\n                f\"MAB stage but Arm for batch size({trial_result.batch_size}) is not found.\"\n            )\n\n        # Get a new arm state based on observation\n        new_arm = self._fit_arm(\n            batch_size_key, job.mab_prior_mean, job.mab_prior_precision, arm_rewards\n        )\n\n        # update the new arm state in db\n        self.service.update_arm_state(\n            UpdateArm(\n                trial=ReadTrial(\n                    job_id=trial_result.job_id,\n                    batch_size=trial_result.batch_size,\n                    trial_number=trial_result.trial_number,\n                ),\n                updated_arm=new_arm,\n            )\n        )\n        # update corresponding trial\n        self.service.update_trial(trial_result)\n\n        arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n        logger.info(\n            \"%s @ %d: arm_rewards = [%s], reward_prec = %.2f\",\n            job.job_id,\n            trial_result.batch_size,\n            arm_rewards_repr,\n            new_arm.reward_precision,\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def __init__(self, service: ZeusService):\n    \"\"\"Set up zeus service to interact with database.\"\"\"\n    self.service = service\n    self.name = \"GaussianTS\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS._fit_arm","title":"_fit_arm","text":"<pre><code>_fit_arm(bs_base, prior_mean, prior_precision, rewards)\n</code></pre> <p>Update the parameter distribution for one arm.</p> <p>Reference: https://en.wikipedia.org/wiki/Conjugate_prior</p> <p>Parameters:</p> Name Type Description Default <code>bs_base</code> <code>BatchSizeBase</code> <p>job id and batch size tha represents this arm</p> required <code>prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> required <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> required <code>rewards</code> <code>ndarray</code> <p>Array of rewards observed by pulling that arm.</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState</code> <p>Updated arm state</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def _fit_arm(\n    self,\n    bs_base: BatchSizeBase,\n    prior_mean: float,\n    prior_precision: float,\n    rewards: np.ndarray,\n) -&gt; GaussianTsArmState:\n    \"\"\"Update the parameter distribution for one arm.\n\n    Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n    Args:\n        bs_base: job id and batch size tha represents this arm\n        prior_mean: Mean of the belief prior distribution.\n        prior_precision: Precision of the belief prior distribution.\n        rewards: Array of rewards observed by pulling that arm.\n\n    Returns:\n        Updated arm state\n    \"\"\"\n    if len(rewards) == 0:\n        raise ZeusBSOValueError(\"No rewards to fit the arm.\")\n\n    variance = np.var(rewards)\n    reward_prec = np.inf if variance == 0.0 else np.reciprocal(variance)\n\n    # Reset to priors\n    mean = prior_mean\n    prec = prior_precision\n\n    # Compute the parameters of the posterior distribution.\n    # The reward distribution's precision is given as infinite only when we\n    # have exactly one observation for the arm, s.t. sampling yields that\n    # exact observation.\n    if reward_prec == np.inf:\n        new_prec = np.inf\n        new_mean = rewards.mean()\n    else:\n        new_prec = prec + len(rewards) * reward_prec\n        new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n    # Updated state.\n    return GaussianTsArmState(\n        job_id=bs_base.job_id,\n        batch_size=bs_base.batch_size,\n        param_mean=new_mean,\n        param_precision=new_prec,\n        reward_precision=reward_prec,\n        num_observations=len(rewards),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.predict","title":"predict","text":"<pre><code>predict(job_id, prior_precision, num_exploration, arms)\n</code></pre> <p>Return the arm with the largest sampled expected reward.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>job id</p> required <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> required <code>num_exploration</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> required <code>arms</code> <code>list[GaussianTsArmState]</code> <p>list of arms</p> required <p>Returns:</p> Type Description <code>int</code> <p>batch size to use</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def predict(\n    self,\n    job_id: str,\n    prior_precision: float,\n    num_exploration: int,\n    arms: list[GaussianTsArmState],\n) -&gt; int:\n    \"\"\"Return the arm with the largest sampled expected reward.\n\n    Args:\n        job_id: job id\n        prior_precision: Precision of the belief prior distribution.\n        num_exploration: How many static explorations to run when no observations are available.\n        arms: list of arms\n\n    Returns:\n        batch size to use\n    \"\"\"\n    arm_dict = {arm.batch_size: arm for arm in arms}\n\n    # Exploration-only phase.\n    # Order is random considering concurrent bandit scenarios.\n    choices = self.service.get_random_choices(\n        GetRandomChoices(job_id=job_id, choices=[arm.batch_size for arm in arms])\n    )\n\n    for arm in choices:\n        if arm_dict[arm].num_observations &lt; num_exploration:\n            logger.info(\"[%s] Explore arm %s.\", self.name, str(arm))\n            return arm\n\n    # Thomopson Sampling phase.\n    # Sample the expected reward for each arm.\n    # Assumes that each arm has been explored at least once. Otherwise,\n    # a value will be sampled from the prior.\n\n    expectations = {}  # A mapping from every arm to their sampled expected reward.\n    for arm in arms:\n        if arm.param_precision == prior_precision:\n            logger.warning(\n                \"predict_expectations called when arm '%d' is cold.\",\n                arm.batch_size,\n                stacklevel=1,\n            )\n        expectations[arm.batch_size] = self.service.get_normal(\n            GetNormal(\n                job_id=job_id,\n                loc=arm.param_mean,\n                scale=np.sqrt(np.reciprocal(arm.param_precision)),\n            )\n        )\n\n    logger.info(\"[%s] Sampled mean rewards:\", self.name)\n    for arm, sample in expectations.items():\n        logger.info(\n            \"[%s] Arm %d: mu ~ N(%.2f, %.2f) -&gt; %.2f\",\n            self.name,\n            arm,\n            arm_dict[arm].param_mean,\n            1 / arm_dict[arm].param_precision,\n            sample,\n        )\n\n    bs = max(expectations, key=expectations.get)  # type: ignore\n    logger.info(\"%s in Thompson Sampling stage -&gt; BS = %d\", job_id, bs)\n    return bs\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.construct_mab","title":"construct_mab  <code>async</code>","text":"<pre><code>construct_mab(job, evidence, good_bs)\n</code></pre> <p>Construct arms and initialize them.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of job.</p> required <code>evidence</code> <code>ExplorationsPerJob</code> <p>Completed explorations. We create arms based on the explorations we have done during pruning stage.</p> required <code>good_bs</code> <code>list[int]</code> <p>Converged batch size list.</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>list of arms that we created</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)</p> <code>`ZeusBSOValueError`</code> <p>No converged batch sizes from pruning stage.</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>async def construct_mab(\n    self, job: JobState, evidence: ExplorationsPerJob, good_bs: list[int]\n) -&gt; list[GaussianTsArmState]:\n    \"\"\"Construct arms and initialize them.\n\n    Args:\n        job: state of job.\n        evidence: Completed explorations. We create arms based on the explorations we have done during pruning stage.\n        good_bs: Converged batch size list.\n\n    Returns:\n        list of arms that we created\n\n    Raises:\n        `ValueError`: If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)\n        `ZeusBSOValueError`: No converged batch sizes from pruning stage.\n    \"\"\"\n    if job.job_id != evidence.job_id:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Job Id is not consistent: job({job.job_id}) != explorations({evidence.job_id})\"\n        )\n\n    if len(good_bs) == 0:\n        raise ZeusBSOValueError(\"While creating arms, no batch size is selected\")\n\n    logger.info(\n        \"Construct MAB for %s with arms %s\",\n        job.job_id,\n        str(good_bs),\n    )\n\n    new_arms: list[GaussianTsArmState] = []\n\n    # Fit the arm for each good batch size.\n    for _, bs in enumerate(good_bs):\n        rewards = []\n        # Collect rewards starting from the most recent ones and backwards.\n        for trial in evidence.explorations_per_bs[bs]:\n            if trial.energy is None or trial.time is None:\n                raise ZeusBSOValueError(\n                    f\"Trial {trial.trial_number} has no energy or time set.\"\n                )\n            rewards.append(\n                -zeus_cost(trial.energy, trial.time, job.eta_knob, job.max_power)\n            )\n\n        new_arms.append(\n            # create an arm\n            self._fit_arm(\n                BatchSizeBase(job_id=job.job_id, batch_size=bs),\n                job.mab_prior_mean,\n                job.mab_prior_precision,\n                np.array(rewards),\n            )\n        )\n\n    # submit new arms to db\n    self.service.create_arms(new_arms)\n    # update job stage from pruning to mab since we created arms\n    self.service.update_job_stage(\n        UpdateJobStage(job_id=job.job_id, stage=Stage.MAB)\n    )\n    return new_arms\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.report","title":"report  <code>async</code>","text":"<pre><code>report(job, trial_result)\n</code></pre> <p>Based on the measurement, update the arm state.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of the job</p> required <code>trial_result</code> <code>UpdateTrial</code> <p>result of training (job id, batch_size, trial_number)</p> required <p>Raises:</p> Type Description <code>`ZeusBSOValueError`</code> <p>When the arm (job id, batch_size) doesn't exist</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>async def report(self, job: JobState, trial_result: UpdateTrial) -&gt; None:\n    \"\"\"Based on the measurement, update the arm state.\n\n    Args:\n        job: state of the job\n        trial_result: result of training (job id, batch_size, trial_number)\n\n    Raises:\n        `ZeusBSOValueError`: When the arm (job id, batch_size) doesn't exist\n    \"\"\"\n    if trial_result.energy is None or trial_result.time is None:\n        raise ZeusBSOValueError(\n            f\"Trial {trial_result.trial_number} has no energy or time set.\"\n        )\n\n    # Since we're learning the reward precision, we need to\n    # 1. re-compute the precision of this arm based on the reward history,\n    # 2. update the arm's reward precision\n    # 3. and `fit` the new MAB instance on all the reward history.\n    # Note that `arm_rewards` always has more than one entry (and hence a\n    # non-zero variance) because we've been through pruning exploration.\n    batch_size_key = BatchSizeBase(\n        job_id=job.job_id, batch_size=trial_result.batch_size\n    )\n\n    # Get measurements of this bs in descending order. At most window_size length\n    history = await self.service.get_trial_results_of_bs(batch_size_key)\n\n    if len(history.results) &gt;= job.window_size and job.window_size &gt; 0:\n        # if the history is already above the window size, pop the last one to leave the spot for the current measurement.\n        history.results.pop()\n        history.results.reverse()  # Now ascending order.\n\n    costs = [\n        -zeus_cost(m.energy, m.time, job.eta_knob, job.max_power)\n        for m in history.results\n    ]\n    # Add current measurement to the costs\n    costs.append(\n        -zeus_cost(\n            trial_result.energy, trial_result.time, job.eta_knob, job.max_power\n        )\n    )\n    arm_rewards = np.array(costs)\n\n    logger.info(\"Arm_rewards: %s\", str(arm_rewards))\n\n    # Get current arm.\n    arm = await self.service.get_arm(batch_size_key)\n\n    if arm is None:\n        raise ZeusBSOValueError(\n            f\"MAB stage but Arm for batch size({trial_result.batch_size}) is not found.\"\n        )\n\n    # Get a new arm state based on observation\n    new_arm = self._fit_arm(\n        batch_size_key, job.mab_prior_mean, job.mab_prior_precision, arm_rewards\n    )\n\n    # update the new arm state in db\n    self.service.update_arm_state(\n        UpdateArm(\n            trial=ReadTrial(\n                job_id=trial_result.job_id,\n                batch_size=trial_result.batch_size,\n                trial_number=trial_result.trial_number,\n            ),\n            updated_arm=new_arm,\n        )\n    )\n    # update corresponding trial\n    self.service.update_trial(trial_result)\n\n    arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n    logger.info(\n        \"%s @ %d: arm_rewards = [%s], reward_prec = %.2f\",\n        job.job_id,\n        trial_result.batch_size,\n        arm_rewards_repr,\n        new_arm.reward_precision,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer","title":"zeus.optimizer.batch_size.server.optimizer","text":"<p>Batch size optimizer top-most layer that provides register/report/predict.</p>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer","title":"ZeusBatchSizeOptimizer","text":"<p>Batch size optimizer server. Manages which stage the job is in and call corresponding manager (pruning or mab).</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>class ZeusBatchSizeOptimizer:\n    \"\"\"Batch size optimizer server. Manages which stage the job is in and call corresponding manager (pruning or mab).\"\"\"\n\n    def __init__(self, service: ZeusService) -&gt; None:\n        \"\"\"Initialize the server. Set the service, pruning manager, and mab.\n\n        Args:\n            service: ZeusService for interacting with database\n        \"\"\"\n        self.service = service\n        self.pruning_manager = PruningExploreManager(service)\n        self.mab = GaussianTS(service)\n\n    async def register_job(self, job: JobSpecFromClient) -&gt; bool:\n        \"\"\"Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.\n\n        Args:\n            job: job configuration\n\n        Returns:\n            True if a job is regiested, False if a job already exists and identical with previous configuration\n\n        Raises:\n            [`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]: In the case of existing job, if job configuration doesn't match with previously registered config\n        \"\"\"\n        registered_job = None\n\n        if job.job_id is None:\n            while True:\n                job.job_id = f\"{job.job_id_prefix}-{hashlib.sha256(str(time.time()).encode()).hexdigest()[:8]}\"\n                if (await self.service.get_job(job.job_id)) is None:\n                    break\n        else:\n            registered_job = await self.service.get_job(job.job_id)\n\n        if registered_job is not None:\n            # Job exists\n            logger.info(\"Job(%s) already exists\", job.job_id)\n            registerd_job_config = JobSpecFromClient.parse_obj(registered_job.dict())\n\n            # check if it is identical\n            if registerd_job_config != job:\n                raise ZeusBSOJobConfigMismatchError(\n                    \"JobSpec doesn't match with existing jobSpec. Use a new job_id for different configuration\"\n                )\n            return False\n\n        self.service.create_job(CreateJob.from_job_config(job))\n        logger.info(\"Registered %s\", job.job_id)\n\n        return True\n\n    async def predict(self, job_id: str) -&gt; TrialId:\n        \"\"\"Return a batch size to use.\n\n        Args:\n            job_id: Id of job\n\n        Returns:\n            batch size to use\n\n        Raises:\n            [`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]: If the job id is unknown, or creating a mab failed due to no converged batch size\n        \"\"\"\n        job = await self.service.get_job(job_id)\n\n        if job is None:\n            raise ZeusBSOValueError(\n                f\"Unknown job({job_id}). Please register the job first\"\n            )\n\n        if job.stage == Stage.MAB:\n            # If we are in MAB stage, use mab to get the next batch size\n            arms = await self.service.get_arms(job_id)\n            next_trial = await self.service.create_trial(\n                CreateMabTrial(\n                    job_id=job_id,\n                    batch_size=self.mab.predict(\n                        job_id, job.mab_prior_precision, job.mab_num_explorations, arms\n                    ),\n                )\n            )\n        else:\n            # Pruning stage\n            explorations = await self.service.get_explorations_of_job(job_id)\n            # First check if pruning explorer can give us any batch size. Returns batch_size or MAB to indicate going to MAB stage\n            res = await self.pruning_manager.next_batch_size(job, explorations)\n\n            if isinstance(res, list):\n                # MAB stage: construct MAB and update the job stage to MAB. Return the batch size from MAB\n                logger.info(\"Constructing a MAB\")\n                arms = await self.mab.construct_mab(job, explorations, res)\n                next_trial = await self.service.create_trial(\n                    CreateMabTrial(\n                        job_id=job_id,\n                        batch_size=self.mab.predict(\n                            job_id,\n                            job.mab_prior_precision,\n                            job.mab_num_explorations,\n                            arms,\n                        ),\n                    )\n                )\n            else:\n                next_trial = res\n\n        return TrialId(\n            job_id=next_trial.job_id,\n            batch_size=next_trial.batch_size,\n            trial_number=next_trial.trial_number,\n        )\n\n    async def report(self, result: TrainingResult) -&gt; ReportResponse:\n        \"\"\"Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.\n\n        Args:\n            result: result of training [`TrainingResult`][zeus.optimizer.batch_size.common.TrainingResult].\n\n        Returns:\n            Decision on training [`ReportResponse`][zeus.optimizer.batch_size.common.ReportResponse].\n        \"\"\"\n        cost_ub = np.inf\n        job = await self.service.get_job(result.job_id)\n        if job is None:\n            raise ZeusBSOServiceBadOperationError(f\"Unknown job {result.job_id}\")\n\n        trial = await self.service.get_trial(\n            ReadTrial(\n                job_id=result.job_id,\n                batch_size=result.batch_size,\n                trial_number=result.trial_number,\n            )\n        )\n        if trial is None:\n            raise ZeusBSOServiceBadOperationError(f\"Unknown trial {result}\")\n\n        if trial.status != TrialStatus.Dispatched:\n            # result is already reported\n            if trial.converged is None:\n                raise ZeusBSOValueError(\n                    f\"Trial({trial.trial_number}) is already reported but converged is not set.\"\n                )\n            return ReportResponse(\n                stop_train=True,\n                converged=trial.converged,\n                message=f\"Result for this trial({trial.trial_number}) is already reported.\",\n            )\n\n        if job.beta_knob is not None and job.min_cost is not None:  # Early stop enabled\n            cost_ub = job.beta_knob * job.min_cost\n\n        reported_cost = zeus_cost(\n            result.energy,\n            result.time,\n            job.eta_knob,\n            job.max_power,\n        )\n\n        within_cost_range = cost_ub &gt;= reported_cost\n        converged = (\n            job.higher_is_better_metric and job.target_metric &lt;= result.metric\n        ) or (not job.higher_is_better_metric and job.target_metric &gt;= result.metric)\n\n        if (\n            within_cost_range\n            and result.current_epoch &lt; job.max_epochs\n            and not converged\n        ):\n            # If it's not converged but below cost upper bound and haven't reached max_epochs, keep training\n\n            return ReportResponse(\n                stop_train=False,\n                converged=False,\n                message=\"Stop condition not met, keep training\",\n            )\n\n        # Two cases below here (training ended)\n        # 1. Converged == true\n        # 2. reached max_epoch OR excceded upper bound cost (error case)\n        if converged and within_cost_range:\n            message = \"Train succeeded\"\n        elif not within_cost_range:\n            message = f\"\"\"Batch Size({result.batch_size}) exceeded the cost upper bound: current cost({reported_cost}) &gt;\n                    beta_knob({job.beta_knob})*min_cost({job.min_cost})\"\"\"\n        else:\n            # not converged\n            message = f\"Train failed to converge within max_epoch({job.max_epochs})\"\n\n        trial_result = UpdateTrial(\n            job_id=result.job_id,\n            batch_size=result.batch_size,\n            status=TrialStatus.Succeeded,\n            trial_number=result.trial_number,\n            time=result.time,\n            energy=result.energy,\n            converged=converged and within_cost_range,\n        )\n\n        if job.stage == Stage.MAB:\n            await self.mab.report(job, trial_result)\n        else:\n            # Pruning stage\n            logger.info(\n                \"%s in pruning stage, Current BS %s that did %s converge.\",\n                result.job_id,\n                result.batch_size,\n                \"not\" * (not converged),\n            )\n            # update trial\n            self.service.update_trial(trial_result)\n\n        assert trial_result.converged is not None, \"This just set to boolean above.\"\n        return ReportResponse(\n            stop_train=True, converged=trial_result.converged, message=message\n        )\n\n    async def end_trial(self, trial_id: TrialId) -&gt; None:\n        \"\"\"Mark the trial as finished. If status is still `Dispatched` make the trial as `Failed`.\n\n        Args:\n            trial_id: Unique identifier of trial\n\n        Raises:\n            [`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]: If there is no corresponding trial.\n        \"\"\"\n        trial = await self.service.get_trial(ReadTrial(**trial_id.dict()))\n\n        if trial is not None:\n            if trial.status == TrialStatus.Dispatched:\n                self.service.update_trial(\n                    UpdateTrial(\n                        job_id=trial_id.job_id,\n                        batch_size=trial_id.batch_size,\n                        trial_number=trial_id.trial_number,\n                        status=TrialStatus.Failed,\n                    )\n                )\n        else:\n            raise ZeusBSOServerNotFoundError(f\"Could not find the trial: {trial_id}\")\n\n    async def delete_job(self, job_id: str) -&gt; None:\n        \"\"\"Delete a job.\n\n        Args:\n            job_id: ID of a job.\n\n        Returns:\n            True if the job is deleted. False if none was deleted\n        \"\"\"\n        if not (await self.service.delete_job(job_id)):\n            raise ZeusBSOServerNotFoundError(\"No job was deleted.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>ZeusService</code> <p>ZeusService for interacting with database</p> required Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>def __init__(self, service: ZeusService) -&gt; None:\n    \"\"\"Initialize the server. Set the service, pruning manager, and mab.\n\n    Args:\n        service: ZeusService for interacting with database\n    \"\"\"\n    self.service = service\n    self.pruning_manager = PruningExploreManager(service)\n    self.mab = GaussianTS(service)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(job)\n</code></pre> <p>Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobSpecFromClient</code> <p>job configuration</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a job is regiested, False if a job already exists and identical with previous configuration</p> <p>Raises:</p> Type Description <code>[`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]</code> <p>In the case of existing job, if job configuration doesn't match with previously registered config</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def register_job(self, job: JobSpecFromClient) -&gt; bool:\n    \"\"\"Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.\n\n    Args:\n        job: job configuration\n\n    Returns:\n        True if a job is regiested, False if a job already exists and identical with previous configuration\n\n    Raises:\n        [`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]: In the case of existing job, if job configuration doesn't match with previously registered config\n    \"\"\"\n    registered_job = None\n\n    if job.job_id is None:\n        while True:\n            job.job_id = f\"{job.job_id_prefix}-{hashlib.sha256(str(time.time()).encode()).hexdigest()[:8]}\"\n            if (await self.service.get_job(job.job_id)) is None:\n                break\n    else:\n        registered_job = await self.service.get_job(job.job_id)\n\n    if registered_job is not None:\n        # Job exists\n        logger.info(\"Job(%s) already exists\", job.job_id)\n        registerd_job_config = JobSpecFromClient.parse_obj(registered_job.dict())\n\n        # check if it is identical\n        if registerd_job_config != job:\n            raise ZeusBSOJobConfigMismatchError(\n                \"JobSpec doesn't match with existing jobSpec. Use a new job_id for different configuration\"\n            )\n        return False\n\n    self.service.create_job(CreateJob.from_job_config(job))\n    logger.info(\"Registered %s\", job.job_id)\n\n    return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(job_id)\n</code></pre> <p>Return a batch size to use.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Id of job</p> required <p>Returns:</p> Type Description <code>TrialId</code> <p>batch size to use</p> <p>Raises:</p> Type Description <code>[`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]</code> <p>If the job id is unknown, or creating a mab failed due to no converged batch size</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def predict(self, job_id: str) -&gt; TrialId:\n    \"\"\"Return a batch size to use.\n\n    Args:\n        job_id: Id of job\n\n    Returns:\n        batch size to use\n\n    Raises:\n        [`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]: If the job id is unknown, or creating a mab failed due to no converged batch size\n    \"\"\"\n    job = await self.service.get_job(job_id)\n\n    if job is None:\n        raise ZeusBSOValueError(\n            f\"Unknown job({job_id}). Please register the job first\"\n        )\n\n    if job.stage == Stage.MAB:\n        # If we are in MAB stage, use mab to get the next batch size\n        arms = await self.service.get_arms(job_id)\n        next_trial = await self.service.create_trial(\n            CreateMabTrial(\n                job_id=job_id,\n                batch_size=self.mab.predict(\n                    job_id, job.mab_prior_precision, job.mab_num_explorations, arms\n                ),\n            )\n        )\n    else:\n        # Pruning stage\n        explorations = await self.service.get_explorations_of_job(job_id)\n        # First check if pruning explorer can give us any batch size. Returns batch_size or MAB to indicate going to MAB stage\n        res = await self.pruning_manager.next_batch_size(job, explorations)\n\n        if isinstance(res, list):\n            # MAB stage: construct MAB and update the job stage to MAB. Return the batch size from MAB\n            logger.info(\"Constructing a MAB\")\n            arms = await self.mab.construct_mab(job, explorations, res)\n            next_trial = await self.service.create_trial(\n                CreateMabTrial(\n                    job_id=job_id,\n                    batch_size=self.mab.predict(\n                        job_id,\n                        job.mab_prior_precision,\n                        job.mab_num_explorations,\n                        arms,\n                    ),\n                )\n            )\n        else:\n            next_trial = res\n\n    return TrialId(\n        job_id=next_trial.job_id,\n        batch_size=next_trial.batch_size,\n        trial_number=next_trial.trial_number,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.report","title":"report  <code>async</code>","text":"<pre><code>report(result)\n</code></pre> <p>Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TrainingResult</code> <p>result of training <code>TrainingResult</code>.</p> required <p>Returns:</p> Type Description <code>ReportResponse</code> <p>Decision on training <code>ReportResponse</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def report(self, result: TrainingResult) -&gt; ReportResponse:\n    \"\"\"Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.\n\n    Args:\n        result: result of training [`TrainingResult`][zeus.optimizer.batch_size.common.TrainingResult].\n\n    Returns:\n        Decision on training [`ReportResponse`][zeus.optimizer.batch_size.common.ReportResponse].\n    \"\"\"\n    cost_ub = np.inf\n    job = await self.service.get_job(result.job_id)\n    if job is None:\n        raise ZeusBSOServiceBadOperationError(f\"Unknown job {result.job_id}\")\n\n    trial = await self.service.get_trial(\n        ReadTrial(\n            job_id=result.job_id,\n            batch_size=result.batch_size,\n            trial_number=result.trial_number,\n        )\n    )\n    if trial is None:\n        raise ZeusBSOServiceBadOperationError(f\"Unknown trial {result}\")\n\n    if trial.status != TrialStatus.Dispatched:\n        # result is already reported\n        if trial.converged is None:\n            raise ZeusBSOValueError(\n                f\"Trial({trial.trial_number}) is already reported but converged is not set.\"\n            )\n        return ReportResponse(\n            stop_train=True,\n            converged=trial.converged,\n            message=f\"Result for this trial({trial.trial_number}) is already reported.\",\n        )\n\n    if job.beta_knob is not None and job.min_cost is not None:  # Early stop enabled\n        cost_ub = job.beta_knob * job.min_cost\n\n    reported_cost = zeus_cost(\n        result.energy,\n        result.time,\n        job.eta_knob,\n        job.max_power,\n    )\n\n    within_cost_range = cost_ub &gt;= reported_cost\n    converged = (\n        job.higher_is_better_metric and job.target_metric &lt;= result.metric\n    ) or (not job.higher_is_better_metric and job.target_metric &gt;= result.metric)\n\n    if (\n        within_cost_range\n        and result.current_epoch &lt; job.max_epochs\n        and not converged\n    ):\n        # If it's not converged but below cost upper bound and haven't reached max_epochs, keep training\n\n        return ReportResponse(\n            stop_train=False,\n            converged=False,\n            message=\"Stop condition not met, keep training\",\n        )\n\n    # Two cases below here (training ended)\n    # 1. Converged == true\n    # 2. reached max_epoch OR excceded upper bound cost (error case)\n    if converged and within_cost_range:\n        message = \"Train succeeded\"\n    elif not within_cost_range:\n        message = f\"\"\"Batch Size({result.batch_size}) exceeded the cost upper bound: current cost({reported_cost}) &gt;\n                beta_knob({job.beta_knob})*min_cost({job.min_cost})\"\"\"\n    else:\n        # not converged\n        message = f\"Train failed to converge within max_epoch({job.max_epochs})\"\n\n    trial_result = UpdateTrial(\n        job_id=result.job_id,\n        batch_size=result.batch_size,\n        status=TrialStatus.Succeeded,\n        trial_number=result.trial_number,\n        time=result.time,\n        energy=result.energy,\n        converged=converged and within_cost_range,\n    )\n\n    if job.stage == Stage.MAB:\n        await self.mab.report(job, trial_result)\n    else:\n        # Pruning stage\n        logger.info(\n            \"%s in pruning stage, Current BS %s that did %s converge.\",\n            result.job_id,\n            result.batch_size,\n            \"not\" * (not converged),\n        )\n        # update trial\n        self.service.update_trial(trial_result)\n\n    assert trial_result.converged is not None, \"This just set to boolean above.\"\n    return ReportResponse(\n        stop_train=True, converged=trial_result.converged, message=message\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.end_trial","title":"end_trial  <code>async</code>","text":"<pre><code>end_trial(trial_id)\n</code></pre> <p>Mark the trial as finished. If status is still <code>Dispatched</code> make the trial as <code>Failed</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trial_id</code> <code>TrialId</code> <p>Unique identifier of trial</p> required <p>Raises:</p> Type Description <code>[`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]</code> <p>If there is no corresponding trial.</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def end_trial(self, trial_id: TrialId) -&gt; None:\n    \"\"\"Mark the trial as finished. If status is still `Dispatched` make the trial as `Failed`.\n\n    Args:\n        trial_id: Unique identifier of trial\n\n    Raises:\n        [`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]: If there is no corresponding trial.\n    \"\"\"\n    trial = await self.service.get_trial(ReadTrial(**trial_id.dict()))\n\n    if trial is not None:\n        if trial.status == TrialStatus.Dispatched:\n            self.service.update_trial(\n                UpdateTrial(\n                    job_id=trial_id.job_id,\n                    batch_size=trial_id.batch_size,\n                    trial_number=trial_id.trial_number,\n                    status=TrialStatus.Failed,\n                )\n            )\n    else:\n        raise ZeusBSOServerNotFoundError(f\"Could not find the trial: {trial_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete a job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of a job.</p> required <p>Returns:</p> Type Description <code>None</code> <p>True if the job is deleted. False if none was deleted</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; None:\n    \"\"\"Delete a job.\n\n    Args:\n        job_id: ID of a job.\n\n    Returns:\n        True if the job is deleted. False if none was deleted\n    \"\"\"\n    if not (await self.service.delete_job(job_id)):\n        raise ZeusBSOServerNotFoundError(\"No job was deleted.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/","title":"router","text":""},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router","title":"zeus.optimizer.batch_size.server.router","text":"<p>Zeus batch size optimizer server FAST API router.</p>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.get_job_locks","title":"get_job_locks","text":"<pre><code>get_job_locks()\n</code></pre> <p>Get global job locks.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>def get_job_locks() -&gt; defaultdict[str, asyncio.Lock]:\n    \"\"\"Get global job locks.\"\"\"\n    return JOB_LOCKS\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.get_prefix_locks","title":"get_prefix_locks","text":"<pre><code>get_prefix_locks()\n</code></pre> <p>Get global job Id prefix locks.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>def get_prefix_locks() -&gt; defaultdict[str, asyncio.Lock]:\n    \"\"\"Get global job Id prefix locks.\"\"\"\n    return PREFIX_LOCKS\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(\n    job,\n    response,\n    db_session=Depends(get_db_session),\n    prefix_locks=Depends(get_prefix_locks),\n)\n</code></pre> <p>Endpoint for users to register a job or check if the job is registered and configuration is identical.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.post(\n    REGISTER_JOB_URL,\n    responses={\n        200: {\"description\": \"Job is already registered\"},\n        201: {\"description\": \"Job is successfully registered\"},\n    },\n    response_model=JobSpecFromClient,\n)\nasync def register_job(\n    job: JobSpecFromClient,\n    response: Response,\n    db_session: AsyncSession = Depends(get_db_session),\n    prefix_locks: defaultdict[str, asyncio.Lock] = Depends(get_prefix_locks),\n):\n    \"\"\"Endpoint for users to register a job or check if the job is registered and configuration is identical.\"\"\"\n    async with prefix_locks[job.job_id_prefix]:\n        # One lock for registering a job. To prevent getting a same lock\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            created = await optimizer.register_job(job)\n            await db_session.commit()\n            if created:\n                # new job is created\n                response.status_code = status.HTTP_201_CREATED\n            else:\n                # job already exists\n                response.status_code = status.HTTP_200_OK\n            return job\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(\n    job_id,\n    db_session=Depends(get_db_session),\n    job_locks=Depends(get_job_locks),\n)\n</code></pre> <p>Endpoint for users to delete a job.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.delete(DELETE_JOB_URL)\nasync def delete_job(\n    job_id: str,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to delete a job.\"\"\"\n    async with job_locks[job_id]:\n        try:\n            optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n            await optimizer.delete_job(job_id)\n            await db_session.commit()\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n        finally:\n            job_locks.pop(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.end_trial","title":"end_trial  <code>async</code>","text":"<pre><code>end_trial(\n    trial,\n    db_session=Depends(get_db_session),\n    job_locks=Depends(get_job_locks),\n)\n</code></pre> <p>Endpoint for users to end the trial.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.patch(REPORT_END_URL)\nasync def end_trial(\n    trial: TrialId,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to end the trial.\"\"\"\n    async with job_locks[trial.job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            await optimizer.end_trial(trial)\n            await db_session.commit()\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(\n    job_id,\n    db_session=Depends(get_db_session),\n    job_locks=Depends(get_job_locks),\n)\n</code></pre> <p>Endpoint for users to receive a batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.get(GET_NEXT_BATCH_SIZE_URL, response_model=TrialId)\nasync def predict(\n    job_id: str,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to receive a batch size.\"\"\"\n    async with job_locks[job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            res = await optimizer.predict(job_id)\n            await db_session.commit()\n            return res\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.report","title":"report  <code>async</code>","text":"<pre><code>report(\n    result,\n    db_session=Depends(get_db_session),\n    job_locks=Depends(get_job_locks),\n)\n</code></pre> <p>Endpoint for users to report the training result.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.post(REPORT_RESULT_URL, response_model=ReportResponse)\nasync def report(\n    result: TrainingResult,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to report the training result.\"\"\"\n    async with job_locks[result.job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            logger.info(\"Report with result %s\", str(result))\n            res = await optimizer.report(result)\n            await db_session.commit()\n            return res\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/","title":"batch_size_state","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/#zeus.optimizer.batch_size.server.batch_size_state","title":"zeus.optimizer.batch_size.server.batch_size_state","text":"<p>Batch size state models, repository, and commands.</p> <p>Batch size state includes trials and GaussianTs states.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands","title":"zeus.optimizer.batch_size.server.batch_size_state.commands","text":"<p>Commands to use <code>BatchSizeStateRepository</code>.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.ReadTrial","title":"ReadTrial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Command to read a trial.</p> <p>Equivalent to primary key of Trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <p>ID of job</p> <code>batch_size</code> <p>batch size of a given trial</p> <code>trial_number</code> <code>int</code> <p>number of trial</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class ReadTrial(BatchSizeBase):\n    \"\"\"Command to read a trial.\n\n    Equivalent to primary key of Trial.\n\n    Attributes:\n        job_id: ID of job\n        batch_size: batch size of a given trial\n        trial_number: number of trial\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrialBase","title":"CreateTrialBase","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Base command to create trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateTrialBase(BatchSizeBase):\n    \"\"\"Base command to create trial.\"\"\"\n\n    type: TrialType\n    start_timestamp: datetime = Field(default_factory=datetime.now)\n    status: TrialStatus = Field(default=TrialStatus.Dispatched, const=True)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrialBase.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial","title":"CreateTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Internal command to create trial.</p> <p>trial_number is populate within ZeusService.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateTrial(CreateTrialBase):\n    \"\"\"Internal command to create trial.\n\n    trial_number is populate within ZeusService.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    def to_orm(self) -&gt; TrialTable:\n        \"\"\"Create an ORM object from pydantic model.\n\n        Returns:\n            `TrialTable`: ORM object representing the trial.\n        \"\"\"\n        d = self.dict()\n        t = TrialTable()\n        for k, v in d.items():\n            setattr(t, k, v)\n        return t\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Create an ORM object from pydantic model.</p> <p>Returns:</p> Type Description <code>TrialTable</code> <p><code>TrialTable</code>: ORM object representing the trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>def to_orm(self) -&gt; TrialTable:\n    \"\"\"Create an ORM object from pydantic model.\n\n    Returns:\n        `TrialTable`: ORM object representing the trial.\n    \"\"\"\n    d = self.dict()\n    t = TrialTable()\n    for k, v in d.items():\n        setattr(t, k, v)\n    return t\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateExplorationTrial","title":"CreateExplorationTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a exploration.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateExplorationTrial(CreateTrialBase):\n    \"\"\"Create a exploration.\"\"\"\n\n    type: TrialType = Field(default=TrialType.Exploration, const=True)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateExplorationTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateMabTrial","title":"CreateMabTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a MAB trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateMabTrial(CreateTrialBase):\n    \"\"\"Create a MAB trial.\"\"\"\n\n    type: TrialType = Field(default=TrialType.MAB, const=True)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateMabTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateConcurrentTrial","title":"CreateConcurrentTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a exploration.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateConcurrentTrial(CreateTrialBase):\n    \"\"\"Create a exploration.\"\"\"\n\n    type: TrialType = Field(default=TrialType.Concurrent, const=True)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateConcurrentTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial","title":"UpdateTrial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Report the result of trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class UpdateTrial(BatchSizeBase):\n    \"\"\"Report the result of trial.\"\"\"\n\n    trial_number: int = Field(gt=0)\n    end_timestamp: datetime = Field(default_factory=datetime.now)\n    status: TrialStatus\n    time: Optional[float] = Field(default=None, ge=0)\n    energy: Optional[float] = Field(default=None, ge=0)\n    converged: Optional[bool] = None\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    @validator(\"status\")\n    def _check_status(cls, s: TrialStatus) -&gt; TrialStatus:\n        \"\"\"Check if status is equal to Dispatched.\"\"\"\n        if s != TrialStatus.Dispatched:\n            return s\n        else:\n            raise ValueError(\n                f\"{s} shouldn't be Dispatched since this is reporting the result.\"\n            )\n\n    @root_validator(skip_on_failure=True)\n    def _validate_sanity(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate result.\n\n        We are checking\n            - if status == Failed, time/energy/converged == None.\n                else, time/energy/converged != None.\n        \"\"\"\n        status: TrialStatus = values[\"status\"]\n\n        time: float | None = values[\"time\"]\n        energy: float | None = values[\"energy\"]\n        converged: bool | None = values[\"converged\"]\n\n        if status != TrialStatus.Failed and (\n            time is None or energy is None or converged is None\n        ):\n            raise ValueError(\n                f\"Result is incomplete: time({time}), energy({energy}), converged({converged})\"\n            )\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial._check_status","title":"_check_status","text":"<pre><code>_check_status(s)\n</code></pre> <p>Check if status is equal to Dispatched.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>@validator(\"status\")\ndef _check_status(cls, s: TrialStatus) -&gt; TrialStatus:\n    \"\"\"Check if status is equal to Dispatched.\"\"\"\n    if s != TrialStatus.Dispatched:\n        return s\n    else:\n        raise ValueError(\n            f\"{s} shouldn't be Dispatched since this is reporting the result.\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial._validate_sanity","title":"_validate_sanity","text":"<pre><code>_validate_sanity(values)\n</code></pre> <p>Validate result.</p> <p>We are checking     - if status == Failed, time/energy/converged == None.         else, time/energy/converged != None.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_sanity(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate result.\n\n    We are checking\n        - if status == Failed, time/energy/converged == None.\n            else, time/energy/converged != None.\n    \"\"\"\n    status: TrialStatus = values[\"status\"]\n\n    time: float | None = values[\"time\"]\n    energy: float | None = values[\"energy\"]\n    converged: bool | None = values[\"converged\"]\n\n    if status != TrialStatus.Failed and (\n        time is None or energy is None or converged is None\n    ):\n        raise ValueError(\n            f\"Result is incomplete: time({time}), energy({energy}), converged({converged})\"\n        )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/","title":"models","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models","title":"zeus.optimizer.batch_size.server.batch_size_state.models","text":"<p>Pydantic models for Batch size/Trials/GaussianTsArmState.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.BatchSizeBase","title":"BatchSizeBase","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for representing batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>batch_size</code> <code>int</code> <p>The size of the batch (greater than 0).</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class BatchSizeBase(BaseModel):\n    \"\"\"Base model for representing batch size.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        batch_size (int): The size of the batch (greater than 0).\n    \"\"\"\n\n    job_id: str\n    batch_size: int = Field(gt=0)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.BatchSizeBase.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial","title":"Trial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Pydantic model that represents Trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>batch_size</code> <code>int</code> <p>The size of the batch (greater than 0).</p> <code>trial_number</code> <code>int</code> <p>Number of trial.</p> <code>start_timestamp</code> <code>datetime</code> <p>Start time of trial.</p> <code>end_timestamp</code> <code>datetime</code> <p>End time of trial.</p> <code>type</code> <code>TrialType</code> <p>Type of this trial, which means in which stage this trial was executed.</p> <code>status</code> <code>TrialStatus</code> <p>Status of trial</p> <code>time</code> <code>Optional[float]</code> <p>Total time consumption of this trial.</p> <code>energy</code> <code>Optional[float]</code> <p>Total energy consumption of this trial.</p> <code>converged</code> <code>Optional[bool]</code> <p>Whether this trial is converged or not.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Trial(BatchSizeBase):\n    \"\"\"Pydantic model that represents Trial.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        batch_size (int): The size of the batch (greater than 0).\n        trial_number (int): Number of trial.\n        start_timestamp (datetime): Start time of trial.\n        end_timestamp (datetime): End time of trial.\n        type (TrialType): Type of this trial, which means in which stage this trial was executed.\n        status (TrialStatus): Status of trial\n        time (Optional[float]): Total time consumption of this trial.\n        energy (Optional[float]): Total energy consumption of this trial.\n        converged (Optional[bool]): Whether this trial is converged or not.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n    start_timestamp: datetime\n    end_timestamp: Optional[datetime] = Field(None)\n    type: TrialType\n    status: TrialStatus\n    time: Optional[float] = Field(None, ge=0)\n    energy: Optional[float] = Field(None, ge=0)\n    converged: Optional[bool] = None\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate trial.\n\n        We are checking\n            - start_timestamp &lt;= end_timestamp\n            - if status == dispatched | Failed, time/energy/converged = None\n                else time/energy/converged != None\n        \"\"\"\n        start_timestamp: datetime = values[\"start_timestamp\"]\n        end_timestamp: datetime | None = values[\"end_timestamp\"]\n        status: TrialStatus = values[\"status\"]\n        time: float | None = values[\"time\"]\n        energy: float | None = values[\"energy\"]\n        converged: bool | None = values[\"converged\"]\n\n        if end_timestamp is not None and start_timestamp &gt; end_timestamp:\n            raise ValueError(\n                f\"start is earlier than end: {start_timestamp} &gt; {end_timestamp}\"\n            )\n        if status in (TrialStatus.Dispatched, TrialStatus.Failed):\n            if not (time is None and energy is None and converged is None):\n                raise ValueError(\"Trial status and result is not matching.\")\n            if status == TrialStatus.Failed and end_timestamp is None:\n                raise ValueError(\"Trial ended but end_timestamp is None.\")\n        elif (\n            time is None or energy is None or converged is None or end_timestamp is None\n        ):\n            raise ValueError(\n                f\"Trial ended but the result is incomplete: time({time}), energy({energy}), converged({converged}), end_timestamp({end_timestamp})\"\n            )\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial._validate_mab","title":"_validate_mab","text":"<pre><code>_validate_mab(values)\n</code></pre> <p>Validate trial.</p> <p>We are checking     - start_timestamp &lt;= end_timestamp     - if status == dispatched | Failed, time/energy/converged = None         else time/energy/converged != None</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate trial.\n\n    We are checking\n        - start_timestamp &lt;= end_timestamp\n        - if status == dispatched | Failed, time/energy/converged = None\n            else time/energy/converged != None\n    \"\"\"\n    start_timestamp: datetime = values[\"start_timestamp\"]\n    end_timestamp: datetime | None = values[\"end_timestamp\"]\n    status: TrialStatus = values[\"status\"]\n    time: float | None = values[\"time\"]\n    energy: float | None = values[\"energy\"]\n    converged: bool | None = values[\"converged\"]\n\n    if end_timestamp is not None and start_timestamp &gt; end_timestamp:\n        raise ValueError(\n            f\"start is earlier than end: {start_timestamp} &gt; {end_timestamp}\"\n        )\n    if status in (TrialStatus.Dispatched, TrialStatus.Failed):\n        if not (time is None and energy is None and converged is None):\n            raise ValueError(\"Trial status and result is not matching.\")\n        if status == TrialStatus.Failed and end_timestamp is None:\n            raise ValueError(\"Trial ended but end_timestamp is None.\")\n    elif (\n        time is None or energy is None or converged is None or end_timestamp is None\n    ):\n        raise ValueError(\n            f\"Trial ended but the result is incomplete: time({time}), energy({energy}), converged({converged}), end_timestamp({end_timestamp})\"\n        )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState","title":"GaussianTsArmState","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model representing Gaussian Thompson Sampling arm state.</p> <p>Attributes:</p> Name Type Description <code>param_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>param_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>reward_precision</code> <code>float</code> <p>Precision (inverse variance) of the reward distribution.</p> <code>num_observations</code> <code>int</code> <p>How many observations we made.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class GaussianTsArmState(BatchSizeBase):\n    \"\"\"Model representing Gaussian Thompson Sampling arm state.\n\n    Attributes:\n        param_mean (float): Mean of the belief prior distribution.\n        param_precision (float): Precision of the belief prior distribution.\n        reward_precision (float): Precision (inverse variance) of the reward distribution.\n        num_observations (int): How many observations we made.\n    \"\"\"\n\n    param_mean: float\n    param_precision: float\n    reward_precision: float\n    num_observations: int = Field(ge=0)\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    def to_orm(self) -&gt; GaussianTsArmStateTable:\n        \"\"\"Convert pydantic model to ORM object.\n\n        Returns:\n            GaussianTsArmState: The ORM object of Gaussian Arm State.\n        \"\"\"\n        d = self.dict()\n        g = GaussianTsArmStateTable()\n        for k, v in d.items():\n            setattr(g, k, v)\n        return g\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Convert pydantic model to ORM object.</p> <p>Returns:</p> Name Type Description <code>GaussianTsArmState</code> <code>GaussianTsArmStateTable</code> <p>The ORM object of Gaussian Arm State.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>def to_orm(self) -&gt; GaussianTsArmStateTable:\n    \"\"\"Convert pydantic model to ORM object.\n\n    Returns:\n        GaussianTsArmState: The ORM object of Gaussian Arm State.\n    \"\"\"\n    d = self.dict()\n    g = GaussianTsArmStateTable()\n    for k, v in d.items():\n        setattr(g, k, v)\n    return g\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult","title":"TrialResult","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model for reading the result of the trial.</p> <p>Refer to <code>Trial</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class TrialResult(BatchSizeBase):\n    \"\"\"Model for reading the result of the trial.\n\n    Refer to [`Trial`][zeus.optimizer.batch_size.server.batch_size_state.models.Trial] for attributes.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n    status: TrialStatus\n    time: float = Field(ge=0)\n    energy: float = Field(ge=0)\n    converged: bool\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    @validator(\"status\")\n    def _check_state(cls, s: TrialStatus) -&gt; TrialStatus:\n        \"\"\"Check if status is equal to succeeded.\"\"\"\n        if s == TrialStatus.Succeeded:\n            return s\n        else:\n            raise ValueError(f\"{s} should be succeeded to have a valid result.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult._check_state","title":"_check_state","text":"<pre><code>_check_state(s)\n</code></pre> <p>Check if status is equal to succeeded.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@validator(\"status\")\ndef _check_state(cls, s: TrialStatus) -&gt; TrialStatus:\n    \"\"\"Check if status is equal to succeeded.\"\"\"\n    if s == TrialStatus.Succeeded:\n        return s\n    else:\n        raise ValueError(f\"{s} should be succeeded to have a valid result.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResultsPerBs","title":"TrialResultsPerBs","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model representing all succeeded results of trial for a given batch size.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[TrialResult]</code> <p>List of TrialResult per batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class TrialResultsPerBs(BatchSizeBase):\n    \"\"\"Model representing all succeeded results of trial for a given batch size.\n\n    Attributes:\n        results (list[TrialResult]): List of TrialResult per batch size.\n    \"\"\"\n\n    results: list[TrialResult]\n\n    @root_validator(skip_on_failure=True)\n    def _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate if job_id and bs are consistent across all items in results.\"\"\"\n        bs: int = values[\"batch_size\"]\n        job_id: str = values[\"job_id\"]\n        ms: list[TrialResult] = values[\"results\"]\n        ms.sort(key=lambda x: x.trial_number, reverse=True)\n\n        for m in ms:\n            if job_id != m.job_id:\n                raise ValueError(\n                    f\"job_id doesn't correspond with results: {job_id} != {m.job_id}\"\n                )\n            if bs != m.batch_size:\n                raise ValueError(\n                    f\"Batch size doesn't correspond with results: {bs} != {m.batch_size}\"\n                )\n            if m.status != TrialStatus.Succeeded:\n                raise ValueError(\n                    f\"This list should only contain succeeded trials. Encounted trial({m.trial_number}) of status = {m.status}\"\n                )\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResultsPerBs._check_explorations","title":"_check_explorations","text":"<pre><code>_check_explorations(values)\n</code></pre> <p>Validate if job_id and bs are consistent across all items in results.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate if job_id and bs are consistent across all items in results.\"\"\"\n    bs: int = values[\"batch_size\"]\n    job_id: str = values[\"job_id\"]\n    ms: list[TrialResult] = values[\"results\"]\n    ms.sort(key=lambda x: x.trial_number, reverse=True)\n\n    for m in ms:\n        if job_id != m.job_id:\n            raise ValueError(\n                f\"job_id doesn't correspond with results: {job_id} != {m.job_id}\"\n            )\n        if bs != m.batch_size:\n            raise ValueError(\n                f\"Batch size doesn't correspond with results: {bs} != {m.batch_size}\"\n            )\n        if m.status != TrialStatus.Succeeded:\n            raise ValueError(\n                f\"This list should only contain succeeded trials. Encounted trial({m.trial_number}) of status = {m.status}\"\n            )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob","title":"ExplorationsPerJob","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model representing all succeeded explorations we have done for a job. Immutable after it's created.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>explorations_per_bs</code> <code>dict[int, list[Trial]]</code> <p>Dictionary of \"succeeded\" explorations per batch size in trial_number ascending order.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class ExplorationsPerJob(BaseModel):\n    \"\"\"Model representing all succeeded explorations we have done for a job. Immutable after it's created.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        explorations_per_bs (dict[int, list[Trial]]): Dictionary of \"succeeded\" explorations per batch size in trial_number ascending order.\n    \"\"\"\n\n    job_id: str\n    explorations_per_bs: dict[int, list[Trial]]  # BS -&gt; Trials with exploration type\n\n    class Config:  # type: ignore\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.\"\"\"\n        job_id: str = values[\"job_id\"]\n        exps_per_bs: dict[int, list[Trial]] = values[\"explorations_per_bs\"]\n\n        for bs, exps in exps_per_bs.items():\n            # Sort ascending just in case. Sql will return asc order anyways.\n            exps.sort(key=lambda x: x.trial_number)\n            for exp in exps:\n                if job_id != exp.job_id:\n                    raise ValueError(\n                        f\"job_id doesn't correspond with explorations: {job_id} != {exp.job_id}\"\n                    )\n                if bs != exp.batch_size:\n                    raise ValueError(\n                        f\"Batch size doesn't correspond with explorations: {bs} != {exp.batch_size}\"\n                    )\n                if exp.type != TrialType.Exploration:\n                    raise ValueError(\"Trial type is not equal to Exploration.\")\n                if exp.status == TrialStatus.Failed:\n                    raise ValueError(\"Should not include failed trial.\")\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob._check_explorations","title":"_check_explorations","text":"<pre><code>_check_explorations(values)\n</code></pre> <p>Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.\"\"\"\n    job_id: str = values[\"job_id\"]\n    exps_per_bs: dict[int, list[Trial]] = values[\"explorations_per_bs\"]\n\n    for bs, exps in exps_per_bs.items():\n        # Sort ascending just in case. Sql will return asc order anyways.\n        exps.sort(key=lambda x: x.trial_number)\n        for exp in exps:\n            if job_id != exp.job_id:\n                raise ValueError(\n                    f\"job_id doesn't correspond with explorations: {job_id} != {exp.job_id}\"\n                )\n            if bs != exp.batch_size:\n                raise ValueError(\n                    f\"Batch size doesn't correspond with explorations: {bs} != {exp.batch_size}\"\n                )\n            if exp.type != TrialType.Exploration:\n                raise ValueError(\"Trial type is not equal to Exploration.\")\n            if exp.status == TrialStatus.Failed:\n                raise ValueError(\"Should not include failed trial.\")\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository","title":"zeus.optimizer.batch_size.server.batch_size_state.repository","text":"<p>Repository for batch size states(Trial, Gaussian Ts arm state).</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository","title":"BatchSizeStateRepository","text":"<p>               Bases: <code>DatabaseRepository</code></p> <p>Repository for handling batch size related operations.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>class BatchSizeStateRepository(DatabaseRepository):\n    \"\"\"Repository for handling batch size related operations.\"\"\"\n\n    def __init__(self, session: AsyncSession):\n        \"\"\"Set db session and intialize fetched trial. We are only updating one trial per session.\"\"\"\n        super().__init__(session)\n        self.fetched_trial: TrialTable | None = None\n        self.fetched_arm: GaussianTsArmStateTable | None = None\n\n    async def get_next_trial_number(self, job_id: str) -&gt; int:\n        \"\"\"Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.\"\"\"\n        stmt = select(func.max(TrialTable.trial_number)).where(\n            and_(\n                TrialTable.job_id == job_id,\n            )\n        )\n        res = await self.session.scalar(stmt)\n        if res is None:\n            return 1\n        return res + 1\n\n    async def get_trial_results_of_bs(\n        self, batch_size: BatchSizeBase, window_size: int\n    ) -&gt; TrialResultsPerBs:\n        \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n        From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.\n\n        Args:\n            batch_size (BatchSizeBase): The batch size object.\n            window_size (int): The size of the measurement window.\n\n        Returns:\n            TrialResultsPerBs: trial results for the given batch size.\n        \"\"\"\n        stmt = (\n            select(TrialTable)\n            .where(\n                and_(\n                    TrialTable.job_id == batch_size.job_id,\n                    TrialTable.batch_size == batch_size.batch_size,\n                    TrialTable.status == TrialStatus.Succeeded,\n                )\n            )\n            .order_by(TrialTable.trial_number.desc())\n        )\n        if window_size &gt; 0:\n            stmt = stmt.limit(window_size)\n\n        res = (await self.session.scalars(stmt)).all()\n        return TrialResultsPerBs(\n            job_id=batch_size.job_id,\n            batch_size=batch_size.batch_size,\n            results=[TrialResult.from_orm(t) for t in res],\n        )\n\n    async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n        \"\"\"Retrieve Gaussian Thompson Sampling arms for a given job.\n\n        Args:\n            job_id (str): The ID of the job.\n\n        Returns:\n            List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        stmt = select(GaussianTsArmStateTable).where(\n            GaussianTsArmStateTable.job_id == job_id\n        )\n        res = (await self.session.scalars(stmt)).all()\n        return [GaussianTsArmState.from_orm(arm) for arm in res]\n\n    async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n        \"\"\"Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.\n\n        Args:\n            bs (BatchSizeBase): The batch size object.\n\n        Returns:\n            Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        stmt = select(GaussianTsArmStateTable).where(\n            and_(\n                GaussianTsArmStateTable.job_id == bs.job_id,\n                GaussianTsArmStateTable.batch_size == bs.batch_size,\n            )\n        )\n        arm = await self.session.scalar(stmt)\n        if arm is None:\n            return None\n        self.fetched_arm = arm\n        return GaussianTsArmState.from_orm(arm)\n\n    async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Get a corresponding trial.\n\n        Args:\n            trial: job_id, batch_size, trial_number.\n\n        Returns:\n            Found Trial. If none found, return None.\n        \"\"\"\n        stmt = select(TrialTable).where(\n            TrialTable.job_id == trial.job_id,\n            TrialTable.batch_size == trial.batch_size,\n            TrialTable.trial_number == trial.trial_number,\n        )\n        fetched_trial = await self.session.scalar(stmt)\n\n        if fetched_trial is None:\n            logger.info(\"get_trial: NoResultFound\")\n            return None\n\n        self.fetched_trial = fetched_trial\n        return Trial.from_orm(fetched_trial)\n\n    def get_trial_from_session(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Fetch a trial from the session.\"\"\"\n        if (\n            self.fetched_trial is None\n            or self.fetched_trial.job_id != trial.job_id\n            or self.fetched_trial.batch_size != trial.batch_size\n            or self.fetched_trial.trial_number != trial.trial_number\n        ):\n            return None\n        return Trial.from_orm(self.fetched_trial)\n\n    def create_trial(self, trial: CreateTrial) -&gt; None:\n        \"\"\"Create a trial in db.\n\n        Refer to `CreateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.\n\n        Args:\n            trial (CreateTrial): The trial to add.\n        \"\"\"\n        self.session.add(trial.to_orm())\n\n    def updated_current_trial(self, updated_trial: UpdateTrial) -&gt; None:\n        \"\"\"Update trial in the database (report the result of trial).\n\n        Args:\n            updated_trial (UpdateTrial): The updated trial. Refer to `UpdateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.\n        \"\"\"\n        if self.fetched_trial is None:\n            raise ZeusBSOValueError(\"No trial is fetched.\")\n\n        if (\n            self.fetched_trial.job_id != updated_trial.job_id\n            or self.fetched_trial.batch_size != updated_trial.batch_size\n            or self.fetched_trial.trial_number != updated_trial.trial_number\n        ):\n            raise ZeusBSOValueError(\"Trying to update invalid trial.\")\n\n        self.fetched_trial.end_timestamp = updated_trial.end_timestamp\n        self.fetched_trial.status = updated_trial.status\n        self.fetched_trial.time = updated_trial.time\n        self.fetched_trial.energy = updated_trial.energy\n        self.fetched_trial.converged = updated_trial.converged\n\n    def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n        \"\"\"Create Gaussian Thompson Sampling arms in the database.\n\n        Args:\n            new_arms (List[GaussianTsArmStateModel]): List of new arms to create.\n                Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        self.session.add_all([arm.to_orm() for arm in new_arms])\n\n    def update_arm_state(self, updated_mab_state: GaussianTsArmState) -&gt; None:\n        \"\"\"Update Gaussian Thompson Sampling arm state in db.\n\n        Args:\n            updated_mab_state (GaussianTsArmStateModel): The updated arm state.\n                Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        if self.fetched_arm is None:\n            raise ZeusBSOValueError(\"No arm is fetched.\")\n\n        if (\n            self.fetched_arm.job_id != updated_mab_state.job_id\n            or self.fetched_arm.batch_size != updated_mab_state.batch_size\n        ):\n            raise ZeusBSOValueError(\n                \"Fetch arm does not correspond with the arm trying to update.\"\n            )\n\n        self.fetched_arm.param_mean = updated_mab_state.param_mean\n        self.fetched_arm.param_precision = updated_mab_state.param_precision\n        self.fetched_arm.reward_precision = updated_mab_state.reward_precision\n        self.fetched_arm.num_observations = updated_mab_state.num_observations\n\n    async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n        \"\"\"Retrieve succeeded or ongoing explorations for a given job.\n\n        Args:\n            job_id: ID of the job\n\n        Returns:\n            ExplorationsPerJob: Explorations for the given batch size.\n            Refer to `ExplorationsPerJob`[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.\n        \"\"\"\n        stmt = (\n            select(TrialTable)\n            .where(\n                and_(\n                    TrialTable.job_id == job_id,\n                    TrialTable.type == TrialType.Exploration,\n                    TrialTable.status != TrialStatus.Failed,\n                )\n            )\n            .order_by(TrialTable.trial_number.asc())\n        )\n\n        explorations = (await self.session.scalars(stmt)).all()\n        exps_per_bs: defaultdict[int, list[Trial]] = defaultdict(list)\n        for exp in explorations:\n            exps_per_bs[exp.batch_size].append(Trial.from_orm(exp))\n\n        return ExplorationsPerJob(job_id=job_id, explorations_per_bs=exps_per_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def __init__(self, session: AsyncSession):\n    \"\"\"Set db session and intialize fetched trial. We are only updating one trial per session.\"\"\"\n    super().__init__(session)\n    self.fetched_trial: TrialTable | None = None\n    self.fetched_arm: GaussianTsArmStateTable | None = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_next_trial_number","title":"get_next_trial_number  <code>async</code>","text":"<pre><code>get_next_trial_number(job_id)\n</code></pre> <p>Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_next_trial_number(self, job_id: str) -&gt; int:\n    \"\"\"Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.\"\"\"\n    stmt = select(func.max(TrialTable.trial_number)).where(\n        and_(\n            TrialTable.job_id == job_id,\n        )\n    )\n    res = await self.session.scalar(stmt)\n    if res is None:\n        return 1\n    return res + 1\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial_results_of_bs","title":"get_trial_results_of_bs  <code>async</code>","text":"<pre><code>get_trial_results_of_bs(batch_size, window_size)\n</code></pre> <p>Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.</p> <p>From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>BatchSizeBase</code> <p>The batch size object.</p> required <code>window_size</code> <code>int</code> <p>The size of the measurement window.</p> required <p>Returns:</p> Name Type Description <code>TrialResultsPerBs</code> <code>TrialResultsPerBs</code> <p>trial results for the given batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_trial_results_of_bs(\n    self, batch_size: BatchSizeBase, window_size: int\n) -&gt; TrialResultsPerBs:\n    \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n    From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.\n\n    Args:\n        batch_size (BatchSizeBase): The batch size object.\n        window_size (int): The size of the measurement window.\n\n    Returns:\n        TrialResultsPerBs: trial results for the given batch size.\n    \"\"\"\n    stmt = (\n        select(TrialTable)\n        .where(\n            and_(\n                TrialTable.job_id == batch_size.job_id,\n                TrialTable.batch_size == batch_size.batch_size,\n                TrialTable.status == TrialStatus.Succeeded,\n            )\n        )\n        .order_by(TrialTable.trial_number.desc())\n    )\n    if window_size &gt; 0:\n        stmt = stmt.limit(window_size)\n\n    res = (await self.session.scalars(stmt)).all()\n    return TrialResultsPerBs(\n        job_id=batch_size.job_id,\n        batch_size=batch_size.batch_size,\n        results=[TrialResult.from_orm(t) for t in res],\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_arms","title":"get_arms  <code>async</code>","text":"<pre><code>get_arms(job_id)\n</code></pre> <p>Retrieve Gaussian Thompson Sampling arms for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The ID of the job.</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).</p> <code>list[GaussianTsArmState]</code> <p>Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n    \"\"\"Retrieve Gaussian Thompson Sampling arms for a given job.\n\n    Args:\n        job_id (str): The ID of the job.\n\n    Returns:\n        List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).\n        Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    stmt = select(GaussianTsArmStateTable).where(\n        GaussianTsArmStateTable.job_id == job_id\n    )\n    res = (await self.session.scalars(stmt)).all()\n    return [GaussianTsArmState.from_orm(arm) for arm in res]\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_arm","title":"get_arm  <code>async</code>","text":"<pre><code>get_arm(bs)\n</code></pre> <p>Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>The batch size object.</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState | None</code> <p>Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.</p> <code>GaussianTsArmState | None</code> <p>Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n    \"\"\"Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.\n\n    Args:\n        bs (BatchSizeBase): The batch size object.\n\n    Returns:\n        Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.\n        Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    stmt = select(GaussianTsArmStateTable).where(\n        and_(\n            GaussianTsArmStateTable.job_id == bs.job_id,\n            GaussianTsArmStateTable.batch_size == bs.batch_size,\n        )\n    )\n    arm = await self.session.scalar(stmt)\n    if arm is None:\n        return None\n    self.fetched_arm = arm\n    return GaussianTsArmState.from_orm(arm)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial","title":"get_trial  <code>async</code>","text":"<pre><code>get_trial(trial)\n</code></pre> <p>Get a corresponding trial.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>ReadTrial</code> <p>job_id, batch_size, trial_number.</p> required <p>Returns:</p> Type Description <code>Trial | None</code> <p>Found Trial. If none found, return None.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Get a corresponding trial.\n\n    Args:\n        trial: job_id, batch_size, trial_number.\n\n    Returns:\n        Found Trial. If none found, return None.\n    \"\"\"\n    stmt = select(TrialTable).where(\n        TrialTable.job_id == trial.job_id,\n        TrialTable.batch_size == trial.batch_size,\n        TrialTable.trial_number == trial.trial_number,\n    )\n    fetched_trial = await self.session.scalar(stmt)\n\n    if fetched_trial is None:\n        logger.info(\"get_trial: NoResultFound\")\n        return None\n\n    self.fetched_trial = fetched_trial\n    return Trial.from_orm(fetched_trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial_from_session","title":"get_trial_from_session","text":"<pre><code>get_trial_from_session(trial)\n</code></pre> <p>Fetch a trial from the session.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def get_trial_from_session(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Fetch a trial from the session.\"\"\"\n    if (\n        self.fetched_trial is None\n        or self.fetched_trial.job_id != trial.job_id\n        or self.fetched_trial.batch_size != trial.batch_size\n        or self.fetched_trial.trial_number != trial.trial_number\n    ):\n        return None\n    return Trial.from_orm(self.fetched_trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.create_trial","title":"create_trial","text":"<pre><code>create_trial(trial)\n</code></pre> <p>Create a trial in db.</p> <p>Refer to <code>CreateTrial</code>[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>CreateTrial</code> <p>The trial to add.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def create_trial(self, trial: CreateTrial) -&gt; None:\n    \"\"\"Create a trial in db.\n\n    Refer to `CreateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.\n\n    Args:\n        trial (CreateTrial): The trial to add.\n    \"\"\"\n    self.session.add(trial.to_orm())\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.updated_current_trial","title":"updated_current_trial","text":"<pre><code>updated_current_trial(updated_trial)\n</code></pre> <p>Update trial in the database (report the result of trial).</p> <p>Parameters:</p> Name Type Description Default <code>updated_trial</code> <code>UpdateTrial</code> <p>The updated trial. Refer to <code>UpdateTrial</code>[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def updated_current_trial(self, updated_trial: UpdateTrial) -&gt; None:\n    \"\"\"Update trial in the database (report the result of trial).\n\n    Args:\n        updated_trial (UpdateTrial): The updated trial. Refer to `UpdateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.\n    \"\"\"\n    if self.fetched_trial is None:\n        raise ZeusBSOValueError(\"No trial is fetched.\")\n\n    if (\n        self.fetched_trial.job_id != updated_trial.job_id\n        or self.fetched_trial.batch_size != updated_trial.batch_size\n        or self.fetched_trial.trial_number != updated_trial.trial_number\n    ):\n        raise ZeusBSOValueError(\"Trying to update invalid trial.\")\n\n    self.fetched_trial.end_timestamp = updated_trial.end_timestamp\n    self.fetched_trial.status = updated_trial.status\n    self.fetched_trial.time = updated_trial.time\n    self.fetched_trial.energy = updated_trial.energy\n    self.fetched_trial.converged = updated_trial.converged\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.create_arms","title":"create_arms","text":"<pre><code>create_arms(new_arms)\n</code></pre> <p>Create Gaussian Thompson Sampling arms in the database.</p> <p>Parameters:</p> Name Type Description Default <code>new_arms</code> <code>List[GaussianTsArmStateModel]</code> <p>List of new arms to create. Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n    \"\"\"Create Gaussian Thompson Sampling arms in the database.\n\n    Args:\n        new_arms (List[GaussianTsArmStateModel]): List of new arms to create.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    self.session.add_all([arm.to_orm() for arm in new_arms])\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.update_arm_state","title":"update_arm_state","text":"<pre><code>update_arm_state(updated_mab_state)\n</code></pre> <p>Update Gaussian Thompson Sampling arm state in db.</p> <p>Parameters:</p> Name Type Description Default <code>updated_mab_state</code> <code>GaussianTsArmStateModel</code> <p>The updated arm state. Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def update_arm_state(self, updated_mab_state: GaussianTsArmState) -&gt; None:\n    \"\"\"Update Gaussian Thompson Sampling arm state in db.\n\n    Args:\n        updated_mab_state (GaussianTsArmStateModel): The updated arm state.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    if self.fetched_arm is None:\n        raise ZeusBSOValueError(\"No arm is fetched.\")\n\n    if (\n        self.fetched_arm.job_id != updated_mab_state.job_id\n        or self.fetched_arm.batch_size != updated_mab_state.batch_size\n    ):\n        raise ZeusBSOValueError(\n            \"Fetch arm does not correspond with the arm trying to update.\"\n        )\n\n    self.fetched_arm.param_mean = updated_mab_state.param_mean\n    self.fetched_arm.param_precision = updated_mab_state.param_precision\n    self.fetched_arm.reward_precision = updated_mab_state.reward_precision\n    self.fetched_arm.num_observations = updated_mab_state.num_observations\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_explorations_of_job","title":"get_explorations_of_job  <code>async</code>","text":"<pre><code>get_explorations_of_job(job_id)\n</code></pre> <p>Retrieve succeeded or ongoing explorations for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the job</p> required <p>Returns:</p> Name Type Description <code>ExplorationsPerJob</code> <code>ExplorationsPerJob</code> <p>Explorations for the given batch size.</p> <code>ExplorationsPerJob</code> <p>Refer to <code>ExplorationsPerJob</code>[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n    \"\"\"Retrieve succeeded or ongoing explorations for a given job.\n\n    Args:\n        job_id: ID of the job\n\n    Returns:\n        ExplorationsPerJob: Explorations for the given batch size.\n        Refer to `ExplorationsPerJob`[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.\n    \"\"\"\n    stmt = (\n        select(TrialTable)\n        .where(\n            and_(\n                TrialTable.job_id == job_id,\n                TrialTable.type == TrialType.Exploration,\n                TrialTable.status != TrialStatus.Failed,\n            )\n        )\n        .order_by(TrialTable.trial_number.asc())\n    )\n\n    explorations = (await self.session.scalars(stmt)).all()\n    exps_per_bs: defaultdict[int, list[Trial]] = defaultdict(list)\n    for exp in explorations:\n        exps_per_bs[exp.batch_size].append(Trial.from_orm(exp))\n\n    return ExplorationsPerJob(job_id=job_id, explorations_per_bs=exps_per_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/","title":"database","text":""},{"location":"reference/optimizer/batch_size/server/database/#zeus.optimizer.batch_size.server.database","title":"zeus.optimizer.batch_size.server.database","text":"<p>Manage database connection and define schema.</p>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/","title":"db_connection","text":""},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection","title":"zeus.optimizer.batch_size.server.database.db_connection","text":"<p>Managing database connection.</p> <p>Heavily inspired by https://praciano.com.br/fastapi-and-async-sqlalchemy-20-with-pytest-done-right.html and https://medium.com/@tclaitken/setting-up-a-fastapi-app-with-async-sqlalchemy-2-0-pydantic-v2-e6c540be4308</p>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager","title":"DatabaseSessionManager","text":"<p>Session manager class.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>class DatabaseSessionManager:\n    \"\"\"Session manager class.\"\"\"\n\n    def __init__(self, host: str, engine_kwargs: dict[str, Any] | None = None):\n        \"\"\"Create async engine and session maker.\"\"\"\n        if engine_kwargs is None:\n            engine_kwargs = {}\n        self._engine = create_async_engine(host, **engine_kwargs)\n        self._sessionmaker = async_sessionmaker(autocommit=False, bind=self._engine)\n\n    async def close(self):\n        \"\"\"Close connection.\"\"\"\n        if self._engine is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n        await self._engine.dispose()\n\n        self._engine = None\n        self._sessionmaker = None\n\n    @contextlib.asynccontextmanager\n    async def connect(self) -&gt; AsyncIterator[AsyncConnection]:\n        \"\"\"Connect to db.\"\"\"\n        if self._engine is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        async with self._engine.begin() as connection:\n            try:\n                yield connection\n            except Exception:\n                await connection.rollback()\n                raise\n\n    @contextlib.asynccontextmanager\n    async def session(self) -&gt; AsyncIterator[AsyncSession]:\n        \"\"\"Get session from session maker.\"\"\"\n        if self._sessionmaker is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        session = self._sessionmaker()\n        try:\n            yield session\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.__init__","title":"__init__","text":"<pre><code>__init__(host, engine_kwargs=None)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>def __init__(self, host: str, engine_kwargs: dict[str, Any] | None = None):\n    \"\"\"Create async engine and session maker.\"\"\"\n    if engine_kwargs is None:\n        engine_kwargs = {}\n    self._engine = create_async_engine(host, **engine_kwargs)\n    self._sessionmaker = async_sessionmaker(autocommit=False, bind=self._engine)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close connection.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>async def close(self):\n    \"\"\"Close connection.\"\"\"\n    if self._engine is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n    await self._engine.dispose()\n\n    self._engine = None\n    self._sessionmaker = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to db.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>@contextlib.asynccontextmanager\nasync def connect(self) -&gt; AsyncIterator[AsyncConnection]:\n    \"\"\"Connect to db.\"\"\"\n    if self._engine is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    async with self._engine.begin() as connection:\n        try:\n            yield connection\n        except Exception:\n            await connection.rollback()\n            raise\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.session","title":"session  <code>async</code>","text":"<pre><code>session()\n</code></pre> <p>Get session from session maker.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>@contextlib.asynccontextmanager\nasync def session(self) -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Get session from session maker.\"\"\"\n    if self._sessionmaker is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    session = self._sessionmaker()\n    try:\n        yield session\n    except Exception:\n        await session.rollback()\n        raise\n    finally:\n        await session.close()\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.get_db_session","title":"get_db_session  <code>async</code>","text":"<pre><code>get_db_session()\n</code></pre> <p>Get db session from session manager. Used with fastapi dependency injection.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>async def get_db_session() -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Get db session from session manager. Used with fastapi dependency injection.\"\"\"\n    async with sessionmanager.session() as session:\n        yield session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository","title":"zeus.optimizer.batch_size.server.database.repository","text":"<p>Database repository (directly interacting with db) base class.</p>"},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository.DatabaseRepository","title":"DatabaseRepository","text":"<p>Base class for all repositories.</p> Source code in <code>zeus/optimizer/batch_size/server/database/repository.py</code> <pre><code>class DatabaseRepository:\n    \"\"\"Base class for all repositories.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initizalize session.\"\"\"\n        self.session = session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository.DatabaseRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/database/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initizalize session.\"\"\"\n    self.session = session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/","title":"schema","text":""},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema","title":"zeus.optimizer.batch_size.server.database.schema","text":"<p>Database schema.</p>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.Base","title":"Base","text":"<p>               Bases: <code>DeclarativeBase</code></p> <p>Base class for schemas.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class Base(DeclarativeBase):\n    \"\"\"Base class for schemas.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.JobTable","title":"JobTable","text":"<p>               Bases: <code>Base</code></p> <p>Job table schema.</p> <p>Refer to <code>JobState</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class JobTable(Base):\n    \"\"\"Job table schema.\n\n    Refer to [`JobState`][zeus.optimizer.batch_size.server.job.models.JobState] for attributes.\n    \"\"\"\n\n    __tablename__ = \"Job\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(400), primary_key=True)\n    job_id_prefix: Mapped[str] = mapped_column(VARCHAR(300), nullable=False)\n    default_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n    higher_is_better_metric: Mapped[bool] = mapped_column(Boolean, default=True)\n    eta_knob: Mapped[float] = mapped_column(Float, default=0.5)\n    beta_knob: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    target_metric: Mapped[float] = mapped_column(Float, default=0.5)\n    max_epochs: Mapped[int] = mapped_column(Integer, default=100)\n    num_pruning_rounds: Mapped[int] = mapped_column(Integer, default=2)\n    window_size: Mapped[int] = mapped_column(Integer, default=10)\n\n    max_power: Mapped[float] = mapped_column(Float, nullable=False)\n    number_of_gpus: Mapped[int] = mapped_column(Integer, nullable=False)\n    gpu_model: Mapped[str] = mapped_column(VARCHAR(length=30), nullable=False)\n\n    mab_prior_mean: Mapped[float] = mapped_column(Float, default=0.0)\n    mab_prior_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    mab_num_explorations: Mapped[int] = mapped_column(Integer, default=2)\n    mab_seed: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)\n\n    mab_random_generator_state: Mapped[Optional[str]] = mapped_column(\n        VARCHAR(length=10000), nullable=True\n    )\n    exp_default_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n\n    stage: Mapped[Stage] = mapped_column(Enum(Stage), default=Stage.Pruning)\n    min_cost: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    min_cost_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n\n    batch_sizes: Mapped[list[\"BatchSizeTable\"]] = relationship(\n        order_by=\"BatchSizeTable.batch_size.asc()\",\n        back_populates=\"job\",\n        # always fetch batch size(int) whenever we fetch the job.\n        # https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html#relationship-loading-techniques\n        lazy=\"joined\",\n        # Delete all children if the job gets deleted.\n        # https://docs.sqlalchemy.org/en/20/orm/cascades.html\n        cascade=\"all, delete-orphan\",\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.BatchSizeTable","title":"BatchSizeTable","text":"<p>               Bases: <code>Base</code></p> <p>Batch size states table schema. Represents one batch size of a job.</p> <p>(job_id, batch_size) as a pk, and have three states(exploration, measurement, GaussianTs arm state) as fk. For explorations and measurements, one-to-many relationship. For arm_state, one-to-(zero or one) relationship.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class BatchSizeTable(Base):\n    \"\"\"Batch size states table schema. Represents one batch size of a job.\n\n    (job_id, batch_size) as a pk, and have three states(exploration, measurement, GaussianTs arm state) as fk.\n    For explorations and measurements, one-to-many relationship. For arm_state, one-to-(zero or one) relationship.\n    \"\"\"\n\n    __tablename__ = \"BatchSize\"\n\n    job_id: Mapped[str] = mapped_column(\n        ForeignKey(\n            \"Job.job_id\",\n            ondelete=\"CASCADE\",\n        ),\n        primary_key=True,\n    )\n    batch_size: Mapped[int] = mapped_column(Integer, primary_key=True)\n\n    trials: Mapped[list[\"TrialTable\"]] = relationship(\n        back_populates=\"batch_size_state\", cascade=\"all, delete-orphan\"\n    )\n\n    arm_state: Mapped[Optional[\"GaussianTsArmStateTable\"]] = relationship(\n        back_populates=\"batch_size_state\",  # populates GaussianTsArmState-&gt;BatchSize\n        # https://stackoverflow.com/questions/39869793/when-do-i-need-to-use-sqlalchemy-back-populates\n        cascade=\"all, delete-orphan\",\n    )\n\n    job: Mapped[\"JobTable\"] = relationship(back_populates=\"batch_sizes\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.GaussianTsArmStateTable","title":"GaussianTsArmStateTable","text":"<p>               Bases: <code>Base</code></p> <p>Gaussian arm state schema. Represents a gaussian thompson arm states of a batch size.</p> <p>Refer to <code>GaussianTsArmState</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class GaussianTsArmStateTable(Base):\n    \"\"\"Gaussian arm state schema. Represents a gaussian thompson arm states of a batch size.\n\n    Refer to [`GaussianTsArmState`][zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState] for attributes.\n    \"\"\"\n\n    __tablename__ = \"GaussianTsArmState\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(300), primary_key=True)\n    batch_size: Mapped[int] = mapped_column(Integer, primary_key=True)  # arm\n\n    param_mean: Mapped[float] = mapped_column(Float, default=0.0)\n    param_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    reward_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    num_observations: Mapped[int] = mapped_column(Integer, default=0)\n\n    batch_size_state: Mapped[\"BatchSizeTable\"] = relationship(\n        back_populates=\"arm_state\"\n    )\n\n    __table_args__ = (\n        ForeignKeyConstraint(\n            [job_id, batch_size],\n            [BatchSizeTable.job_id, BatchSizeTable.batch_size],\n            ondelete=\"CASCADE\",\n        ),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialType","title":"TrialType","text":"<p>               Bases: <code>Enum</code></p> <p>Type of trial.</p> <p>Exploration is a trial done during Pruning stage. Concurrent is a trial done as a concurrent job submission. MAB is a trial done during the MAB stage.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialType(enum.Enum):\n    \"\"\"Type of trial.\n\n    Exploration is a trial done during Pruning stage.\n    Concurrent is a trial done as a concurrent job submission.\n    MAB is a trial done during the MAB stage.\n    \"\"\"\n\n    Exploration = \"Exploration\"\n    Concurrent = \"Concurrent\"\n    MAB = \"MAB\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialStatus","title":"TrialStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Status of trial.</p> <p>Dispatched means this trial is issued. Succeded means trial ended without error. Failed means trial ended with error.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialStatus(enum.Enum):\n    \"\"\"Status of trial.\n\n    Dispatched means this trial is issued.\n    Succeded means trial ended without error.\n    Failed means trial ended with error.\n    \"\"\"\n\n    Dispatched = \"Dispatched\"\n    Succeeded = \"Succeeded\"\n    Failed = \"Failed\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialTable","title":"TrialTable","text":"<p>               Bases: <code>Base</code></p> <p>Represents each trial of training.</p> <p>Refer to <code>Trial</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialTable(Base):\n    \"\"\"Represents each trial of training.\n\n    Refer to [`Trial`][zeus.optimizer.batch_size.server.batch_size_state.models.Trial] for attributes.\n    \"\"\"\n\n    __tablename__ = \"Trial\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(300), primary_key=True, nullable=False)\n    batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n    trial_number: Mapped[int] = mapped_column(Integer, primary_key=True, nullable=False)\n    start_timestamp: Mapped[datetime] = mapped_column(DateTime, nullable=False)\n    type: Mapped[TrialType] = mapped_column(Enum(TrialType), nullable=False)\n    status: Mapped[TrialStatus] = mapped_column(Enum(TrialStatus), nullable=False)\n\n    end_timestamp: Mapped[Optional[datetime]] = mapped_column(DateTime, nullable=True)\n    time: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    energy: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    converged: Mapped[Optional[bool]] = mapped_column(Boolean, nullable=True)\n\n    batch_size_state: Mapped[\"BatchSizeTable\"] = relationship(back_populates=\"trials\")\n\n    __table_args__ = (\n        ForeignKeyConstraint(\n            [job_id, batch_size],\n            [BatchSizeTable.job_id, BatchSizeTable.batch_size],\n            ondelete=\"CASCADE\",\n        ),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/","title":"job","text":""},{"location":"reference/optimizer/batch_size/server/job/#zeus.optimizer.batch_size.server.job","title":"zeus.optimizer.batch_size.server.job","text":"<p>Models, commands, and repository for job states.</p>"},{"location":"reference/optimizer/batch_size/server/job/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands","title":"zeus.optimizer.batch_size.server.job.commands","text":"<p>Commands to use <code>JobStateRepository</code>.</p>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateExpDefaultBs","title":"UpdateExpDefaultBs","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the exploration default batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>exp_default_batch_size</code> <code>int</code> <p>new default batch size to use.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateExpDefaultBs(BaseModel):\n    \"\"\"Parameters to update the exploration default batch size.\n\n    Attributes:\n        job_id: Job Id.\n        exp_default_batch_size: new default batch size to use.\n    \"\"\"\n\n    job_id: str\n    exp_default_batch_size: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateJobStage","title":"UpdateJobStage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the job stage.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>stage</code> <code>Stage</code> <p>Set it to MAB since we only go from Pruning to MAB.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateJobStage(BaseModel):\n    \"\"\"Parameters to update the job stage.\n\n    Attributes:\n        job_id: Job Id.\n        stage: Set it to MAB since we only go from Pruning to MAB.\n    \"\"\"\n\n    job_id: str\n    stage: Stage = Field(Stage.MAB, const=True)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateGeneratorState","title":"UpdateGeneratorState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the generator state.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>state</code> <code>str</code> <p>Generator state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateGeneratorState(BaseModel):\n    \"\"\"Parameters to update the generator state.\n\n    Attributes:\n        job_id: Job Id.\n        state: Generator state.\n    \"\"\"\n\n    job_id: str\n    state: str\n\n    @validator(\"state\")\n    def _validate_state(cls, state: str) -&gt; str:\n        \"\"\"Validate the sanity of state.\"\"\"\n        try:\n            np.random.default_rng(1).__setstate__(json.loads(state))\n            return state\n        except (TypeError, ValueError) as err:\n            raise ValueError(f\"Invalid generator state ({state})\") from err\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateGeneratorState._validate_state","title":"_validate_state","text":"<pre><code>_validate_state(state)\n</code></pre> <p>Validate the sanity of state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@validator(\"state\")\ndef _validate_state(cls, state: str) -&gt; str:\n    \"\"\"Validate the sanity of state.\"\"\"\n    try:\n        np.random.default_rng(1).__setstate__(json.loads(state))\n        return state\n    except (TypeError, ValueError) as err:\n        raise ValueError(f\"Invalid generator state ({state})\") from err\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateJobMinCost","title":"UpdateJobMinCost","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the min training cost and corresponding batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>min_cost</code> <code>float</code> <p>Min training cost.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Corresponding batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateJobMinCost(BaseModel):\n    \"\"\"Parameters to update the min training cost and corresponding batch size.\n\n    Attributes:\n        job_id: Job Id.\n        min_cost: Min training cost.\n        min_cost_batch_size: Corresponding batch size.\n    \"\"\"\n\n    job_id: str\n    min_cost: float = Field(ge=0)\n    min_cost_batch_size: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob","title":"CreateJob","text":"<p>               Bases: <code>GpuConfig</code>, <code>JobParams</code></p> <p>Parameters to create a new job.</p> <p>Attributes:</p> Name Type Description <code>exp_default_batch_size</code> <code>int</code> <p>Exploration default batch size that is used during Pruning stage.</p> <code>min_cost</code> <code>None</code> <p>Min training cost observed. Initially, None.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Batch size that has minimum training cost observed.</p> <code>stage</code> <code>Stage</code> <p>Stage of the job.</p> <code>mab_random_generator_state</code> <code>Optional[str]</code> <p>Generator state if mab_seed is not None. Otherwise, None.</p> <p>For the rest of attributes, refer to <code>JobParams</code>[zeus.optimizer.batch_size.common.JobParams] and <code>GpuConfig</code>[zeus.optimizer.batch_size.common.GpuConfig]</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class CreateJob(GpuConfig, JobParams):\n    \"\"\"Parameters to create a new job.\n\n    Attributes:\n        exp_default_batch_size: Exploration default batch size that is used during Pruning stage.\n        min_cost: Min training cost observed. Initially, None.\n        min_cost_batch_size: Batch size that has minimum training cost observed.\n        stage: Stage of the job.\n        mab_random_generator_state: Generator state if mab_seed is not None. Otherwise, None.\n\n    For the rest of attributes, refer to `JobParams`[zeus.optimizer.batch_size.common.JobParams] and `GpuConfig`[zeus.optimizer.batch_size.common.GpuConfig]\n    \"\"\"\n\n    exp_default_batch_size: int\n    min_cost: None = Field(None, const=True)\n    min_cost_batch_size: int\n    stage: Stage = Field(Stage.Pruning, const=True)\n    mab_random_generator_state: Optional[str] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after creation.\n        \"\"\"\n\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _validate_states(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate Job states.\n\n        We are checking,\n            - If mab seed and generator state is matching.\n            - If default, exp_default, min batch sizes are correctly intialized.\n            - If default batch size is in the list of batch sizes.\n        \"\"\"\n        state: str | None = values[\"mab_random_generator_state\"]\n        mab_seed: int | None = values[\"mab_seed\"]\n        bss: list[int] = values[\"batch_sizes\"]\n        dbs: int = values[\"default_batch_size\"]\n        ebs: int = values[\"exp_default_batch_size\"]\n        mbs: int = values[\"min_cost_batch_size\"]\n\n        if mab_seed is not None:\n            if state is None:\n                raise ValueError(\"mab_seed is not none, but generator state is none\")\n            else:\n                try:\n                    np.random.default_rng(1).__setstate__(json.loads(state))\n                except (TypeError, ValueError) as err:\n                    raise ValueError(f\"Invalid generator state ({state})\") from err\n\n        if not (dbs == ebs == mbs):\n            raise ValueError(\n                f\"During initialization, default_batch_size({dbs}), exp_default_batch_size({ebs}), min_batch_size({mbs}) should be all the same\"\n            )\n        if dbs not in bss:\n            raise ValueError(\n                f\"default_batch_size({dbs}) is not in the batch size list({bss})\"\n            )\n\n        return values\n\n    @classmethod\n    def from_job_config(cls, js: JobSpecFromClient) -&gt; \"CreateJob\":\n        \"\"\"From JobConfig, instantiate `CreateJob`.\n\n        Initialize generator state, exp_default_batch_size, and min_cost_batch_size.\n        \"\"\"\n        d = js.dict()\n        d[\"exp_default_batch_size\"] = js.default_batch_size\n        if js.mab_seed is not None:\n            rng = np.random.default_rng(js.mab_seed)\n            d[\"mab_random_generator_state\"] = json.dumps(rng.__getstate__())\n        d[\"min_cost_batch_size\"] = js.default_batch_size\n        return cls.parse_obj(d)\n\n    def to_orm(self) -&gt; JobTable:\n        \"\"\"Convert pydantic model `CreateJob` to ORM object Job.\"\"\"\n        d = self.dict()\n        job = JobTable()\n        for k, v in d.items():\n            if k != \"batch_sizes\":\n                setattr(job, k, v)\n        job.batch_sizes = [\n            BatchSizeTable(job_id=self.job_id, batch_size=bs) for bs in self.batch_sizes\n        ]\n        return job\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after creation.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after creation.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob._validate_states","title":"_validate_states","text":"<pre><code>_validate_states(values)\n</code></pre> <p>Validate Job states.</p> <p>We are checking,     - If mab seed and generator state is matching.     - If default, exp_default, min batch sizes are correctly intialized.     - If default batch size is in the list of batch sizes.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_states(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate Job states.\n\n    We are checking,\n        - If mab seed and generator state is matching.\n        - If default, exp_default, min batch sizes are correctly intialized.\n        - If default batch size is in the list of batch sizes.\n    \"\"\"\n    state: str | None = values[\"mab_random_generator_state\"]\n    mab_seed: int | None = values[\"mab_seed\"]\n    bss: list[int] = values[\"batch_sizes\"]\n    dbs: int = values[\"default_batch_size\"]\n    ebs: int = values[\"exp_default_batch_size\"]\n    mbs: int = values[\"min_cost_batch_size\"]\n\n    if mab_seed is not None:\n        if state is None:\n            raise ValueError(\"mab_seed is not none, but generator state is none\")\n        else:\n            try:\n                np.random.default_rng(1).__setstate__(json.loads(state))\n            except (TypeError, ValueError) as err:\n                raise ValueError(f\"Invalid generator state ({state})\") from err\n\n    if not (dbs == ebs == mbs):\n        raise ValueError(\n            f\"During initialization, default_batch_size({dbs}), exp_default_batch_size({ebs}), min_batch_size({mbs}) should be all the same\"\n        )\n    if dbs not in bss:\n        raise ValueError(\n            f\"default_batch_size({dbs}) is not in the batch size list({bss})\"\n        )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.from_job_config","title":"from_job_config  <code>classmethod</code>","text":"<pre><code>from_job_config(js)\n</code></pre> <p>From JobConfig, instantiate <code>CreateJob</code>.</p> <p>Initialize generator state, exp_default_batch_size, and min_cost_batch_size.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@classmethod\ndef from_job_config(cls, js: JobSpecFromClient) -&gt; \"CreateJob\":\n    \"\"\"From JobConfig, instantiate `CreateJob`.\n\n    Initialize generator state, exp_default_batch_size, and min_cost_batch_size.\n    \"\"\"\n    d = js.dict()\n    d[\"exp_default_batch_size\"] = js.default_batch_size\n    if js.mab_seed is not None:\n        rng = np.random.default_rng(js.mab_seed)\n        d[\"mab_random_generator_state\"] = json.dumps(rng.__getstate__())\n    d[\"min_cost_batch_size\"] = js.default_batch_size\n    return cls.parse_obj(d)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Convert pydantic model <code>CreateJob</code> to ORM object Job.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>def to_orm(self) -&gt; JobTable:\n    \"\"\"Convert pydantic model `CreateJob` to ORM object Job.\"\"\"\n    d = self.dict()\n    job = JobTable()\n    for k, v in d.items():\n        if k != \"batch_sizes\":\n            setattr(job, k, v)\n    job.batch_sizes = [\n        BatchSizeTable(job_id=self.job_id, batch_size=bs) for bs in self.batch_sizes\n    ]\n    return job\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/","title":"models","text":""},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models","title":"zeus.optimizer.batch_size.server.job.models","text":"<p>Pydantic models for Job.</p>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.Stage","title":"Stage","text":"<p>               Bases: <code>Enum</code></p> <p>Job Stage.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class Stage(Enum):\n    \"\"\"Job Stage.\"\"\"\n\n    Pruning = \"Pruning\"\n    MAB = \"MAB\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobGetter","title":"JobGetter","text":"<p>               Bases: <code>GetterDict</code></p> <p>Getter for batch size to convert ORM batch size object to integer.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class JobGetter(GetterDict):\n    \"\"\"Getter for batch size to convert ORM batch size object to integer.\"\"\"\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get value from dict.\"\"\"\n        if key == \"batch_sizes\":\n            # If the key is batch_sizes, parse the integer from object.\n            return [bs.batch_size for bs in self._obj.batch_sizes]\n\n        return super().get(key, default)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobGetter.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Get value from dict.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get value from dict.\"\"\"\n    if key == \"batch_sizes\":\n        # If the key is batch_sizes, parse the integer from object.\n        return [bs.batch_size for bs in self._obj.batch_sizes]\n\n    return super().get(key, default)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState","title":"JobState","text":"<p>               Bases: <code>JobParams</code>, <code>GpuConfig</code></p> <p>Pydantic model for Job which includes job-level states.</p> <p>Attributes:</p> Name Type Description <code>exp_default_batch_size</code> <code>int</code> <p>Exploration default batch size that is used during Pruning stage.</p> <code>min_cost</code> <code>Optional[float]</code> <p>Min training cost observed. Initially, None.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Batch size that has minimum training cost observed.</p> <code>stage</code> <code>Stage</code> <p>Stage of the job.</p> <code>mab_random_generator_state</code> <code>Optional[str]</code> <p>Generator state if mab_seed is not None. Otherwise, None.</p> <p>For the rest of attributes, refer to <code>JobParams</code> and <code>GpuConfig</code></p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class JobState(JobParams, GpuConfig):\n    \"\"\"Pydantic model for Job which includes job-level states.\n\n    Attributes:\n        exp_default_batch_size: Exploration default batch size that is used during Pruning stage.\n        min_cost: Min training cost observed. Initially, None.\n        min_cost_batch_size: Batch size that has minimum training cost observed.\n        stage: Stage of the job.\n        mab_random_generator_state: Generator state if mab_seed is not None. Otherwise, None.\n\n    For the rest of attributes, refer to [`JobParams`][zeus.optimizer.batch_size.common.JobParams] and [`GpuConfig`][zeus.optimizer.batch_size.common.GpuConfig]\n    \"\"\"\n\n    exp_default_batch_size: int\n\n    min_cost: Optional[float] = None\n    min_cost_batch_size: int\n    stage: Stage = Stage.Pruning\n\n    mab_random_generator_state: Optional[str] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Allow instantiating the model from an ORM object.\n        \"\"\"\n\n        orm_mode = True\n        getter_dict = JobGetter\n\n    @root_validator(skip_on_failure=True)\n    def _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate generator state.\"\"\"\n        state: str | None = values[\"mab_random_generator_state\"]\n        mab_seed: int | None = values[\"mab_seed\"]\n\n        if mab_seed is not None:\n            if state is None:\n                raise ValueError(\"mab_seed is not none, but generator state is none\")\n            else:\n                try:\n                    # Check sanity of the generator state.\n                    np.random.default_rng(1).__setstate__(json.loads(state))\n                except (TypeError, ValueError) as err:\n                    raise ValueError(f\"Invalid generator state ({state})\") from err\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState.Config","title":"Config","text":"<p>Model configuration.</p> <p>Allow instantiating the model from an ORM object.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Allow instantiating the model from an ORM object.\n    \"\"\"\n\n    orm_mode = True\n    getter_dict = JobGetter\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState._validate_mab","title":"_validate_mab","text":"<pre><code>_validate_mab(values)\n</code></pre> <p>Validate generator state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate generator state.\"\"\"\n    state: str | None = values[\"mab_random_generator_state\"]\n    mab_seed: int | None = values[\"mab_seed\"]\n\n    if mab_seed is not None:\n        if state is None:\n            raise ValueError(\"mab_seed is not none, but generator state is none\")\n        else:\n            try:\n                # Check sanity of the generator state.\n                np.random.default_rng(1).__setstate__(json.loads(state))\n            except (TypeError, ValueError) as err:\n                raise ValueError(f\"Invalid generator state ({state})\") from err\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository","title":"zeus.optimizer.batch_size.server.job.repository","text":"<p>Repository for manipulating Job table.</p>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository","title":"JobStateRepository","text":"<p>               Bases: <code>DatabaseRepository</code></p> <p>Repository that provides basic interfaces to interact with Job table.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>class JobStateRepository(DatabaseRepository):\n    \"\"\"Repository that provides basic interfaces to interact with Job table.\"\"\"\n\n    def __init__(self, session: AsyncSession):\n        \"\"\"Set db session and intialize job. We are working with only one job per session.\"\"\"\n        super().__init__(session)\n        self.fetched_job: JobTable | None = None\n\n    async def get_job(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            set fetched_job and return `JobState` if we found a job, unless return None.\n        \"\"\"\n        stmt = select(JobTable).where(JobTable.job_id == job_id)\n        job = await self.session.scalar(stmt)\n\n        if job is None:\n            logger.info(\"get_job: NoResultFound\")\n            return None\n\n        self.fetched_job = job\n        return JobState.from_orm(job)\n\n    def get_job_from_session(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get a job that was fetched from this session.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            Corresponding `JobState`. If none was found, return None.\n        \"\"\"\n        if self.fetched_job is None or self.fetched_job.job_id != job_id:\n            return None\n        return JobState.from_orm(self.fetched_job)\n\n    def update_exp_default_bs(self, updated_bs: UpdateExpDefaultBs) -&gt; None:\n        \"\"\"Update exploration default batch size on fetched job.\n\n        Args:\n            updated_bs: Job Id and new batch size.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if updated_bs.job_id == self.fetched_job.job_id:\n            self.fetched_job.exp_default_batch_size = updated_bs.exp_default_batch_size\n        else:\n            raise ZeusBSOValueError(\n                f\"Unknown job_id ({updated_bs.job_id}). Expecting {self.fetched_job.job_id}\"\n            )\n\n    def update_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n        \"\"\"Update stage on fetched job.\n\n        Args:\n            updated_stage: Job Id and new stage.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_stage.job_id:\n            self.fetched_job.stage = updated_stage.stage\n        else:\n            raise ZeusBSOValueError(\n                f\"Unknown job_id ({updated_stage.job_id}). Expecting {self.fetched_job.job_id}\"\n            )\n\n    def update_min(self, updated_min: UpdateJobMinCost) -&gt; None:\n        \"\"\"Update exploration min training cost and corresponding batch size on fetched job.\n\n        Args:\n            updated_min: Job Id, new min cost and batch size.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_min.job_id:\n            self.fetched_job.min_cost = updated_min.min_cost\n            self.fetched_job.min_cost_batch_size = updated_min.min_cost_batch_size\n        else:\n            raise ZeusBSOValueError(\n                f\"Unknown job_id ({updated_min.job_id}). Expecting {self.fetched_job.job_id}\"\n            )\n\n    def update_generator_state(self, updated_state: UpdateGeneratorState) -&gt; None:\n        \"\"\"Update generator state on fetched job.\n\n        Args:\n            updated_state: Job Id and new generator state.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_state.job_id:\n            self.fetched_job.mab_random_generator_state = updated_state.state\n        else:\n            raise ZeusBSOValueError(\n                f\"Unknown job_id ({updated_state.job_id}). Expecting {self.fetched_job.job_id}\"\n            )\n\n    def create_job(self, new_job: CreateJob) -&gt; None:\n        \"\"\"Create a new job by adding a new job to the session.\n\n        Args:\n            new_job: Job configuration for a new job.\n        \"\"\"\n        self.session.add(new_job.to_orm())\n\n    def check_job_fetched(self, job_id: str) -&gt; bool:\n        \"\"\"Check if this job is already fetched before.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            True if this job was fetched and in session. Otherwise, return false.\n        \"\"\"\n        return not (self.fetched_job is None or self.fetched_job.job_id != job_id)\n\n    async def delete_job(self, job_id: str) -&gt; bool:\n        \"\"\"Delete the job of a given job_Id.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            True if the job got deleted.\n        \"\"\"\n        stmt = select(JobTable).where(JobTable.job_id == job_id)\n        job = await self.session.scalar(stmt)\n\n        if job is None:\n            return False\n\n        # We can't straight delete using a query, since some db such as sqlite\n        # Foreign Key is default to OFF, so \"on delete = cascade\" will not be fired.\n        await self.session.delete(job)\n        return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def __init__(self, session: AsyncSession):\n    \"\"\"Set db session and intialize job. We are working with only one job per session.\"\"\"\n    super().__init__(session)\n    self.fetched_job: JobTable | None = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.get_job","title":"get_job  <code>async</code>","text":"<pre><code>get_job(job_id)\n</code></pre> <p>Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>set fetched_job and return <code>JobState</code> if we found a job, unless return None.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>async def get_job(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        set fetched_job and return `JobState` if we found a job, unless return None.\n    \"\"\"\n    stmt = select(JobTable).where(JobTable.job_id == job_id)\n    job = await self.session.scalar(stmt)\n\n    if job is None:\n        logger.info(\"get_job: NoResultFound\")\n        return None\n\n    self.fetched_job = job\n    return JobState.from_orm(job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.get_job_from_session","title":"get_job_from_session","text":"<pre><code>get_job_from_session(job_id)\n</code></pre> <p>Get a job that was fetched from this session.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>Corresponding <code>JobState</code>. If none was found, return None.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def get_job_from_session(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get a job that was fetched from this session.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        Corresponding `JobState`. If none was found, return None.\n    \"\"\"\n    if self.fetched_job is None or self.fetched_job.job_id != job_id:\n        return None\n    return JobState.from_orm(self.fetched_job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_exp_default_bs","title":"update_exp_default_bs","text":"<pre><code>update_exp_default_bs(updated_bs)\n</code></pre> <p>Update exploration default batch size on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_bs</code> <code>UpdateExpDefaultBs</code> <p>Job Id and new batch size.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_exp_default_bs(self, updated_bs: UpdateExpDefaultBs) -&gt; None:\n    \"\"\"Update exploration default batch size on fetched job.\n\n    Args:\n        updated_bs: Job Id and new batch size.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if updated_bs.job_id == self.fetched_job.job_id:\n        self.fetched_job.exp_default_batch_size = updated_bs.exp_default_batch_size\n    else:\n        raise ZeusBSOValueError(\n            f\"Unknown job_id ({updated_bs.job_id}). Expecting {self.fetched_job.job_id}\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_stage","title":"update_stage","text":"<pre><code>update_stage(updated_stage)\n</code></pre> <p>Update stage on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_stage</code> <code>UpdateJobStage</code> <p>Job Id and new stage.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n    \"\"\"Update stage on fetched job.\n\n    Args:\n        updated_stage: Job Id and new stage.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_stage.job_id:\n        self.fetched_job.stage = updated_stage.stage\n    else:\n        raise ZeusBSOValueError(\n            f\"Unknown job_id ({updated_stage.job_id}). Expecting {self.fetched_job.job_id}\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_min","title":"update_min","text":"<pre><code>update_min(updated_min)\n</code></pre> <p>Update exploration min training cost and corresponding batch size on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_min</code> <code>UpdateJobMinCost</code> <p>Job Id, new min cost and batch size.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_min(self, updated_min: UpdateJobMinCost) -&gt; None:\n    \"\"\"Update exploration min training cost and corresponding batch size on fetched job.\n\n    Args:\n        updated_min: Job Id, new min cost and batch size.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_min.job_id:\n        self.fetched_job.min_cost = updated_min.min_cost\n        self.fetched_job.min_cost_batch_size = updated_min.min_cost_batch_size\n    else:\n        raise ZeusBSOValueError(\n            f\"Unknown job_id ({updated_min.job_id}). Expecting {self.fetched_job.job_id}\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_generator_state","title":"update_generator_state","text":"<pre><code>update_generator_state(updated_state)\n</code></pre> <p>Update generator state on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_state</code> <code>UpdateGeneratorState</code> <p>Job Id and new generator state.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_generator_state(self, updated_state: UpdateGeneratorState) -&gt; None:\n    \"\"\"Update generator state on fetched job.\n\n    Args:\n        updated_state: Job Id and new generator state.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_state.job_id:\n        self.fetched_job.mab_random_generator_state = updated_state.state\n    else:\n        raise ZeusBSOValueError(\n            f\"Unknown job_id ({updated_state.job_id}). Expecting {self.fetched_job.job_id}\"\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.create_job","title":"create_job","text":"<pre><code>create_job(new_job)\n</code></pre> <p>Create a new job by adding a new job to the session.</p> <p>Parameters:</p> Name Type Description Default <code>new_job</code> <code>CreateJob</code> <p>Job configuration for a new job.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def create_job(self, new_job: CreateJob) -&gt; None:\n    \"\"\"Create a new job by adding a new job to the session.\n\n    Args:\n        new_job: Job configuration for a new job.\n    \"\"\"\n    self.session.add(new_job.to_orm())\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.check_job_fetched","title":"check_job_fetched","text":"<pre><code>check_job_fetched(job_id)\n</code></pre> <p>Check if this job is already fetched before.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this job was fetched and in session. Otherwise, return false.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def check_job_fetched(self, job_id: str) -&gt; bool:\n    \"\"\"Check if this job is already fetched before.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        True if this job was fetched and in session. Otherwise, return false.\n    \"\"\"\n    return not (self.fetched_job is None or self.fetched_job.job_id != job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete the job of a given job_Id.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the job got deleted.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; bool:\n    \"\"\"Delete the job of a given job_Id.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        True if the job got deleted.\n    \"\"\"\n    stmt = select(JobTable).where(JobTable.job_id == job_id)\n    job = await self.session.scalar(stmt)\n\n    if job is None:\n        return False\n\n    # We can't straight delete using a query, since some db such as sqlite\n    # Foreign Key is default to OFF, so \"on delete = cascade\" will not be fired.\n    await self.session.delete(job)\n    return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/","title":"services","text":""},{"location":"reference/optimizer/batch_size/server/services/#zeus.optimizer.batch_size.server.services","title":"zeus.optimizer.batch_size.server.services","text":"<p>Service layer on top of repository layer. Provides core methods to interact with database.</p>"},{"location":"reference/optimizer/batch_size/server/services/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands","title":"zeus.optimizer.batch_size.server.services.commands","text":"<p>Commands on how to use some methods from the <code>ZeusService</code>.</p>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.GetRandomChoices","title":"GetRandomChoices","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for getting a random choices.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id</p> <code>choices</code> <code>list[int]</code> <p>List of choices</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class GetRandomChoices(BaseModel):\n    \"\"\"Parameters for getting a random choices.\n\n    Attributes:\n        job_id: Job Id\n        choices: List of choices\n    \"\"\"\n\n    job_id: str\n    choices: list[int]\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.GetNormal","title":"GetNormal","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for getting a random sample from normal distribution.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job id</p> <code>loc</code> <code>float</code> <p>Mean</p> <code>scale</code> <code>float</code> <p>Stdev</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class GetNormal(BaseModel):\n    \"\"\"Parameters for getting a random sample from normal distribution.\n\n    Attributes:\n        job_id: Job id\n        loc: Mean\n        scale: Stdev\n    \"\"\"\n\n    job_id: str\n    loc: float\n    scale: float\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.UpdateArm","title":"UpdateArm","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update an arm.</p> <p>Attributes:</p> Name Type Description <code>trial</code> <code>ReadTrial</code> <p>Identifier of trial</p> <code>updated_arm</code> <code>GaussianTsArmState</code> <p>Updated state of arm.</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class UpdateArm(BaseModel):\n    \"\"\"Parameters to update an arm.\n\n    Attributes:\n        trial: Identifier of trial\n        updated_arm: Updated state of arm.\n    \"\"\"\n\n    trial: ReadTrial\n    updated_arm: GaussianTsArmState\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/","title":"service","text":""},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service","title":"zeus.optimizer.batch_size.server.services.service","text":"<p>Zeus batch size optimizer service layer.</p>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService","title":"ZeusService","text":"<p>Zeus Service that interacts with database using repository.</p> <p>Provides application layer methods to communicate with database. Each method is one or more number of db operations that have to be done at the same time.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>class ZeusService:\n    \"\"\"Zeus Service that interacts with database using repository.\n\n    Provides application layer methods to communicate with database.\n    Each method is one or more number of db operations that have to be done at the same time.\n    \"\"\"\n\n    def __init__(self, db_session: AsyncSession):\n        \"\"\"Set up repositories to use to talk to database.\"\"\"\n        self.bs_repo = BatchSizeStateRepository(db_session)\n        self.job_repo = JobStateRepository(db_session)\n\n    async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n        \"\"\"Get GaussianTs arm states for all arms(job_id, batch size).\n\n        Args:\n            job_id: Job id\n\n        Returns:\n            list of arms\n        \"\"\"\n        return await self.bs_repo.get_arms(job_id)\n\n    async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n        \"\"\"Get arm state for one arm.\n\n        Args:\n            bs: (job_id, batch size) pair that represents one arm\n\n        Returns:\n            Result arm state or None if we cannot find that arm\n        \"\"\"\n        return await self.bs_repo.get_arm(bs)\n\n    async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n        \"\"\"Get all explorations we have done for that job.\n\n        Args:\n            job_id: Job id\n\n        Returns:\n            list of explorations per each batch size\n        \"\"\"\n        return await self.bs_repo.get_explorations_of_job(job_id)\n\n    def update_trial(self, updated_trial: UpdateTrial) -&gt; None:\n        \"\"\"Update trial.\n\n        (1) update the corresponding trial.\n        (2) we update the min training cost observed so far if we have to.\n\n        Args:\n            updated_trial: Result of training that batch size\n\n        Raises:\n            [`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]: When we didn't fetch the job or trial during this session. This operation should have\n                    fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n        \"\"\"\n        trial = self._get_trial(\n            ReadTrial(\n                job_id=updated_trial.job_id,\n                batch_size=updated_trial.batch_size,\n                trial_number=updated_trial.trial_number,\n            )\n        )\n        if trial.status != TrialStatus.Dispatched:\n            raise ZeusBSOServiceBadOperationError(\"Trial already has a result.\")\n\n        self.bs_repo.updated_current_trial(updated_trial)\n\n        # Update the corresponding batch size's min cost if needed.\n        if updated_trial.status != TrialStatus.Failed:\n            job = self._get_job(updated_trial.job_id)\n            if updated_trial.energy is None or updated_trial.time is None:\n                raise ZeusBSOValueError(\n                    \"Energy and time should be set if the trial is not failed.\"\n                )\n            cur_cost = zeus_cost(\n                updated_trial.energy, updated_trial.time, job.eta_knob, job.max_power\n            )\n            if job.min_cost is None or job.min_cost &gt; cur_cost:\n                self.job_repo.update_min(\n                    UpdateJobMinCost(\n                        job_id=job.job_id,\n                        min_cost=cur_cost,\n                        min_cost_batch_size=updated_trial.batch_size,\n                    )\n                )\n\n    def update_arm_state(\n        self,\n        arm: UpdateArm,\n    ) -&gt; None:\n        \"\"\"Update arm state.\n\n        Args:\n            arm: Updated arm state.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job or trial during this session. This operation should have\n                    fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n        \"\"\"\n        self._check_job_fetched(arm.trial.job_id)\n        trial = self._get_trial(\n            ReadTrial(\n                job_id=arm.trial.job_id,\n                batch_size=arm.trial.batch_size,\n                trial_number=arm.trial.trial_number,\n            )\n        )\n        if trial.type != TrialType.MAB:\n            raise ZeusBSOServiceBadOperationError(\n                \"Cannot update an arm since this trial is not issued from MAB stage.\"\n            )\n        self.bs_repo.update_arm_state(arm.updated_arm)\n\n    def update_exp_default_bs(self, updated_default_bs: UpdateExpDefaultBs) -&gt; None:\n        \"\"\"Update the default batch size for exploration.\n\n        Args:\n            updated_default_bs: Job Id and new default batch size\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(updated_default_bs.job_id)\n        self.job_repo.update_exp_default_bs(updated_default_bs)\n\n    async def create_trial(\n        self, trial: CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial\n    ) -&gt; ReadTrial:\n        \"\"\"Create a new trial.\n\n        Args:\n            trial: New trial to create.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(trial.job_id)\n        trial_number = await self.bs_repo.get_next_trial_number(trial.job_id)\n        self.bs_repo.create_trial(\n            CreateTrial(**trial.dict(), trial_number=trial_number)\n        )\n        return ReadTrial(\n            job_id=trial.job_id, batch_size=trial.batch_size, trial_number=trial_number\n        )\n\n    def get_random_choices(self, choice: GetRandomChoices) -&gt; np.ndarray[Any, Any]:\n        \"\"\"Get randome choices based on job's seed.\n\n        If seed is not None (set by the user) we get the random choices from the generator that is stored in the database.\n        Otherwise, we get random choices based on random seed.\n\n        Args:\n            choice: Job id and list of choices\n\n        Returns:\n            reuslt random choices\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        arr = np.array(choice.choices)\n        rng, should_update = self._get_generator(choice.job_id)\n        res = rng.choice(arr, len(arr), replace=False)\n\n        if should_update:\n            # If we used the generator from database, should update the generator state after using it\n            self.job_repo.update_generator_state(\n                UpdateGeneratorState(\n                    job_id=choice.job_id, state=json.dumps(rng.__getstate__())\n                )\n            )\n\n        return res\n\n    def get_normal(self, arg: GetNormal) -&gt; float:\n        \"\"\"Sample from normal distribution and update the generator state if seed was set.\n\n        Args:\n            arg: args for `numpy.random.normal`, which is loc(mean of distribution) and scale(stdev of distribution)\n\n        Returns:\n            Drawn sample.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        rng, should_update = self._get_generator(arg.job_id)\n        res = rng.normal(arg.loc, arg.scale)\n\n        if should_update:\n            # If we used the generator from database, should update the generator state after using it\n            self.job_repo.update_generator_state(\n                UpdateGeneratorState(\n                    job_id=arg.job_id, state=json.dumps(rng.__getstate__())\n                )\n            )\n\n        return res\n\n    async def get_job(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get job from database.\n\n        Args:\n            job_id: Job Id\n\n        Returns:\n            JobState if we found one, None if we couldn't find a job matching the job id.\n        \"\"\"\n        return await self.job_repo.get_job(job_id)\n\n    async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Get a trial from database.\n\n        Args:\n            trial: (Job Id, batch size, trial_number) triplet.\n\n        Returns:\n            Trial if we found one, None if we couldn't find a job matching trial.\n        \"\"\"\n        return await self.bs_repo.get_trial(trial)\n\n    def create_job(self, new_job: CreateJob) -&gt; None:\n        \"\"\"Create a new job.\n\n        Args:\n            new_job: Configuration of a new job\n        \"\"\"\n        return self.job_repo.create_job(new_job)\n\n    async def get_trial_results_of_bs(self, bs: BatchSizeBase) -&gt; TrialResultsPerBs:\n        \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n        Args:\n            bs: (job_id, batch size) pair.\n\n        Returns:\n            list of windowed measurements in descending order for that (job_id, batch size)\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        job = self._get_job(bs.job_id)\n        return await self.bs_repo.get_trial_results_of_bs(\n            BatchSizeBase(job_id=bs.job_id, batch_size=bs.batch_size),\n            job.window_size,\n        )\n\n    def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n        \"\"\"Create GuassianTs arms for the job.\n\n        Args:\n            new_arms: List of new arm states\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        if len(new_arms) != 0:\n            self._check_job_fetched(new_arms[0].job_id)\n            self.bs_repo.create_arms(new_arms)\n\n    def update_job_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n        \"\"\"Update the job stage (Pruning -&gt; MAB).\n\n        Args:\n            updated_stage: Updated stage.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(updated_stage.job_id)\n        self.job_repo.update_stage(updated_stage)\n\n    async def delete_job(self, job_id: str) -&gt; bool:\n        \"\"\"Delete the job.\n\n        Args:\n            job_id: ID of the job.\n\n        Returns:\n            True if the job is deleted. False if none was deleted\n        \"\"\"\n        return await self.job_repo.delete_job(job_id)\n\n    def _get_generator(self, job_id: str) -&gt; tuple[np_Generator, bool]:\n        \"\"\"Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.\n\n        Returns:\n            Tuple of [Generator, if we should update state]\n        \"\"\"\n        job_state = self._get_job(job_id)\n\n        rng = np.random.default_rng(int(datetime.now().timestamp()))\n\n        should_update = job_state.mab_seed is not None\n        if job_state.mab_seed is not None:\n            if job_state.mab_random_generator_state is None:\n                raise ZeusBSOValueError(\n                    \"Seed is set but generator state is none. Should be impossible\"\n                )\n\n            state = json.loads(job_state.mab_random_generator_state)\n            rng.__setstate__(state)\n\n        return (rng, should_update)\n\n    def _get_job(self, job_id: str) -&gt; JobState:\n        \"\"\"Get the job from the session. If we couldn't find the job, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        res = self.job_repo.get_job_from_session(job_id)\n        if res is None:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Should have fetched the job first or job does not exist(job_id = {job_id})\"\n            )\n        return res\n\n    def _get_trial(self, trial: ReadTrial) -&gt; Trial:\n        \"\"\"Get the job from the session. If we couldn't find the trial, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        res = self.bs_repo.get_trial_from_session(trial)\n        if res is None:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Should have fetched the trial first or trial does not exist(trial = {trial})\"\n            )\n        return res\n\n    def _check_job_fetched(self, job_id: str) -&gt; None:\n        \"\"\"Check if we fetched the job in the current session. If we didn't raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        if not self.job_repo.check_job_fetched(job_id):\n            raise ZeusBSOServiceBadOperationError(\n                f\"check_job_fetched: {job_id} is not currently in the session\"\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.__init__","title":"__init__","text":"<pre><code>__init__(db_session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def __init__(self, db_session: AsyncSession):\n    \"\"\"Set up repositories to use to talk to database.\"\"\"\n    self.bs_repo = BatchSizeStateRepository(db_session)\n    self.job_repo = JobStateRepository(db_session)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_arms","title":"get_arms  <code>async</code>","text":"<pre><code>get_arms(job_id)\n</code></pre> <p>Get GaussianTs arm states for all arms(job_id, batch size).</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>list of arms</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n    \"\"\"Get GaussianTs arm states for all arms(job_id, batch size).\n\n    Args:\n        job_id: Job id\n\n    Returns:\n        list of arms\n    \"\"\"\n    return await self.bs_repo.get_arms(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_arm","title":"get_arm  <code>async</code>","text":"<pre><code>get_arm(bs)\n</code></pre> <p>Get arm state for one arm.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>(job_id, batch size) pair that represents one arm</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState | None</code> <p>Result arm state or None if we cannot find that arm</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n    \"\"\"Get arm state for one arm.\n\n    Args:\n        bs: (job_id, batch size) pair that represents one arm\n\n    Returns:\n        Result arm state or None if we cannot find that arm\n    \"\"\"\n    return await self.bs_repo.get_arm(bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_explorations_of_job","title":"get_explorations_of_job  <code>async</code>","text":"<pre><code>get_explorations_of_job(job_id)\n</code></pre> <p>Get all explorations we have done for that job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id</p> required <p>Returns:</p> Type Description <code>ExplorationsPerJob</code> <p>list of explorations per each batch size</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n    \"\"\"Get all explorations we have done for that job.\n\n    Args:\n        job_id: Job id\n\n    Returns:\n        list of explorations per each batch size\n    \"\"\"\n    return await self.bs_repo.get_explorations_of_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_trial","title":"update_trial","text":"<pre><code>update_trial(updated_trial)\n</code></pre> <p>Update trial.</p> <p>(1) update the corresponding trial. (2) we update the min training cost observed so far if we have to.</p> <p>Parameters:</p> Name Type Description Default <code>updated_trial</code> <code>UpdateTrial</code> <p>Result of training that batch size</p> required <p>Raises:</p> Type Description <code>[`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]</code> <p>When we didn't fetch the job or trial during this session. This operation should have     fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_trial(self, updated_trial: UpdateTrial) -&gt; None:\n    \"\"\"Update trial.\n\n    (1) update the corresponding trial.\n    (2) we update the min training cost observed so far if we have to.\n\n    Args:\n        updated_trial: Result of training that batch size\n\n    Raises:\n        [`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]: When we didn't fetch the job or trial during this session. This operation should have\n                fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n    \"\"\"\n    trial = self._get_trial(\n        ReadTrial(\n            job_id=updated_trial.job_id,\n            batch_size=updated_trial.batch_size,\n            trial_number=updated_trial.trial_number,\n        )\n    )\n    if trial.status != TrialStatus.Dispatched:\n        raise ZeusBSOServiceBadOperationError(\"Trial already has a result.\")\n\n    self.bs_repo.updated_current_trial(updated_trial)\n\n    # Update the corresponding batch size's min cost if needed.\n    if updated_trial.status != TrialStatus.Failed:\n        job = self._get_job(updated_trial.job_id)\n        if updated_trial.energy is None or updated_trial.time is None:\n            raise ZeusBSOValueError(\n                \"Energy and time should be set if the trial is not failed.\"\n            )\n        cur_cost = zeus_cost(\n            updated_trial.energy, updated_trial.time, job.eta_knob, job.max_power\n        )\n        if job.min_cost is None or job.min_cost &gt; cur_cost:\n            self.job_repo.update_min(\n                UpdateJobMinCost(\n                    job_id=job.job_id,\n                    min_cost=cur_cost,\n                    min_cost_batch_size=updated_trial.batch_size,\n                )\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_arm_state","title":"update_arm_state","text":"<pre><code>update_arm_state(arm)\n</code></pre> <p>Update arm state.</p> <p>Parameters:</p> Name Type Description Default <code>arm</code> <code>UpdateArm</code> <p>Updated arm state.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job or trial during this session. This operation should have     fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_arm_state(\n    self,\n    arm: UpdateArm,\n) -&gt; None:\n    \"\"\"Update arm state.\n\n    Args:\n        arm: Updated arm state.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job or trial during this session. This operation should have\n                fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n    \"\"\"\n    self._check_job_fetched(arm.trial.job_id)\n    trial = self._get_trial(\n        ReadTrial(\n            job_id=arm.trial.job_id,\n            batch_size=arm.trial.batch_size,\n            trial_number=arm.trial.trial_number,\n        )\n    )\n    if trial.type != TrialType.MAB:\n        raise ZeusBSOServiceBadOperationError(\n            \"Cannot update an arm since this trial is not issued from MAB stage.\"\n        )\n    self.bs_repo.update_arm_state(arm.updated_arm)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_exp_default_bs","title":"update_exp_default_bs","text":"<pre><code>update_exp_default_bs(updated_default_bs)\n</code></pre> <p>Update the default batch size for exploration.</p> <p>Parameters:</p> Name Type Description Default <code>updated_default_bs</code> <code>UpdateExpDefaultBs</code> <p>Job Id and new default batch size</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_exp_default_bs(self, updated_default_bs: UpdateExpDefaultBs) -&gt; None:\n    \"\"\"Update the default batch size for exploration.\n\n    Args:\n        updated_default_bs: Job Id and new default batch size\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(updated_default_bs.job_id)\n    self.job_repo.update_exp_default_bs(updated_default_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_trial","title":"create_trial  <code>async</code>","text":"<pre><code>create_trial(trial)\n</code></pre> <p>Create a new trial.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial</code> <p>New trial to create.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def create_trial(\n    self, trial: CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial\n) -&gt; ReadTrial:\n    \"\"\"Create a new trial.\n\n    Args:\n        trial: New trial to create.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(trial.job_id)\n    trial_number = await self.bs_repo.get_next_trial_number(trial.job_id)\n    self.bs_repo.create_trial(\n        CreateTrial(**trial.dict(), trial_number=trial_number)\n    )\n    return ReadTrial(\n        job_id=trial.job_id, batch_size=trial.batch_size, trial_number=trial_number\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_random_choices","title":"get_random_choices","text":"<pre><code>get_random_choices(choice)\n</code></pre> <p>Get randome choices based on job's seed.</p> <p>If seed is not None (set by the user) we get the random choices from the generator that is stored in the database. Otherwise, we get random choices based on random seed.</p> <p>Parameters:</p> Name Type Description Default <code>choice</code> <code>GetRandomChoices</code> <p>Job id and list of choices</p> required <p>Returns:</p> Type Description <code>ndarray[Any, Any]</code> <p>reuslt random choices</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def get_random_choices(self, choice: GetRandomChoices) -&gt; np.ndarray[Any, Any]:\n    \"\"\"Get randome choices based on job's seed.\n\n    If seed is not None (set by the user) we get the random choices from the generator that is stored in the database.\n    Otherwise, we get random choices based on random seed.\n\n    Args:\n        choice: Job id and list of choices\n\n    Returns:\n        reuslt random choices\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    arr = np.array(choice.choices)\n    rng, should_update = self._get_generator(choice.job_id)\n    res = rng.choice(arr, len(arr), replace=False)\n\n    if should_update:\n        # If we used the generator from database, should update the generator state after using it\n        self.job_repo.update_generator_state(\n            UpdateGeneratorState(\n                job_id=choice.job_id, state=json.dumps(rng.__getstate__())\n            )\n        )\n\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_normal","title":"get_normal","text":"<pre><code>get_normal(arg)\n</code></pre> <p>Sample from normal distribution and update the generator state if seed was set.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>GetNormal</code> <p>args for <code>numpy.random.normal</code>, which is loc(mean of distribution) and scale(stdev of distribution)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Drawn sample.</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def get_normal(self, arg: GetNormal) -&gt; float:\n    \"\"\"Sample from normal distribution and update the generator state if seed was set.\n\n    Args:\n        arg: args for `numpy.random.normal`, which is loc(mean of distribution) and scale(stdev of distribution)\n\n    Returns:\n        Drawn sample.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    rng, should_update = self._get_generator(arg.job_id)\n    res = rng.normal(arg.loc, arg.scale)\n\n    if should_update:\n        # If we used the generator from database, should update the generator state after using it\n        self.job_repo.update_generator_state(\n            UpdateGeneratorState(\n                job_id=arg.job_id, state=json.dumps(rng.__getstate__())\n            )\n        )\n\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_job","title":"get_job  <code>async</code>","text":"<pre><code>get_job(job_id)\n</code></pre> <p>Get job from database.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job Id</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>JobState if we found one, None if we couldn't find a job matching the job id.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_job(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get job from database.\n\n    Args:\n        job_id: Job Id\n\n    Returns:\n        JobState if we found one, None if we couldn't find a job matching the job id.\n    \"\"\"\n    return await self.job_repo.get_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_trial","title":"get_trial  <code>async</code>","text":"<pre><code>get_trial(trial)\n</code></pre> <p>Get a trial from database.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>ReadTrial</code> <p>(Job Id, batch size, trial_number) triplet.</p> required <p>Returns:</p> Type Description <code>Trial | None</code> <p>Trial if we found one, None if we couldn't find a job matching trial.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Get a trial from database.\n\n    Args:\n        trial: (Job Id, batch size, trial_number) triplet.\n\n    Returns:\n        Trial if we found one, None if we couldn't find a job matching trial.\n    \"\"\"\n    return await self.bs_repo.get_trial(trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_job","title":"create_job","text":"<pre><code>create_job(new_job)\n</code></pre> <p>Create a new job.</p> <p>Parameters:</p> Name Type Description Default <code>new_job</code> <code>CreateJob</code> <p>Configuration of a new job</p> required Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def create_job(self, new_job: CreateJob) -&gt; None:\n    \"\"\"Create a new job.\n\n    Args:\n        new_job: Configuration of a new job\n    \"\"\"\n    return self.job_repo.create_job(new_job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_trial_results_of_bs","title":"get_trial_results_of_bs  <code>async</code>","text":"<pre><code>get_trial_results_of_bs(bs)\n</code></pre> <p>Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>(job_id, batch size) pair.</p> required <p>Returns:</p> Type Description <code>TrialResultsPerBs</code> <p>list of windowed measurements in descending order for that (job_id, batch size)</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_trial_results_of_bs(self, bs: BatchSizeBase) -&gt; TrialResultsPerBs:\n    \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n    Args:\n        bs: (job_id, batch size) pair.\n\n    Returns:\n        list of windowed measurements in descending order for that (job_id, batch size)\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    job = self._get_job(bs.job_id)\n    return await self.bs_repo.get_trial_results_of_bs(\n        BatchSizeBase(job_id=bs.job_id, batch_size=bs.batch_size),\n        job.window_size,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_arms","title":"create_arms","text":"<pre><code>create_arms(new_arms)\n</code></pre> <p>Create GuassianTs arms for the job.</p> <p>Parameters:</p> Name Type Description Default <code>new_arms</code> <code>list[GaussianTsArmState]</code> <p>List of new arm states</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n    \"\"\"Create GuassianTs arms for the job.\n\n    Args:\n        new_arms: List of new arm states\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    if len(new_arms) != 0:\n        self._check_job_fetched(new_arms[0].job_id)\n        self.bs_repo.create_arms(new_arms)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_job_stage","title":"update_job_stage","text":"<pre><code>update_job_stage(updated_stage)\n</code></pre> <p>Update the job stage (Pruning -&gt; MAB).</p> <p>Parameters:</p> Name Type Description Default <code>updated_stage</code> <code>UpdateJobStage</code> <p>Updated stage.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_job_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n    \"\"\"Update the job stage (Pruning -&gt; MAB).\n\n    Args:\n        updated_stage: Updated stage.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(updated_stage.job_id)\n    self.job_repo.update_stage(updated_stage)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete the job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the job.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the job is deleted. False if none was deleted</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; bool:\n    \"\"\"Delete the job.\n\n    Args:\n        job_id: ID of the job.\n\n    Returns:\n        True if the job is deleted. False if none was deleted\n    \"\"\"\n    return await self.job_repo.delete_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_generator","title":"_get_generator","text":"<pre><code>_get_generator(job_id)\n</code></pre> <p>Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.</p> <p>Returns:</p> Type Description <code>tuple[Generator, bool]</code> <p>Tuple of [Generator, if we should update state]</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_generator(self, job_id: str) -&gt; tuple[np_Generator, bool]:\n    \"\"\"Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.\n\n    Returns:\n        Tuple of [Generator, if we should update state]\n    \"\"\"\n    job_state = self._get_job(job_id)\n\n    rng = np.random.default_rng(int(datetime.now().timestamp()))\n\n    should_update = job_state.mab_seed is not None\n    if job_state.mab_seed is not None:\n        if job_state.mab_random_generator_state is None:\n            raise ZeusBSOValueError(\n                \"Seed is set but generator state is none. Should be impossible\"\n            )\n\n        state = json.loads(job_state.mab_random_generator_state)\n        rng.__setstate__(state)\n\n    return (rng, should_update)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_job","title":"_get_job","text":"<pre><code>_get_job(job_id)\n</code></pre> <p>Get the job from the session. If we couldn't find the job, raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_job(self, job_id: str) -&gt; JobState:\n    \"\"\"Get the job from the session. If we couldn't find the job, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    res = self.job_repo.get_job_from_session(job_id)\n    if res is None:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Should have fetched the job first or job does not exist(job_id = {job_id})\"\n        )\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_trial","title":"_get_trial","text":"<pre><code>_get_trial(trial)\n</code></pre> <p>Get the job from the session. If we couldn't find the trial, raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_trial(self, trial: ReadTrial) -&gt; Trial:\n    \"\"\"Get the job from the session. If we couldn't find the trial, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    res = self.bs_repo.get_trial_from_session(trial)\n    if res is None:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Should have fetched the trial first or trial does not exist(trial = {trial})\"\n        )\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._check_job_fetched","title":"_check_job_fetched","text":"<pre><code>_check_job_fetched(job_id)\n</code></pre> <p>Check if we fetched the job in the current session. If we didn't raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _check_job_fetched(self, job_id: str) -&gt; None:\n    \"\"\"Check if we fetched the job in the current session. If we didn't raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    if not self.job_repo.check_job_fetched(job_id):\n        raise ZeusBSOServiceBadOperationError(\n            f\"check_job_fetched: {job_id} is not currently in the session\"\n        )\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/","title":"pipeline_frequency","text":""},{"location":"reference/optimizer/pipeline_frequency/#zeus.optimizer.pipeline_frequency","title":"zeus.optimizer.pipeline_frequency","text":"<p>Optimize the energy consumption of large model training with Perseus.</p> <p>A a high-level, this optimizer assigns each forward and backward computation in a pipeline parallel training iteration with a GPU frequency that leads to a Pareto-optimal training iteration time and energy consumption.</p> <p>Currently, this optimizer depends on PyTorch.</p>"},{"location":"reference/optimizer/pipeline_frequency/common/","title":"common","text":""},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common","title":"zeus.optimizer.pipeline_frequency.common","text":"<p>Shared constants and models between the server and the client (optimizer).</p>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings","title":"PFOServerSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>PFO server settings, configurable via environment variables.</p> <p>For instance, setting <code>ZEUS_PFO_LOG_LEVEL=INFO</code> will automatically set the <code>log_level</code> variable to <code>\"INFO\"</code>.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>PyObject</code> <p>Name of the <code>FrequencyScheduler</code> to use.</p> <code>scheduler_args</code> <code>dict[str, Any]</code> <p>Any extra arguments required by <code>scheduler.__init__</code>.</p> <code>log_level</code> <code>str</code> <p>Log level, e.g. \"debug\", \"info\".</p> <code>dump_data</code> <code>bool</code> <p>Whether the scheduler should dump internal state to the filesystem (for future inspection purposes).</p> <code>dump_dir</code> <code>str</code> <p>Directory to dump state in (if enabled)</p> <code>max_job_idle_time</code> <code>int</code> <p>Maximum time in seconds that a job can be idle for before its states are automatically deleted from the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class PFOServerSettings(BaseSettings):\n    \"\"\"PFO server settings, configurable via environment variables.\n\n    For instance, setting `ZEUS_PFO_LOG_LEVEL=INFO` will automatically set\n    the `log_level` variable to `\"INFO\"`.\n\n    Attributes:\n        scheduler: Name of the `FrequencyScheduler` to use.\n        scheduler_args: Any extra arguments required by `scheduler.__init__`.\n        log_level: Log level, e.g. \"debug\", \"info\".\n        dump_data: Whether the scheduler should dump internal state to the filesystem\n            (for future inspection purposes).\n        dump_dir: Directory to dump state in (if enabled)\n        max_job_idle_time: Maximum time in seconds that a job can be idle for before\n            its states are automatically deleted from the server.\n    \"\"\"\n\n    scheduler: PyObject = \"PointSolution\"  # type: ignore\n    scheduler_args: dict[str, Any] = {}\n    log_level: str = \"DEBUG\"\n    dump_data: bool = True\n    dump_dir: str = \"./dump\"\n    max_job_idle_time: int = 60 * 60 * 24 * 7  # 1 week\n\n    @validator(\"scheduler\", pre=True)\n    def _fix_scheduler_import_path(cls, value):\n        \"\"\"Prepend `zeus.optimizer.pipeline_frequency.server.scheduler.` to the scheduler type name.\"\"\"\n        return f\"zeus.optimizer.pipeline_frequency.server.scheduler.{value}\"\n\n    @validator(\"scheduler_args\")\n    def _validate_scheduler_args(cls, args, values):\n        \"\"\"Check whether args are as expected by the scheduler's constructor.\"\"\"\n        scheduler = values[\"scheduler\"]\n        full_args = args | dict(job_info=None, rank_infos=None, pfo_settings=None)\n        constructor_args = inspect.signature(scheduler)\n        try:\n            constructor_args.bind(**full_args)\n        except TypeError as e:\n            raise ValueError(f\"Invalid scheduler args: {e}\") from None\n        return args\n\n    @validator(\"log_level\")\n    def _make_upper_case(cls, value):\n        return value.upper()\n\n    class Config:  # type: ignore\n        \"\"\"Configuration class read by pydantic.\"\"\"\n\n        env_prefix = \"zeus_pfo_\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings.Config","title":"Config","text":"<p>Configuration class read by pydantic.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class Config:  # type: ignore\n    \"\"\"Configuration class read by pydantic.\"\"\"\n\n    env_prefix = \"zeus_pfo_\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings._fix_scheduler_import_path","title":"_fix_scheduler_import_path","text":"<pre><code>_fix_scheduler_import_path(value)\n</code></pre> <p>Prepend <code>zeus.optimizer.pipeline_frequency.server.scheduler.</code> to the scheduler type name.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"scheduler\", pre=True)\ndef _fix_scheduler_import_path(cls, value):\n    \"\"\"Prepend `zeus.optimizer.pipeline_frequency.server.scheduler.` to the scheduler type name.\"\"\"\n    return f\"zeus.optimizer.pipeline_frequency.server.scheduler.{value}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings._validate_scheduler_args","title":"_validate_scheduler_args","text":"<pre><code>_validate_scheduler_args(args, values)\n</code></pre> <p>Check whether args are as expected by the scheduler's constructor.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"scheduler_args\")\ndef _validate_scheduler_args(cls, args, values):\n    \"\"\"Check whether args are as expected by the scheduler's constructor.\"\"\"\n    scheduler = values[\"scheduler\"]\n    full_args = args | dict(job_info=None, rank_infos=None, pfo_settings=None)\n    constructor_args = inspect.signature(scheduler)\n    try:\n        constructor_args.bind(**full_args)\n    except TypeError as e:\n        raise ValueError(f\"Invalid scheduler args: {e}\") from None\n    return args\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo","title":"JobInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Training job information reported to the server.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Globally unique ID of the training job, generated by the server. This field should be an empty string when sent to the server.</p> <code>pp_degree</code> <code>int</code> <p>Pipeline parallel degree.</p> <code>dp_degree</code> <code>int</code> <p>Data parallel degree.</p> <code>tp_degree</code> <code>int</code> <p>Tensor parallel degree.</p> <code>world_size</code> <code>int</code> <p>World size of the training job.</p> <code>job_metadata</code> <code>Optional[str]</code> <p>An optional arbitrary string that describes the job. This will be appended to the job ID if given. Typically for logging purposes.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class JobInfo(BaseModel):\n    \"\"\"Training job information reported to the server.\n\n    Attributes:\n        job_id: Globally unique ID of the training job, generated by the server.\n            This field should be an empty string when sent to the server.\n        pp_degree: Pipeline parallel degree.\n        dp_degree: Data parallel degree.\n        tp_degree: Tensor parallel degree.\n        world_size: World size of the training job.\n        job_metadata: An optional arbitrary string that describes the job. This will\n            be appended to the job ID if given. Typically for logging purposes.\n    \"\"\"\n\n    job_id: str = \"\"\n    pp_degree: int = Field(ge=1)\n    dp_degree: int = Field(ge=1)\n    tp_degree: int = Field(ge=1)\n    world_size: int = Field(ge=1)\n    job_metadata: Optional[str] = None\n\n    @validator(\"job_id\")\n    def _check_empty_job_id(cls, job_id):\n        assert not job_id\n        return job_id\n\n    @validator(\"world_size\")\n    def _check_world_size(cls, world_size, values):\n        \"\"\"Product of PP, DP, and TP degree would be identical to the world size.\"\"\"\n        assert (\n            values[\"pp_degree\"] * values[\"dp_degree\"] * values[\"tp_degree\"]\n            == world_size\n        )\n        return world_size\n\n    def set_job_id(self, scheduler_name: str):\n        \"\"\"Generate and set the job ID.\"\"\"\n        self.job_id = \"+\".join(\n            [\n                datetime.now().strftime(\"%F-%H-%M-%S\"),\n                f\"dp{self.dp_degree}\",\n                f\"pp{self.pp_degree}\",\n                f\"tp{self.tp_degree}\",\n                scheduler_name,\n            ]\n        )\n        if self.job_metadata:\n            self.job_id += f\"+{self.job_metadata}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo._check_world_size","title":"_check_world_size","text":"<pre><code>_check_world_size(world_size, values)\n</code></pre> <p>Product of PP, DP, and TP degree would be identical to the world size.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"world_size\")\ndef _check_world_size(cls, world_size, values):\n    \"\"\"Product of PP, DP, and TP degree would be identical to the world size.\"\"\"\n    assert (\n        values[\"pp_degree\"] * values[\"dp_degree\"] * values[\"tp_degree\"]\n        == world_size\n    )\n    return world_size\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo.set_job_id","title":"set_job_id","text":"<pre><code>set_job_id(scheduler_name)\n</code></pre> <p>Generate and set the job ID.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def set_job_id(self, scheduler_name: str):\n    \"\"\"Generate and set the job ID.\"\"\"\n    self.job_id = \"+\".join(\n        [\n            datetime.now().strftime(\"%F-%H-%M-%S\"),\n            f\"dp{self.dp_degree}\",\n            f\"pp{self.pp_degree}\",\n            f\"tp{self.tp_degree}\",\n            scheduler_name,\n        ]\n    )\n    if self.job_metadata:\n        self.job_id += f\"+{self.job_metadata}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.RankInfo","title":"RankInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information passed to the server from each rank.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting process.</p> <code>dp_rank</code> <code>int</code> <p>Data parallel rank of the reporting procees.</p> <code>pp_rank</code> <code>int</code> <p>Pipeline parallel rank of the reporting procees.</p> <code>tp_rank</code> <code>int</code> <p>Tensor parallel rank of the reporting procees.</p> <code>available_frequencies</code> <code>list[int]</code> <p>List of available frequencies for the rank's GPU.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class RankInfo(BaseModel):\n    \"\"\"Information passed to the server from each rank.\n\n    Attributes:\n        rank: Global rank of the reporting process.\n        dp_rank: Data parallel rank of the reporting procees.\n        pp_rank: Pipeline parallel rank of the reporting procees.\n        tp_rank: Tensor parallel rank of the reporting procees.\n        available_frequencies: List of available frequencies for the rank's GPU.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    dp_rank: int = Field(ge=0)\n    pp_rank: int = Field(ge=0)\n    tp_rank: int = Field(ge=0)\n    available_frequencies: list[int]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.FrequencySchedule","title":"FrequencySchedule","text":"<p>               Bases: <code>BaseModel</code></p> <p>Frequency schedule for one iteration.</p> <p><code>frequencies</code> is a list of tuples, where the first element is the name of the instruction and the second element is the frequency to use for that instruction.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class FrequencySchedule(BaseModel):\n    \"\"\"Frequency schedule for one iteration.\n\n    `frequencies` is a list of tuples, where the first element is the name of the\n    instruction and the second element is the frequency to use for that instruction.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    frequencies: list[tuple[str, int]]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.ProfilingResult","title":"ProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Profiling results for a <code>FrequencySchedule</code> of a rank.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting client.</p> <code>iter_time</code> <code>list[float]</code> <p>List of latency of all iterations within the profiling window in seconds.</p> <code>iter_energy</code> <code>list[float]</code> <p>List of energy consumption of all iterations within the profiling window in Joules.</p> <code>time_breakdown</code> <code>dict[str, list[list[float]]]</code> <p>Duration of each operation across multiple iterations. e.g. <code>time_breakdown[\"forward\"][i]</code> is the list of latencies of all forward computations in the <code>i</code>th iteration.</p> <code>energy_breakdown</code> <code>dict[str, list[list[float]]]</code> <p>Energy consumption of each operation across multple iterations. Value has the same structure as <code>time_breakdown</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class ProfilingResult(BaseModel):\n    \"\"\"Profiling results for a `FrequencySchedule` of a rank.\n\n    Attributes:\n        rank: Global rank of the reporting client.\n        iter_time: List of latency of all iterations within the profiling window in seconds.\n        iter_energy: List of energy consumption of all iterations within the profiling window in Joules.\n        time_breakdown: Duration of each operation across multiple iterations.\n            e.g. `time_breakdown[\"forward\"][i]` is the list of latencies of all forward computations\n            in the `i`th iteration.\n        energy_breakdown: Energy consumption of each operation across multple iterations.\n            Value has the same structure as `time_breakdown`.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    iter_time: list[float]\n    iter_energy: list[float]\n    time_breakdown: dict[str, list[list[float]]] = {}\n    energy_breakdown: dict[str, list[list[float]]] = {}\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.OfflineProfilingResult","title":"OfflineProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Profiling results generated from offline profiling each instruction.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting client.</p> <code>dp_rank</code> <code>int</code> <p>Data parallel rank of the reporting procees.</p> <code>pp_rank</code> <code>int</code> <p>Pipeline parallel rank of the reporting procees.</p> <code>tp_rank</code> <code>int</code> <p>Tensor parallel rank of the reporting procees.</p> <code>forward_time</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average forward computation time.</p> <code>forward_energy</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average forward computation energy.</p> <code>backward_time</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average backward computation time.</p> <code>backward_energy</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average backward computation energy.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class OfflineProfilingResult(BaseModel):\n    \"\"\"Profiling results generated from offline profiling each instruction.\n\n    Attributes:\n        rank: Global rank of the reporting client.\n        dp_rank: Data parallel rank of the reporting procees.\n        pp_rank: Pipeline parallel rank of the reporting procees.\n        tp_rank: Tensor parallel rank of the reporting procees.\n        forward_time: Dict that maps frequency to average forward computation time.\n        forward_energy: Dict that maps frequency to average forward computation energy.\n        backward_time: Dict that maps frequency to average backward computation time.\n        backward_energy: Dict that maps frequency to average backward computation energy.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    dp_rank: int = Field(ge=0)\n    pp_rank: int = Field(ge=0)\n    tp_rank: int = Field(ge=0)\n    forward_time: dict[int, float]\n    forward_energy: dict[int, float]\n    backward_time: dict[int, float]\n    backward_energy: dict[int, float]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.InstructionProfilingResult","title":"InstructionProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Time and energy profiling results for each instruction in each stage.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class InstructionProfilingResult(BaseModel):\n    \"\"\"Time and energy profiling results for each instruction in each stage.\"\"\"\n\n    __root__: list[OfflineProfilingResult]\n\n    def to_csv(self, filepath: str) -&gt; None:\n        \"\"\"Serialize and save this object into a CSV file.\n\n        Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy\n        Notes\n            - `rank` is the global rank of the process.\n            - `pp_rank` and `stage` are always the same, for backwards compatibility.\n            - All ranks and `stage` are zero-indexed.\n            - `instruction` is either \"forward\" or \"backward\".\n            - `time` and `energy` are already averaged over profiling iterations.\n        \"\"\"\n        if not filepath.endswith(\".csv\"):\n            raise ValueError(\"Filepath does not end with '.csv'\")\n\n        # fmt: off\n        headers = [\"rank\", \"dp_rank\", \"pp_rank\", \"tp_rank\", \"stage\", \"instruction\", \"frequency\", \"time\", \"energy\"]\n        records: list[tuple[int, int, int, int, int, str, int, float, float]] = []\n        for res in self.__root__:\n            prefix = (res.rank, res.dp_rank, res.pp_rank, res.tp_rank, res.pp_rank)\n            for freq in res.forward_time:\n                records.append((*prefix, \"forward\", freq, res.forward_time[freq], res.forward_energy[freq]))\n            for freq in res.backward_time:\n                records.append((*prefix, \"backward\", freq, res.backward_time[freq], res.backward_energy[freq]))\n        # fmt: on\n\n        df = pd.DataFrame.from_records(records, columns=headers)\n        df.to_csv(filepath, index=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.InstructionProfilingResult.to_csv","title":"to_csv","text":"<pre><code>to_csv(filepath)\n</code></pre> <p>Serialize and save this object into a CSV file.</p> <p>Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy Notes     - <code>rank</code> is the global rank of the process.     - <code>pp_rank</code> and <code>stage</code> are always the same, for backwards compatibility.     - All ranks and <code>stage</code> are zero-indexed.     - <code>instruction</code> is either \"forward\" or \"backward\".     - <code>time</code> and <code>energy</code> are already averaged over profiling iterations.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def to_csv(self, filepath: str) -&gt; None:\n    \"\"\"Serialize and save this object into a CSV file.\n\n    Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy\n    Notes\n        - `rank` is the global rank of the process.\n        - `pp_rank` and `stage` are always the same, for backwards compatibility.\n        - All ranks and `stage` are zero-indexed.\n        - `instruction` is either \"forward\" or \"backward\".\n        - `time` and `energy` are already averaged over profiling iterations.\n    \"\"\"\n    if not filepath.endswith(\".csv\"):\n        raise ValueError(\"Filepath does not end with '.csv'\")\n\n    # fmt: off\n    headers = [\"rank\", \"dp_rank\", \"pp_rank\", \"tp_rank\", \"stage\", \"instruction\", \"frequency\", \"time\", \"energy\"]\n    records: list[tuple[int, int, int, int, int, str, int, float, float]] = []\n    for res in self.__root__:\n        prefix = (res.rank, res.dp_rank, res.pp_rank, res.tp_rank, res.pp_rank)\n        for freq in res.forward_time:\n            records.append((*prefix, \"forward\", freq, res.forward_time[freq], res.forward_energy[freq]))\n        for freq in res.backward_time:\n            records.append((*prefix, \"backward\", freq, res.backward_time[freq], res.backward_energy[freq]))\n    # fmt: on\n\n    df = pd.DataFrame.from_records(records, columns=headers)\n    df.to_csv(filepath, index=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_prof","title":"save_prof  <code>async</code>","text":"<pre><code>save_prof(data, directory, schedule_num)\n</code></pre> <p>Save a list of <code>ProfilingResult</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_prof(\n    data: list[ProfilingResult],\n    directory: str,\n    schedule_num: int,\n) -&gt; None:\n    \"\"\"Save a list of `ProfilingResult`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/{schedule_num}.prof.json\", \"w\") as f:\n        obj = _ProfilingResultList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_prof","title":"load_prof","text":"<pre><code>load_prof(directory, schedule_num)\n</code></pre> <p>Load a list of <code>ProfilingResult</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_prof(directory: str, schedule_num: int) -&gt; list[ProfilingResult]:\n    \"\"\"Load a list of `ProfilingResult`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/{schedule_num}.prof.json\"\n    return _ProfilingResultList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_sched","title":"save_sched  <code>async</code>","text":"<pre><code>save_sched(data, directory, schedule_num)\n</code></pre> <p>Save a list of <code>FrequencySchedule</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_sched(\n    data: list[FrequencySchedule],\n    directory: str,\n    schedule_num: int,\n) -&gt; None:\n    \"\"\"Save a list of `FrequencySchedule`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/{schedule_num}.sched.json\", \"w\") as f:\n        obj = _FrequencyScheduleList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_sched","title":"load_sched","text":"<pre><code>load_sched(directory, schedule_num)\n</code></pre> <p>Load a list of <code>FrequencySchedule</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_sched(directory: str, schedule_num: int) -&gt; list[FrequencySchedule]:\n    \"\"\"Load a list of `FrequencySchedule`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/{schedule_num}.sched.json\"\n    return _FrequencyScheduleList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_ranks","title":"save_ranks  <code>async</code>","text":"<pre><code>save_ranks(data, directory)\n</code></pre> <p>Save a list of <code>RankInfo</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_ranks(data: list[RankInfo], directory: str) -&gt; None:\n    \"\"\"Save a list of `RankInfo`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/ranks.json\", \"w\") as f:\n        obj = _RankInfoList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_ranks","title":"load_ranks","text":"<pre><code>load_ranks(directory)\n</code></pre> <p>Load a list of <code>RankInfo</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_ranks(directory: str) -&gt; list[RankInfo]:\n    \"\"\"Load a list of `RankInfo`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/ranks.json\"\n    return _RankInfoList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/","title":"frequency_controller","text":""},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller","title":"zeus.optimizer.pipeline_frequency.frequency_controller","text":"<p>Controller that sets the GPU's frequency in a non-blocking fashion.</p>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController","title":"FrequencyController","text":"<p>Spawns a separate process that sets the GPU frequency.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>class FrequencyController:\n    \"\"\"Spawns a separate process that sets the GPU frequency.\"\"\"\n\n    def __init__(self, device_id: int = 0) -&gt; None:\n        \"\"\"Instantiate the frequency controller.\n\n        Args:\n            device_id: Device ID of the GPU to control.\n        \"\"\"\n        self._q: mp.Queue[int | None] = mp.Queue()\n        self._proc = mp.Process(target=self._controller_process, args=(device_id,))\n\n        atexit.register(self.end)\n        self._proc.start()\n\n    def set_frequency(self, frequency: int) -&gt; None:\n        \"\"\"Set the GPU's frequency asynchronously.\n\n        If `frequency` is zero, returns without doing anything.\n        \"\"\"\n        if frequency != 0:\n            self._q.put(frequency, block=False)\n\n    def end(self) -&gt; None:\n        \"\"\"Stop the controller process.\"\"\"\n        self._q.put(None, block=False)\n\n    def _controller_process(self, device_id: int) -&gt; None:\n        \"\"\"Receive frequency values through a queue and apply it.\"\"\"\n        gpus = get_gpus()\n        # Return the power limit to the default.\n        gpus.resetPowerManagementLimit(device_id)\n\n        # Set the memory frequency to be the highest.\n        max_mem_freq = max(gpus.getSupportedMemoryClocks(device_id))\n        with contextlib.suppress(ZeusGPUNotSupportedError):\n            gpus.setMemoryLockedClocks(device_id, max_mem_freq, max_mem_freq)\n\n        # Set the SM frequency to be the highest.\n        max_freq = max(gpus.getSupportedGraphicsClocks(device_id, max_mem_freq))\n        gpus.setGpuLockedClocks(device_id, max_freq, max_freq)\n        current_freq = max_freq\n\n        # Wait on the queue for the next frequency to set.\n        while True:\n            target_freq = self._q.get(block=True)\n            if target_freq is None:\n                break\n            if current_freq != target_freq:\n                gpus.setGpuLockedClocks(device_id, target_freq, target_freq)\n                current_freq = target_freq\n\n        # Reset everything.\n        with contextlib.suppress(ZeusGPUNotSupportedError):\n            gpus.resetMemoryLockedClocks(device_id)\n        gpus.resetGpuLockedClocks(device_id)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.__init__","title":"__init__","text":"<pre><code>__init__(device_id=0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>device_id</code> <code>int</code> <p>Device ID of the GPU to control.</p> <code>0</code> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def __init__(self, device_id: int = 0) -&gt; None:\n    \"\"\"Instantiate the frequency controller.\n\n    Args:\n        device_id: Device ID of the GPU to control.\n    \"\"\"\n    self._q: mp.Queue[int | None] = mp.Queue()\n    self._proc = mp.Process(target=self._controller_process, args=(device_id,))\n\n    atexit.register(self.end)\n    self._proc.start()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.set_frequency","title":"set_frequency","text":"<pre><code>set_frequency(frequency)\n</code></pre> <p>Set the GPU's frequency asynchronously.</p> <p>If <code>frequency</code> is zero, returns without doing anything.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def set_frequency(self, frequency: int) -&gt; None:\n    \"\"\"Set the GPU's frequency asynchronously.\n\n    If `frequency` is zero, returns without doing anything.\n    \"\"\"\n    if frequency != 0:\n        self._q.put(frequency, block=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.end","title":"end","text":"<pre><code>end()\n</code></pre> <p>Stop the controller process.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def end(self) -&gt; None:\n    \"\"\"Stop the controller process.\"\"\"\n    self._q.put(None, block=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController._controller_process","title":"_controller_process","text":"<pre><code>_controller_process(device_id)\n</code></pre> <p>Receive frequency values through a queue and apply it.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def _controller_process(self, device_id: int) -&gt; None:\n    \"\"\"Receive frequency values through a queue and apply it.\"\"\"\n    gpus = get_gpus()\n    # Return the power limit to the default.\n    gpus.resetPowerManagementLimit(device_id)\n\n    # Set the memory frequency to be the highest.\n    max_mem_freq = max(gpus.getSupportedMemoryClocks(device_id))\n    with contextlib.suppress(ZeusGPUNotSupportedError):\n        gpus.setMemoryLockedClocks(device_id, max_mem_freq, max_mem_freq)\n\n    # Set the SM frequency to be the highest.\n    max_freq = max(gpus.getSupportedGraphicsClocks(device_id, max_mem_freq))\n    gpus.setGpuLockedClocks(device_id, max_freq, max_freq)\n    current_freq = max_freq\n\n    # Wait on the queue for the next frequency to set.\n    while True:\n        target_freq = self._q.get(block=True)\n        if target_freq is None:\n            break\n        if current_freq != target_freq:\n            gpus.setGpuLockedClocks(device_id, target_freq, target_freq)\n            current_freq = target_freq\n\n    # Reset everything.\n    with contextlib.suppress(ZeusGPUNotSupportedError):\n        gpus.resetMemoryLockedClocks(device_id)\n    gpus.resetGpuLockedClocks(device_id)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer","title":"zeus.optimizer.pipeline_frequency.optimizer","text":"<p>Pipeline frequency optimizer implementation.</p> <p>The <code>PipelineFrequencyOptimizer</code> is to be integrated into the training framework. It is responsible for communicating with the PFO server and managing the <code>FrequencyController</code> instance, which is responsible for controlling the frequency of the CPU of the current process.</p>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer","title":"PipelineFrequencyOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Pipeline frequency optimizer.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>class PipelineFrequencyOptimizer(Callback):\n    \"\"\"Pipeline frequency optimizer.\"\"\"\n\n    def __init__(\n        self,\n        rank: int,\n        dp_rank: int,\n        pp_rank: int,\n        tp_rank: int,\n        device_id: int,\n        dp_degree: int,\n        pp_degree: int,\n        tp_degree: int,\n        world_size: int,\n        server_url: str,\n        job_metadata: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Pipeline frequency optimizer.\n\n        Assumptions:\n            - `torch.distributed` has been initialized.\n            - `torch.cuda.set_device` has been called with `device_id`.\n                This is needed to broadcast the job ID to all ranks.\n\n        The master process (rank 0) will register the job with the Peresus\n        server and retrieve the job ID of this job. Then, each rank will\n        report itself to the PFO server with the job ID.\n\n        Args:\n            rank: Global rank of the current process.\n            dp_rank: Rank in the data parallel group.\n            pp_rank: Rank in the pipeline parallel group.\n            tp_rank: Rank in the tensor parallel group.\n            device_id: CUDA device ID that the current process manages.\n            dp_degree: Size of the data parallel group.\n            pp_degree: Size of the pipeline parallel group.\n            tp_degree: Size of the tensor parallel group.\n            world_size: Total number of ranks that participate in training.\n            server_url: URL of the PFO server.\n            job_metadata: An optional arbitrary string that describes the job. This will\n                be appended to the job ID if given. Typically for logging purposes.\n        \"\"\"\n        if not dist.is_initialized():\n            raise RuntimeError(\n                \"Instantiate `PipelineFrequencyOptimizer` after `init_process_group`.\"\n            )\n\n        self.server_url = server_url\n        self.rank = rank\n        self.dp_rank = dp_rank\n        self.pp_rank = pp_rank\n        self.tp_rank = tp_rank\n        self.device_id = device_id\n\n        gpus = get_gpus()\n        torch.cuda.set_device(device_id)\n\n        # Rank 0 registers the job with the PFO server and retrieves the job ID.\n        job_id = None\n        if rank == 0:\n            job_info = JobInfo(\n                pp_degree=pp_degree,\n                dp_degree=dp_degree,\n                tp_degree=tp_degree,\n                world_size=world_size,\n                job_metadata=job_metadata,\n            )\n            response = httpx.post(\n                self.server_url + REGISTER_JOB_URL, json=job_info.dict()\n            )\n            if (code := response.status_code) != 200:\n                raise RuntimeError(\n                    f\"PFO server returned status code {code}: {response.text}\"\n                )\n            job_id = response.json()\n            if not isinstance(job_id, str):\n                raise RuntimeError(f\"PFO server returned a strange job ID: {job_id=}\")\n\n        # Rank 0 broadcasts the job ID across all ranks.\n        objects = [job_id]\n        dist.broadcast_object_list(objects, src=0)\n        self.job_id = objects[0]\n        if self.job_id is None:\n            raise RuntimeError(\"Failed to broadcast job ID to all ranks\")\n\n        # Query the list of available frequencies of the GPU.\n        max_mem_freq = max(gpus.getSupportedMemoryClocks(device_id))\n        freqs = sorted(\n            gpus.getSupportedGraphicsClocks(device_id, max_mem_freq),\n            reverse=True,\n        )\n\n        # Each rank reports itself to the PFO server with the job ID.\n        rank_info = RankInfo(\n            rank=self.rank,\n            dp_rank=self.dp_rank,\n            pp_rank=self.pp_rank,\n            tp_rank=self.tp_rank,\n            available_frequencies=freqs,\n        )\n        response = httpx.post(\n            self.server_url + REGISTER_RANK_URL.format(job_id=self.job_id),\n            json=rank_info.dict(),\n        )\n        if (code := response.status_code) != 200:\n            raise RuntimeError(\n                f\"PFO server returned status code {code}: {response.text}\"\n            )\n\n        # The frequency controller is responsible for controlling the frequency\n        # of the GPU (device_id) asynchronously.\n        self.frequency_controller = FrequencyController(device_id=device_id)\n\n        # Fetch the frequency schedule from the PFO server.\n        self.freq_schedule = self._get_frequency_schedule()\n        self.freq_schedule_iter = iter(self.freq_schedule)\n\n    def _get_frequency_schedule(self) -&gt; list[tuple[str, int]]:\n        \"\"\"Get the frequency schedule from the PFO server.\"\"\"\n        response = httpx.get(\n            self.server_url + GET_FREQUENCY_SCHEDULE_URL.format(job_id=self.job_id),\n            params={\"rank\": self.rank},\n            timeout=None,\n        )\n        if (code := response.status_code) != 200:\n            raise RuntimeError(\n                f\"PFO server returned status code {code}: {response.text}\"\n            )\n        schedule = FrequencySchedule.parse_raw(response.text)\n        if schedule.rank != self.rank:\n            raise RuntimeError(\n                f\"PFO server returned a schedule for rank {schedule.rank} to rank {self.rank}\"\n            )\n        return schedule.frequencies\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Mark the beginning of a step.\n\n        TODO(jaywonchung): InstructionProfiler iteration start mark.\n        \"\"\"\n        pass\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Mark the end of a step.\n\n        TODO(jaywonchung): InstructionProfiler iteration end mark.\n        Also report the profiling result to the PFO server after N iterations.\n        \"\"\"\n        # Frequency schedule holds one iteration-worth of frequencies, so at\n        # the end of each iteration, the iterator should be exhausted.\n        item = next(self.freq_schedule_iter, None)\n        if item is not None:\n            raise RuntimeError(\n                \"PFO server returned more frequencies than expected. \"\n                f\"Next expected instruction and frequency is {item}\"\n            )\n        self.freq_schedule_iter = iter(self.freq_schedule)\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Mark the beginning of an instruction, like forward and backward.\n\n        Retrieve the next frequency from the schedule, check whether the next\n        expected instruction matches the name of the instruction, and set the\n        frequency accordingly.\n        \"\"\"\n        cuda_sync(self.device_id)\n\n        # Retrieve the next frequency from the schedule.\n        item = next(self.freq_schedule_iter, None)\n        if item is None:\n            raise RuntimeError(\"PFO server returned fewer frequencies than expected\")\n\n        # Check whether the next expected instruction matches the name of the instruction.\n        instruction, frequency = item\n        if instruction != name:\n            raise RuntimeError(\n                f\"The next expected instruction is not forward: {instruction}\"\n            )\n\n        self.frequency_controller.set_frequency(frequency)\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Mark the end of an instruction, like forward and backward.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    rank,\n    dp_rank,\n    pp_rank,\n    tp_rank,\n    device_id,\n    dp_degree,\n    pp_degree,\n    tp_degree,\n    world_size,\n    server_url,\n    job_metadata=None,\n)\n</code></pre> Assumptions <ul> <li><code>torch.distributed</code> has been initialized.</li> <li><code>torch.cuda.set_device</code> has been called with <code>device_id</code>.     This is needed to broadcast the job ID to all ranks.</li> </ul> <p>The master process (rank 0) will register the job with the Peresus server and retrieve the job ID of this job. Then, each rank will report itself to the PFO server with the job ID.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Global rank of the current process.</p> required <code>dp_rank</code> <code>int</code> <p>Rank in the data parallel group.</p> required <code>pp_rank</code> <code>int</code> <p>Rank in the pipeline parallel group.</p> required <code>tp_rank</code> <code>int</code> <p>Rank in the tensor parallel group.</p> required <code>device_id</code> <code>int</code> <p>CUDA device ID that the current process manages.</p> required <code>dp_degree</code> <code>int</code> <p>Size of the data parallel group.</p> required <code>pp_degree</code> <code>int</code> <p>Size of the pipeline parallel group.</p> required <code>tp_degree</code> <code>int</code> <p>Size of the tensor parallel group.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks that participate in training.</p> required <code>server_url</code> <code>str</code> <p>URL of the PFO server.</p> required <code>job_metadata</code> <code>str | None</code> <p>An optional arbitrary string that describes the job. This will be appended to the job ID if given. Typically for logging purposes.</p> <code>None</code> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def __init__(\n    self,\n    rank: int,\n    dp_rank: int,\n    pp_rank: int,\n    tp_rank: int,\n    device_id: int,\n    dp_degree: int,\n    pp_degree: int,\n    tp_degree: int,\n    world_size: int,\n    server_url: str,\n    job_metadata: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Pipeline frequency optimizer.\n\n    Assumptions:\n        - `torch.distributed` has been initialized.\n        - `torch.cuda.set_device` has been called with `device_id`.\n            This is needed to broadcast the job ID to all ranks.\n\n    The master process (rank 0) will register the job with the Peresus\n    server and retrieve the job ID of this job. Then, each rank will\n    report itself to the PFO server with the job ID.\n\n    Args:\n        rank: Global rank of the current process.\n        dp_rank: Rank in the data parallel group.\n        pp_rank: Rank in the pipeline parallel group.\n        tp_rank: Rank in the tensor parallel group.\n        device_id: CUDA device ID that the current process manages.\n        dp_degree: Size of the data parallel group.\n        pp_degree: Size of the pipeline parallel group.\n        tp_degree: Size of the tensor parallel group.\n        world_size: Total number of ranks that participate in training.\n        server_url: URL of the PFO server.\n        job_metadata: An optional arbitrary string that describes the job. This will\n            be appended to the job ID if given. Typically for logging purposes.\n    \"\"\"\n    if not dist.is_initialized():\n        raise RuntimeError(\n            \"Instantiate `PipelineFrequencyOptimizer` after `init_process_group`.\"\n        )\n\n    self.server_url = server_url\n    self.rank = rank\n    self.dp_rank = dp_rank\n    self.pp_rank = pp_rank\n    self.tp_rank = tp_rank\n    self.device_id = device_id\n\n    gpus = get_gpus()\n    torch.cuda.set_device(device_id)\n\n    # Rank 0 registers the job with the PFO server and retrieves the job ID.\n    job_id = None\n    if rank == 0:\n        job_info = JobInfo(\n            pp_degree=pp_degree,\n            dp_degree=dp_degree,\n            tp_degree=tp_degree,\n            world_size=world_size,\n            job_metadata=job_metadata,\n        )\n        response = httpx.post(\n            self.server_url + REGISTER_JOB_URL, json=job_info.dict()\n        )\n        if (code := response.status_code) != 200:\n            raise RuntimeError(\n                f\"PFO server returned status code {code}: {response.text}\"\n            )\n        job_id = response.json()\n        if not isinstance(job_id, str):\n            raise RuntimeError(f\"PFO server returned a strange job ID: {job_id=}\")\n\n    # Rank 0 broadcasts the job ID across all ranks.\n    objects = [job_id]\n    dist.broadcast_object_list(objects, src=0)\n    self.job_id = objects[0]\n    if self.job_id is None:\n        raise RuntimeError(\"Failed to broadcast job ID to all ranks\")\n\n    # Query the list of available frequencies of the GPU.\n    max_mem_freq = max(gpus.getSupportedMemoryClocks(device_id))\n    freqs = sorted(\n        gpus.getSupportedGraphicsClocks(device_id, max_mem_freq),\n        reverse=True,\n    )\n\n    # Each rank reports itself to the PFO server with the job ID.\n    rank_info = RankInfo(\n        rank=self.rank,\n        dp_rank=self.dp_rank,\n        pp_rank=self.pp_rank,\n        tp_rank=self.tp_rank,\n        available_frequencies=freqs,\n    )\n    response = httpx.post(\n        self.server_url + REGISTER_RANK_URL.format(job_id=self.job_id),\n        json=rank_info.dict(),\n    )\n    if (code := response.status_code) != 200:\n        raise RuntimeError(\n            f\"PFO server returned status code {code}: {response.text}\"\n        )\n\n    # The frequency controller is responsible for controlling the frequency\n    # of the GPU (device_id) asynchronously.\n    self.frequency_controller = FrequencyController(device_id=device_id)\n\n    # Fetch the frequency schedule from the PFO server.\n    self.freq_schedule = self._get_frequency_schedule()\n    self.freq_schedule_iter = iter(self.freq_schedule)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer._get_frequency_schedule","title":"_get_frequency_schedule","text":"<pre><code>_get_frequency_schedule()\n</code></pre> <p>Get the frequency schedule from the PFO server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def _get_frequency_schedule(self) -&gt; list[tuple[str, int]]:\n    \"\"\"Get the frequency schedule from the PFO server.\"\"\"\n    response = httpx.get(\n        self.server_url + GET_FREQUENCY_SCHEDULE_URL.format(job_id=self.job_id),\n        params={\"rank\": self.rank},\n        timeout=None,\n    )\n    if (code := response.status_code) != 200:\n        raise RuntimeError(\n            f\"PFO server returned status code {code}: {response.text}\"\n        )\n    schedule = FrequencySchedule.parse_raw(response.text)\n    if schedule.rank != self.rank:\n        raise RuntimeError(\n            f\"PFO server returned a schedule for rank {schedule.rank} to rank {self.rank}\"\n        )\n    return schedule.frequencies\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Mark the beginning of a step.</p> <p>TODO(jaywonchung): InstructionProfiler iteration start mark.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Mark the beginning of a step.\n\n    TODO(jaywonchung): InstructionProfiler iteration start mark.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Mark the end of a step.</p> <p>TODO(jaywonchung): InstructionProfiler iteration end mark. Also report the profiling result to the PFO server after N iterations.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Mark the end of a step.\n\n    TODO(jaywonchung): InstructionProfiler iteration end mark.\n    Also report the profiling result to the PFO server after N iterations.\n    \"\"\"\n    # Frequency schedule holds one iteration-worth of frequencies, so at\n    # the end of each iteration, the iterator should be exhausted.\n    item = next(self.freq_schedule_iter, None)\n    if item is not None:\n        raise RuntimeError(\n            \"PFO server returned more frequencies than expected. \"\n            f\"Next expected instruction and frequency is {item}\"\n        )\n    self.freq_schedule_iter = iter(self.freq_schedule)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Mark the beginning of an instruction, like forward and backward.</p> <p>Retrieve the next frequency from the schedule, check whether the next expected instruction matches the name of the instruction, and set the frequency accordingly.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Mark the beginning of an instruction, like forward and backward.\n\n    Retrieve the next frequency from the schedule, check whether the next\n    expected instruction matches the name of the instruction, and set the\n    frequency accordingly.\n    \"\"\"\n    cuda_sync(self.device_id)\n\n    # Retrieve the next frequency from the schedule.\n    item = next(self.freq_schedule_iter, None)\n    if item is None:\n        raise RuntimeError(\"PFO server returned fewer frequencies than expected\")\n\n    # Check whether the next expected instruction matches the name of the instruction.\n    instruction, frequency = item\n    if instruction != name:\n        raise RuntimeError(\n            f\"The next expected instruction is not forward: {instruction}\"\n        )\n\n    self.frequency_controller.set_frequency(frequency)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Mark the end of an instruction, like forward and backward.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Mark the end of an instruction, like forward and backward.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/","title":"server","text":""},{"location":"reference/optimizer/pipeline_frequency/server/#zeus.optimizer.pipeline_frequency.server","title":"zeus.optimizer.pipeline_frequency.server","text":"<p>The server guides the <code>PipelineFrequencyOptimizer</code> with frequency plans.</p> <p>The server is agnostic to the training framework the <code>PipelineFrequencyOptimizer</code> is integrated with. A server is useful because large model training is typically distributed, and we still need one place to coordinate the frequency plans. Later, the server will be extended to support complete online profiling and optimization.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/","title":"job_manager","text":""},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager","title":"zeus.optimizer.pipeline_frequency.server.job_manager","text":"<p>The JobManager singleton class manages all job states.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager","title":"JobManager","text":"<p>A singleton class that manages all states.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>class JobManager:\n    \"\"\"A singleton class that manages all states.\"\"\"\n\n    def __init__(self, pfo_settings: PFOServerSettings) -&gt; None:\n        \"\"\"Initialize the job manager.\"\"\"\n        self.pfo_settings = pfo_settings\n\n        self._job_infos: dict[str, JobInfo] = {}\n        self._job_rank_infos: dict[str, list[RankInfo]] = {}\n        self._job_tasks: dict[str, asyncio.Task] = {}\n        self._job_result_channels: dict[str, asyncio.Queue[ProfilingResult]] = {}\n        self._job_sched_request_channels: dict[str, asyncio.Queue] = {}\n        self._job_sched_response_channels: dict[str, list[asyncio.Queue]] = {}\n        self._job_last_active_time: dict[str, float] = {}\n\n        # Spawn cleanup task that evicts the state of jobs that have not been active\n        # for a long time.\n        create_task(\n            self._cleanup_task(\n                cleanup_period=60,\n                max_idle_time=pfo_settings.max_job_idle_time,\n            ),\n            logger=logger,\n        )\n\n    def register_job(self, job_info: JobInfo) -&gt; None:\n        \"\"\"Prepare internal state for a new job.\n\n        This method will be invoked exactly once by the global rank 0 (master) process.\n        \"\"\"\n        job_id = job_info.job_id\n        world_size = job_info.world_size\n        self._job_infos[job_id] = job_info\n        self._job_rank_infos[job_id] = []\n        self._job_result_channels[job_id] = asyncio.Queue(maxsize=world_size)\n        self._job_sched_request_channels[job_id] = asyncio.Queue(maxsize=world_size)\n        self._job_sched_response_channels[job_id] = [\n            asyncio.Queue(maxsize=1) for _ in range(world_size)\n        ]\n        self._job_tasks[job_id] = create_task(\n            self._job_task(job_id, self.pfo_settings.dump_data),\n            logger=logger,\n        )\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    def register_rank(self, job_id: str, rank_info: RankInfo) -&gt; None:\n        \"\"\"Register rank-specific information for an already registered job.\n\n        This method will be invoked `world_size` number of times (once per rank).\n        \"\"\"\n        self._job_rank_infos[job_id].append(rank_info)\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    async def get_frequency_schedule(self, job_id: str, rank: int) -&gt; FrequencySchedule:\n        \"\"\"Get the next frequency schedule for a rank.\n\n        This method will be called `world_size` number of times (once per rank).\n        All ranks will block on this method untill everyone reports their\n        profiling results and calls this method.\n\n        When an internal scheduler error happened at any point of servicing the\n        job, clients will be notified through this API with a 500 Internal Error.\n        \"\"\"\n        await self._job_sched_request_channels[job_id].put(rank)\n        res = await self._job_sched_response_channels[job_id][rank].get()\n        if isinstance(res, Exception):\n            code = 400 if isinstance(res, ValueError) else 500\n            raise HTTPException(\n                status_code=code,\n                detail=\"\".join(\n                    traceback.format_exception(type(res), res, res.__traceback__)\n                ),\n            )\n        self._job_last_active_time[job_id] = time.monotonic()\n        return res\n\n    def report_profiling_result(self, job_id: str, result: ProfilingResult) -&gt; None:\n        \"\"\"Send the profiling result to the job task and immediately return.\n\n        This method will be called `world_size` number of times - one for each rank.\n        \"\"\"\n        self._job_result_channels[job_id].put_nowait(result)\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    async def _cleanup_task(\n        self,\n        cleanup_period: int,\n        max_idle_time: int,\n    ) -&gt; None:\n        \"\"\"Periodically evict job states.\n\n        Args:\n            cleanup_period: How often to run the cleanup task, in seconds.\n            max_idle_time: Maximum amount of time a job can be idle for, in seconds.\n        \"\"\"\n        while True:\n            await asyncio.sleep(cleanup_period)\n            for job_id in list(self._job_last_active_time.keys()):\n                if (\n                    time.monotonic() - self._job_last_active_time[job_id]\n                    &gt; max_idle_time\n                ):\n                    self._job_tasks[job_id].cancel()\n                    del self._job_infos[job_id]\n                    del self._job_rank_infos[job_id]\n                    del self._job_result_channels[job_id]\n                    del self._job_sched_request_channels[job_id]\n                    del self._job_sched_response_channels[job_id]\n                    del self._job_tasks[job_id]\n                    del self._job_last_active_time[job_id]\n\n    async def _job_task(self, job_id: str, dump_data: bool) -&gt; None:\n        \"\"\"Coalese requests and responses of each rank and interface with the scheduler.\"\"\"\n        result_chan = self._job_result_channels[job_id]\n        sched_req_chan = self._job_sched_request_channels[job_id]\n        sched_resp_chan = self._job_sched_response_channels[job_id]\n\n        job_info = self._job_infos[job_id]\n\n        try:\n            # Wait until all ranks have reported their `RankInfo`s.\n            rank_infos = self._job_rank_infos[job_id]\n            while True:\n                await asyncio.sleep(0.1)\n                # Indexing the first element is always safe because this task is\n                # created after putting the `RankInfo` of the first-connected rank\n                # in `self.job_rank_infos[job_id]`.\n                if len(rank_infos) == job_info.world_size:\n                    break\n\n            # Sort `RankInfo`s in rank order.\n            rank_infos.sort(key=lambda r: r.rank)\n\n            # Create directory to dump PFO server states.\n            dump_dir = f\"{self.pfo_settings.dump_dir}/{job_id}\"\n            if dump_data:\n                await save_ranks(rank_infos, dump_dir)\n\n            # Instantiate the frequency scheduler.\n            scheduler = self.pfo_settings.scheduler(\n                job_info,\n                rank_infos,\n                self.pfo_settings,\n                **self.pfo_settings.scheduler_args,\n            )\n\n            # Provide next schedules, observe profiling results, and repeat.\n            schedule_num = 0\n            while True:\n                # Compute the next `FrequencySchedule`s.\n                schedules = scheduler.next_schedule()\n\n                # Wait until all the ranks ask for the next schedule.\n                await asyncio.gather(*[sched_req_chan.get() for _ in rank_infos])\n\n                # Send out `FrequencySchedule`s.\n                await asyncio.gather(\n                    *[sched_resp_chan[s.rank].put(s) for s in schedules]\n                )\n\n                # Gather profiling results from all ranks.\n                results = await asyncio.gather(*[result_chan.get() for _ in rank_infos])\n                results.sort(key=lambda r: r.rank)\n\n                # Dump profiling results and schedules.\n                if dump_data:\n                    schedules.sort(key=lambda s: s.rank)\n                    await save_prof(results, dump_dir, schedule_num)\n                    await save_sched(schedules, dump_dir, schedule_num)\n\n                # Send `ProfilingResult`s to the scheduler.\n                scheduler.observe(results)\n\n                # Increment schedule number.\n                schedule_num += 1\n\n        except asyncio.CancelledError:\n            # This task gets cancelled when it's idle for too long and evicted.\n            pass\n\n        except Exception as exc:\n            # In case the scheduler errored, send out the exception to the clients.\n            # The clients will receive the error when they ask for the next schedule.\n            for chan in sched_resp_chan:\n                chan.put_nowait(exc)\n            raise\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.__init__","title":"__init__","text":"<pre><code>__init__(pfo_settings)\n</code></pre> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def __init__(self, pfo_settings: PFOServerSettings) -&gt; None:\n    \"\"\"Initialize the job manager.\"\"\"\n    self.pfo_settings = pfo_settings\n\n    self._job_infos: dict[str, JobInfo] = {}\n    self._job_rank_infos: dict[str, list[RankInfo]] = {}\n    self._job_tasks: dict[str, asyncio.Task] = {}\n    self._job_result_channels: dict[str, asyncio.Queue[ProfilingResult]] = {}\n    self._job_sched_request_channels: dict[str, asyncio.Queue] = {}\n    self._job_sched_response_channels: dict[str, list[asyncio.Queue]] = {}\n    self._job_last_active_time: dict[str, float] = {}\n\n    # Spawn cleanup task that evicts the state of jobs that have not been active\n    # for a long time.\n    create_task(\n        self._cleanup_task(\n            cleanup_period=60,\n            max_idle_time=pfo_settings.max_job_idle_time,\n        ),\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.register_job","title":"register_job","text":"<pre><code>register_job(job_info)\n</code></pre> <p>Prepare internal state for a new job.</p> <p>This method will be invoked exactly once by the global rank 0 (master) process.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def register_job(self, job_info: JobInfo) -&gt; None:\n    \"\"\"Prepare internal state for a new job.\n\n    This method will be invoked exactly once by the global rank 0 (master) process.\n    \"\"\"\n    job_id = job_info.job_id\n    world_size = job_info.world_size\n    self._job_infos[job_id] = job_info\n    self._job_rank_infos[job_id] = []\n    self._job_result_channels[job_id] = asyncio.Queue(maxsize=world_size)\n    self._job_sched_request_channels[job_id] = asyncio.Queue(maxsize=world_size)\n    self._job_sched_response_channels[job_id] = [\n        asyncio.Queue(maxsize=1) for _ in range(world_size)\n    ]\n    self._job_tasks[job_id] = create_task(\n        self._job_task(job_id, self.pfo_settings.dump_data),\n        logger=logger,\n    )\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.register_rank","title":"register_rank","text":"<pre><code>register_rank(job_id, rank_info)\n</code></pre> <p>Register rank-specific information for an already registered job.</p> <p>This method will be invoked <code>world_size</code> number of times (once per rank).</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def register_rank(self, job_id: str, rank_info: RankInfo) -&gt; None:\n    \"\"\"Register rank-specific information for an already registered job.\n\n    This method will be invoked `world_size` number of times (once per rank).\n    \"\"\"\n    self._job_rank_infos[job_id].append(rank_info)\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.get_frequency_schedule","title":"get_frequency_schedule  <code>async</code>","text":"<pre><code>get_frequency_schedule(job_id, rank)\n</code></pre> <p>Get the next frequency schedule for a rank.</p> <p>This method will be called <code>world_size</code> number of times (once per rank). All ranks will block on this method untill everyone reports their profiling results and calls this method.</p> <p>When an internal scheduler error happened at any point of servicing the job, clients will be notified through this API with a 500 Internal Error.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def get_frequency_schedule(self, job_id: str, rank: int) -&gt; FrequencySchedule:\n    \"\"\"Get the next frequency schedule for a rank.\n\n    This method will be called `world_size` number of times (once per rank).\n    All ranks will block on this method untill everyone reports their\n    profiling results and calls this method.\n\n    When an internal scheduler error happened at any point of servicing the\n    job, clients will be notified through this API with a 500 Internal Error.\n    \"\"\"\n    await self._job_sched_request_channels[job_id].put(rank)\n    res = await self._job_sched_response_channels[job_id][rank].get()\n    if isinstance(res, Exception):\n        code = 400 if isinstance(res, ValueError) else 500\n        raise HTTPException(\n            status_code=code,\n            detail=\"\".join(\n                traceback.format_exception(type(res), res, res.__traceback__)\n            ),\n        )\n    self._job_last_active_time[job_id] = time.monotonic()\n    return res\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.report_profiling_result","title":"report_profiling_result","text":"<pre><code>report_profiling_result(job_id, result)\n</code></pre> <p>Send the profiling result to the job task and immediately return.</p> <p>This method will be called <code>world_size</code> number of times - one for each rank.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def report_profiling_result(self, job_id: str, result: ProfilingResult) -&gt; None:\n    \"\"\"Send the profiling result to the job task and immediately return.\n\n    This method will be called `world_size` number of times - one for each rank.\n    \"\"\"\n    self._job_result_channels[job_id].put_nowait(result)\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager._cleanup_task","title":"_cleanup_task  <code>async</code>","text":"<pre><code>_cleanup_task(cleanup_period, max_idle_time)\n</code></pre> <p>Periodically evict job states.</p> <p>Parameters:</p> Name Type Description Default <code>cleanup_period</code> <code>int</code> <p>How often to run the cleanup task, in seconds.</p> required <code>max_idle_time</code> <code>int</code> <p>Maximum amount of time a job can be idle for, in seconds.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def _cleanup_task(\n    self,\n    cleanup_period: int,\n    max_idle_time: int,\n) -&gt; None:\n    \"\"\"Periodically evict job states.\n\n    Args:\n        cleanup_period: How often to run the cleanup task, in seconds.\n        max_idle_time: Maximum amount of time a job can be idle for, in seconds.\n    \"\"\"\n    while True:\n        await asyncio.sleep(cleanup_period)\n        for job_id in list(self._job_last_active_time.keys()):\n            if (\n                time.monotonic() - self._job_last_active_time[job_id]\n                &gt; max_idle_time\n            ):\n                self._job_tasks[job_id].cancel()\n                del self._job_infos[job_id]\n                del self._job_rank_infos[job_id]\n                del self._job_result_channels[job_id]\n                del self._job_sched_request_channels[job_id]\n                del self._job_sched_response_channels[job_id]\n                del self._job_tasks[job_id]\n                del self._job_last_active_time[job_id]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager._job_task","title":"_job_task  <code>async</code>","text":"<pre><code>_job_task(job_id, dump_data)\n</code></pre> <p>Coalese requests and responses of each rank and interface with the scheduler.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def _job_task(self, job_id: str, dump_data: bool) -&gt; None:\n    \"\"\"Coalese requests and responses of each rank and interface with the scheduler.\"\"\"\n    result_chan = self._job_result_channels[job_id]\n    sched_req_chan = self._job_sched_request_channels[job_id]\n    sched_resp_chan = self._job_sched_response_channels[job_id]\n\n    job_info = self._job_infos[job_id]\n\n    try:\n        # Wait until all ranks have reported their `RankInfo`s.\n        rank_infos = self._job_rank_infos[job_id]\n        while True:\n            await asyncio.sleep(0.1)\n            # Indexing the first element is always safe because this task is\n            # created after putting the `RankInfo` of the first-connected rank\n            # in `self.job_rank_infos[job_id]`.\n            if len(rank_infos) == job_info.world_size:\n                break\n\n        # Sort `RankInfo`s in rank order.\n        rank_infos.sort(key=lambda r: r.rank)\n\n        # Create directory to dump PFO server states.\n        dump_dir = f\"{self.pfo_settings.dump_dir}/{job_id}\"\n        if dump_data:\n            await save_ranks(rank_infos, dump_dir)\n\n        # Instantiate the frequency scheduler.\n        scheduler = self.pfo_settings.scheduler(\n            job_info,\n            rank_infos,\n            self.pfo_settings,\n            **self.pfo_settings.scheduler_args,\n        )\n\n        # Provide next schedules, observe profiling results, and repeat.\n        schedule_num = 0\n        while True:\n            # Compute the next `FrequencySchedule`s.\n            schedules = scheduler.next_schedule()\n\n            # Wait until all the ranks ask for the next schedule.\n            await asyncio.gather(*[sched_req_chan.get() for _ in rank_infos])\n\n            # Send out `FrequencySchedule`s.\n            await asyncio.gather(\n                *[sched_resp_chan[s.rank].put(s) for s in schedules]\n            )\n\n            # Gather profiling results from all ranks.\n            results = await asyncio.gather(*[result_chan.get() for _ in rank_infos])\n            results.sort(key=lambda r: r.rank)\n\n            # Dump profiling results and schedules.\n            if dump_data:\n                schedules.sort(key=lambda s: s.rank)\n                await save_prof(results, dump_dir, schedule_num)\n                await save_sched(schedules, dump_dir, schedule_num)\n\n            # Send `ProfilingResult`s to the scheduler.\n            scheduler.observe(results)\n\n            # Increment schedule number.\n            schedule_num += 1\n\n    except asyncio.CancelledError:\n        # This task gets cancelled when it's idle for too long and evicted.\n        pass\n\n    except Exception as exc:\n        # In case the scheduler errored, send out the exception to the clients.\n        # The clients will receive the error when they ask for the next schedule.\n        for chan in sched_resp_chan:\n            chan.put_nowait(exc)\n        raise\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.init_global_job_manager","title":"init_global_job_manager","text":"<pre><code>init_global_job_manager(pfo_settings)\n</code></pre> <p>Instantiate the global singleton <code>JobManager</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def init_global_job_manager(pfo_settings: PFOServerSettings) -&gt; None:\n    \"\"\"Instantiate the global singleton `JobManager`.\"\"\"\n    global GLOBAL_JOB_MANAGER\n    GLOBAL_JOB_MANAGER = JobManager(pfo_settings=pfo_settings)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.get_global_job_manager","title":"get_global_job_manager","text":"<pre><code>get_global_job_manager()\n</code></pre> <p>Fetch the global singleton <code>JobManager</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def get_global_job_manager() -&gt; JobManager:\n    \"\"\"Fetch the global singleton `JobManager`.\"\"\"\n    assert GLOBAL_JOB_MANAGER is not None, \"`init_global_job_manager` was not called.\"\n    return GLOBAL_JOB_MANAGER\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/","title":"router","text":""},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router","title":"zeus.optimizer.pipeline_frequency.server.router","text":"<p>Pipeline frequency optimizer server FastAPI router.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.LoggingRoute","title":"LoggingRoute","text":"<p>               Bases: <code>APIRoute</code></p> <p>Route handler that logs out all requests and responses in DEBUG level.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>class LoggingRoute(APIRoute):\n    \"\"\"Route handler that logs out all requests and responses in DEBUG level.\"\"\"\n\n    def get_route_handler(self) -&gt; Callable:\n        \"\"\"Wrap the original handler with debug messages.\"\"\"\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -&gt; Response:\n            response: Response = await original_route_handler(request)\n            logger.debug(\n                \"%s %s: %s -&gt; %s\",\n                request.method,\n                request.url,\n                await request.json() if await request.body() else \"None\",\n                response.body.decode(response.charset),\n            )\n            return response\n\n        return custom_route_handler\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.LoggingRoute.get_route_handler","title":"get_route_handler","text":"<pre><code>get_route_handler()\n</code></pre> <p>Wrap the original handler with debug messages.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>def get_route_handler(self) -&gt; Callable:\n    \"\"\"Wrap the original handler with debug messages.\"\"\"\n    original_route_handler = super().get_route_handler()\n\n    async def custom_route_handler(request: Request) -&gt; Response:\n        response: Response = await original_route_handler(request)\n        logger.debug(\n            \"%s %s: %s -&gt; %s\",\n            request.method,\n            request.url,\n            await request.json() if await request.body() else \"None\",\n            response.body.decode(response.charset),\n        )\n        return response\n\n    return custom_route_handler\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.startup_hook","title":"startup_hook  <code>async</code>","text":"<pre><code>startup_hook()\n</code></pre> <p>Startup hook.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.on_event(\"startup\")\nasync def startup_hook():\n    \"\"\"Startup hook.\"\"\"\n    logger.info(\"Using scheduler `%s`\", settings.scheduler.__name__)\n    init_global_job_manager(settings)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(\n    job_info, job_manager=Depends(get_global_job_manager)\n)\n</code></pre> <p>Register the training job's information in the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REGISTER_JOB_URL, response_model=str)\nasync def register_job(\n    job_info: JobInfo, job_manager: JobManager = Depends(get_global_job_manager)\n) -&gt; str:\n    \"\"\"Register the training job's information in the server.\"\"\"\n    job_info.set_job_id(scheduler_name=settings.scheduler.__name__)\n    job_manager.register_job(job_info)\n    return job_info.job_id\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.register_rank","title":"register_rank  <code>async</code>","text":"<pre><code>register_rank(\n    job_id,\n    rank_info,\n    job_manager=Depends(get_global_job_manager),\n)\n</code></pre> <p>Register each rank's information in the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REGISTER_RANK_URL)\nasync def register_rank(\n    job_id: str,\n    rank_info: RankInfo,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; None:\n    \"\"\"Register each rank's information in the server.\"\"\"\n    job_manager.register_rank(job_id, rank_info)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.get_frequency_schedule","title":"get_frequency_schedule  <code>async</code>","text":"<pre><code>get_frequency_schedule(\n    job_id,\n    rank,\n    job_manager=Depends(get_global_job_manager),\n)\n</code></pre> <p>Return the next frequency schedule for the rank.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.get(GET_FREQUENCY_SCHEDULE_URL, response_model=FrequencySchedule)\nasync def get_frequency_schedule(\n    job_id: str,\n    rank: int,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; FrequencySchedule:\n    \"\"\"Return the next frequency schedule for the rank.\"\"\"\n    return await job_manager.get_frequency_schedule(job_id, rank)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.report_profiling_result","title":"report_profiling_result  <code>async</code>","text":"<pre><code>report_profiling_result(\n    job_id,\n    profiling_result,\n    job_manager=Depends(get_global_job_manager),\n)\n</code></pre> <p>Report the profiling result for the most recent frequency schedule.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REPORT_PROFILING_RESULT_URL)\nasync def report_profiling_result(\n    job_id: str,\n    profiling_result: ProfilingResult,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; None:\n    \"\"\"Report the profiling result for the most recent frequency schedule.\"\"\"\n    job_manager.report_profiling_result(job_id, profiling_result)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/","title":"scheduler","text":""},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler","title":"zeus.optimizer.pipeline_frequency.server.scheduler","text":"<p>Interfaces for defining frequency schedulers.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler","title":"FrequencyScheduler","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for classes that enclose frequency scheduling policies.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>class FrequencyScheduler(ABC):\n    \"\"\"Interface for classes that enclose frequency scheduling policies.\"\"\"\n\n    def __init__(\n        self,\n        job_info: JobInfo,\n        rank_infos: list[RankInfo],\n        pfo_settings: PFOServerSettings,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            job_info: Info about the training job.\n            rank_infos: Info about all ranks. May not be sorted in rank order.\n            pfo_settings: PFOServerSettings object.\n        \"\"\"\n        self.job_info = job_info\n        self.rank_infos = sorted(rank_infos, key=lambda info: info.rank)\n        self.world_size = self.job_info.world_size\n        self.pfo_settings = pfo_settings\n\n        self._generator = self._run()\n        self._next_schedule: list[FrequencySchedule] | None = None\n\n    def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n        \"\"\"Ingest the profiling results for the previous schedule.\n\n        Args:\n            profiling_results: Doesn't have to be sorted in rank order.\n        \"\"\"\n        # When there are no more schedules left to yield, the generator will\n        # raise `StopIteration`. We just ignore this, and later invocations of\n        # `next_schedule()` will return the last schedule returned forever.\n        with suppress(StopIteration):\n            self._next_schedule = self._generator.send(profiling_results)\n\n    def next_schedule(self) -&gt; list[FrequencySchedule]:\n        \"\"\"Return the schedules for the next round of iterations.\n\n        Returns:\n            A list of `FrequencySchedule`s. May not be sorted in rank order.\n        \"\"\"\n        if self._next_schedule is None:\n            try:\n                self._next_schedule = next(self._generator)\n            except StopIteration as exc:\n                raise RuntimeError(\n                    \"The _run generator raised StopIteration on its first next call.\",\n                ) from exc\n        return self._next_schedule\n\n    @abstractmethod\n    def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n        \"\"\"Yield next schedules and receives profiling results in one place.\n\n        This is an alternative way to write a frequency scheduler. The advantage is\n        that everything is enclosed inside this method. The downside is that you'll\n        have to read this and understand how this generator works.\n\n        The following implementation is a simple example of writing a scheduler using\n        this class. `yield` the next frequency schedule, and receive the profiling\n        results corresponding to that schedule from the `yield`. `observe` and\n        `next_schedule` will run the generator for you.\n\n        In general, this generator should be designed to `yield` schedules infinitely.\n        However, if this was written to write a finite number of next schedules and\n        raise `StopIteration`, the last schedule cached inside `self._next_schedule`\n        will infinitely be returned from the call to `next_schedule`. This can be\n        useful when you converge to the optimal schedule and stop the generator, and\n        the rest of training will run with the final optimal schedule indefinitely.\n        \"\"\"\n        # This is an example implementation.\n        while True:\n            # Generate the next frequency schedule\n            next_schedule: list[FrequencySchedule] = []\n            # Send the next schedule to client and receive the profiling result from client\n            profiling_results = yield next_schedule\n            # Ingest the profiling result\n            logger.debug(\"%s\", profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.__init__","title":"__init__","text":"<pre><code>__init__(job_info, rank_infos, pfo_settings)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>job_info</code> <code>JobInfo</code> <p>Info about the training job.</p> required <code>rank_infos</code> <code>list[RankInfo]</code> <p>Info about all ranks. May not be sorted in rank order.</p> required <code>pfo_settings</code> <code>PFOServerSettings</code> <p>PFOServerSettings object.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def __init__(\n    self,\n    job_info: JobInfo,\n    rank_infos: list[RankInfo],\n    pfo_settings: PFOServerSettings,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        job_info: Info about the training job.\n        rank_infos: Info about all ranks. May not be sorted in rank order.\n        pfo_settings: PFOServerSettings object.\n    \"\"\"\n    self.job_info = job_info\n    self.rank_infos = sorted(rank_infos, key=lambda info: info.rank)\n    self.world_size = self.job_info.world_size\n    self.pfo_settings = pfo_settings\n\n    self._generator = self._run()\n    self._next_schedule: list[FrequencySchedule] | None = None\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.observe","title":"observe","text":"<pre><code>observe(profiling_results)\n</code></pre> <p>Ingest the profiling results for the previous schedule.</p> <p>Parameters:</p> Name Type Description Default <code>profiling_results</code> <code>list[ProfilingResult]</code> <p>Doesn't have to be sorted in rank order.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n    \"\"\"Ingest the profiling results for the previous schedule.\n\n    Args:\n        profiling_results: Doesn't have to be sorted in rank order.\n    \"\"\"\n    # When there are no more schedules left to yield, the generator will\n    # raise `StopIteration`. We just ignore this, and later invocations of\n    # `next_schedule()` will return the last schedule returned forever.\n    with suppress(StopIteration):\n        self._next_schedule = self._generator.send(profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.next_schedule","title":"next_schedule","text":"<pre><code>next_schedule()\n</code></pre> <p>Return the schedules for the next round of iterations.</p> <p>Returns:</p> Type Description <code>list[FrequencySchedule]</code> <p>A list of <code>FrequencySchedule</code>s. May not be sorted in rank order.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def next_schedule(self) -&gt; list[FrequencySchedule]:\n    \"\"\"Return the schedules for the next round of iterations.\n\n    Returns:\n        A list of `FrequencySchedule`s. May not be sorted in rank order.\n    \"\"\"\n    if self._next_schedule is None:\n        try:\n            self._next_schedule = next(self._generator)\n        except StopIteration as exc:\n            raise RuntimeError(\n                \"The _run generator raised StopIteration on its first next call.\",\n            ) from exc\n    return self._next_schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler._run","title":"_run  <code>abstractmethod</code>","text":"<pre><code>_run()\n</code></pre> <p>Yield next schedules and receives profiling results in one place.</p> <p>This is an alternative way to write a frequency scheduler. The advantage is that everything is enclosed inside this method. The downside is that you'll have to read this and understand how this generator works.</p> <p>The following implementation is a simple example of writing a scheduler using this class. <code>yield</code> the next frequency schedule, and receive the profiling results corresponding to that schedule from the <code>yield</code>. <code>observe</code> and <code>next_schedule</code> will run the generator for you.</p> <p>In general, this generator should be designed to <code>yield</code> schedules infinitely. However, if this was written to write a finite number of next schedules and raise <code>StopIteration</code>, the last schedule cached inside <code>self._next_schedule</code> will infinitely be returned from the call to <code>next_schedule</code>. This can be useful when you converge to the optimal schedule and stop the generator, and the rest of training will run with the final optimal schedule indefinitely.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>@abstractmethod\ndef _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n    \"\"\"Yield next schedules and receives profiling results in one place.\n\n    This is an alternative way to write a frequency scheduler. The advantage is\n    that everything is enclosed inside this method. The downside is that you'll\n    have to read this and understand how this generator works.\n\n    The following implementation is a simple example of writing a scheduler using\n    this class. `yield` the next frequency schedule, and receive the profiling\n    results corresponding to that schedule from the `yield`. `observe` and\n    `next_schedule` will run the generator for you.\n\n    In general, this generator should be designed to `yield` schedules infinitely.\n    However, if this was written to write a finite number of next schedules and\n    raise `StopIteration`, the last schedule cached inside `self._next_schedule`\n    will infinitely be returned from the call to `next_schedule`. This can be\n    useful when you converge to the optimal schedule and stop the generator, and\n    the rest of training will run with the final optimal schedule indefinitely.\n    \"\"\"\n    # This is an example implementation.\n    while True:\n        # Generate the next frequency schedule\n        next_schedule: list[FrequencySchedule] = []\n        # Send the next schedule to client and receive the profiling result from client\n        profiling_results = yield next_schedule\n        # Ingest the profiling result\n        logger.debug(\"%s\", profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution","title":"PointSolution","text":"<p>               Bases: <code>FrequencyScheduler</code></p> <p>Runs the given frequency schedule.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>class PointSolution(FrequencyScheduler):\n    \"\"\"Runs the given frequency schedule.\"\"\"\n\n    def __init__(\n        self,\n        job_info: JobInfo,\n        rank_infos: list[RankInfo],\n        pfo_settings: PFOServerSettings,\n        solution_path: str,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            job_info: Info about the training job.\n            rank_infos: Info about all ranks. May not be sorted in rank order.\n            pfo_settings: PFOServerSettings object.\n            solution_path: Path to the frequency Python file generated by lowtime.\n        \"\"\"\n        super().__init__(job_info, rank_infos, pfo_settings)\n\n        self.solution_path = Path(solution_path)\n        if not self.solution_path.is_file():\n            raise RuntimeError(f\"Solution file not found: {solution_path}\")\n        if self.solution_path.suffix != \".py\":\n            raise RuntimeError(f\"Solution file is not a Python file: {solution_path}\")\n\n        with open(self.solution_path, encoding=\"utf-8\") as f:\n            schedule: list[list[tuple[str, int]]] = eval(f.read())\n            if len(schedule) != self.world_size:\n                raise RuntimeError(\n                    f\"Solution file assumes {len(schedule)} ranks, but \"\n                    f\"the job has {self.world_size} ranks.\"\n                )\n\n            self.schedule = []\n            for rank, freqs in enumerate(schedule):\n                self.schedule.append(FrequencySchedule(rank=rank, frequencies=freqs))\n\n    def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n        \"\"\"Yield the schedule given by the solution path.\"\"\"\n        yield self.schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution.__init__","title":"__init__","text":"<pre><code>__init__(job_info, rank_infos, pfo_settings, solution_path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>job_info</code> <code>JobInfo</code> <p>Info about the training job.</p> required <code>rank_infos</code> <code>list[RankInfo]</code> <p>Info about all ranks. May not be sorted in rank order.</p> required <code>pfo_settings</code> <code>PFOServerSettings</code> <p>PFOServerSettings object.</p> required <code>solution_path</code> <code>str</code> <p>Path to the frequency Python file generated by lowtime.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def __init__(\n    self,\n    job_info: JobInfo,\n    rank_infos: list[RankInfo],\n    pfo_settings: PFOServerSettings,\n    solution_path: str,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        job_info: Info about the training job.\n        rank_infos: Info about all ranks. May not be sorted in rank order.\n        pfo_settings: PFOServerSettings object.\n        solution_path: Path to the frequency Python file generated by lowtime.\n    \"\"\"\n    super().__init__(job_info, rank_infos, pfo_settings)\n\n    self.solution_path = Path(solution_path)\n    if not self.solution_path.is_file():\n        raise RuntimeError(f\"Solution file not found: {solution_path}\")\n    if self.solution_path.suffix != \".py\":\n        raise RuntimeError(f\"Solution file is not a Python file: {solution_path}\")\n\n    with open(self.solution_path, encoding=\"utf-8\") as f:\n        schedule: list[list[tuple[str, int]]] = eval(f.read())\n        if len(schedule) != self.world_size:\n            raise RuntimeError(\n                f\"Solution file assumes {len(schedule)} ranks, but \"\n                f\"the job has {self.world_size} ranks.\"\n            )\n\n        self.schedule = []\n        for rank, freqs in enumerate(schedule):\n            self.schedule.append(FrequencySchedule(rank=rank, frequencies=freqs))\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution._run","title":"_run","text":"<pre><code>_run()\n</code></pre> <p>Yield the schedule given by the solution path.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n    \"\"\"Yield the schedule given by the solution path.\"\"\"\n    yield self.schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.make_3d_parallel","title":"make_3d_parallel","text":"<pre><code>make_3d_parallel(sched_cls, name=None)\n</code></pre> <p>Wrap <code>sched_cls</code> so that it is aware of 3D parallelism.</p> <p>Internally, this function subclasses <code>sched_cls</code> and overrides <code>observe</code> and <code>next_schedule</code>. <code>observe</code> will aggregate the profiling results from all ranks that share the same pp_rank and feed it to <code>super().observe</code>, while <code>next_schedule</code> will first retrieve the per-stage schedule from <code>super().next_schedule</code> and then copy-paste it to all ranks that share the same pp_rank. With this, the wrapped scheduler can operate under the illusion that it's only deadling with pure pipeline parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>sched_cls</code> <code>Type[FrequencyScheduler]</code> <p>The scheduler class to wrap.</p> required <code>name</code> <code>str | None</code> <p>Name of the scheduler. If None, use <code>sched_cls.__name__ + \"3D\"</code>.</p> <code>None</code> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def make_3d_parallel(\n    sched_cls: Type[FrequencyScheduler], name: str | None = None\n) -&gt; Type[FrequencyScheduler]:\n    \"\"\"Wrap `sched_cls` so that it is aware of 3D parallelism.\n\n    Internally, this function subclasses `sched_cls` and overrides `observe` and\n    `next_schedule`. `observe` will aggregate the profiling results from all ranks\n    that share the same pp_rank and feed it to `super().observe`, while `next_schedule`\n    will first retrieve the per-stage schedule from `super().next_schedule` and then\n    copy-paste it to all ranks that share the same pp_rank. With this, the wrapped\n    scheduler can operate under the illusion that it's only deadling with pure pipeline\n    parallelism.\n\n    Args:\n        sched_cls: The scheduler class to wrap.\n        name: Name of the scheduler. If None, use `sched_cls.__name__ + \"3D\"`.\n    \"\"\"\n\n    class Wrapper(sched_cls):  # type: ignore[valid-type,misc]\n        def __init__(\n            self,\n            job_info: JobInfo,\n            rank_infos: list[RankInfo],\n            pfo_settings: PFOServerSettings,\n            *args,\n            **kwargs,\n        ) -&gt; None:\n            self._orig_job_info = job_info\n            self._orig_rank_infos = rank_infos\n\n            # Give the wrapped scheduler a perfect illusion of pure pipeline parallelism\n            # and no data or tensor parallelism. New rank is given by pp_rank.\n            job_info = copy.deepcopy(job_info)\n            job_info.dp_degree = 1\n            job_info.tp_degree = 1\n            job_info.world_size = job_info.pp_degree\n\n            new_rank_infos = []\n            for rank_info in rank_infos:\n                if rank_info.dp_rank == 0 and rank_info.tp_rank == 0:\n                    new_rank_info = copy.deepcopy(rank_info)\n                    new_rank_info.rank = rank_info.pp_rank\n                    new_rank_infos.append(new_rank_info)\n\n            super().__init__(job_info, rank_infos, pfo_settings, *args, **kwargs)\n\n        def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n            \"\"\"Aggregate results so that each pipeline stage has one result.\"\"\"\n            # Aggregate results from ranks that share the same pp_rank.\n            rank_to_pp_rank = {\n                rank_info.rank: rank_info.pp_rank for rank_info in self._orig_rank_infos\n            }\n            pp_results: list[list[ProfilingResult]] = [\n                [] for _ in range(self._orig_job_info.pp_degree)\n            ]\n            for result in profiling_results:\n                pp_results[rank_to_pp_rank[result.rank]].append(result)\n\n            # For each stage, construct a new ProfilingResult that aggregates all ranks.\n            # For iter_time and values in time_breakdown, take the max.\n            # For iter_energy and values in energy_breakdown, take the sum.\n            def agg_list(values: Sequence[list[float]], fun: Callable) -&gt; list[float]:\n                return [fun(vals) for vals in zip(*values)]\n\n            def agg_list_of_list(\n                values: Sequence[list[list[float]]], fun: Callable\n            ) -&gt; list[list[float]]:\n                return [agg_list(vals, fun) for vals in zip(*values)]\n\n            agg_results = []\n            for pp_rank, results in enumerate(pp_results):\n                agg_result = ProfilingResult(\n                    rank=pp_rank,\n                    iter_time=agg_list([result.iter_time for result in results], max),\n                    iter_energy=agg_list(\n                        [result.iter_energy for result in results], sum\n                    ),\n                    time_breakdown={\n                        key: agg_list_of_list(\n                            [result.time_breakdown[key] for result in results], max\n                        )\n                        for key in results[0].time_breakdown\n                    },\n                    energy_breakdown={\n                        key: agg_list_of_list(\n                            [result.energy_breakdown[key] for result in results], sum\n                        )\n                        for key in results[0].energy_breakdown\n                    },\n                )\n                agg_results.append(agg_result)\n                logger.debug(\n                    \"Aggregated rank %s results for pp_rank %d: %s\",\n                    \", \".join([str(r.rank) for r in results]),\n                    pp_rank,\n                    agg_result,\n                )\n\n            # Finally, let the wrapped scheduler observe the aggregated results.\n            super().observe(agg_results)\n\n        def next_schedule(self) -&gt; list[FrequencySchedule]:\n            \"\"\"Copy and paste the schedule for each stage to all ranks in that stage.\"\"\"\n            # Retrive the next schedule for each stage.\n            schedules = super().next_schedule()\n\n            # Copy and paste the schedule for each stage to all ranks in that stage.\n            rank_to_pp_rank = {\n                rank_info.rank: rank_info.pp_rank for rank_info in self._orig_rank_infos\n            }\n            next_schedule = []\n            for rank in range(self._orig_job_info.world_size):\n                pp_rank = rank_to_pp_rank[rank]\n                sched = copy.deepcopy(schedules[pp_rank])\n                sched.rank = rank\n                next_schedule.append(sched)\n                logger.debug(\n                    \"Copied schedule for pp_rank %d to rank %d: %s\",\n                    pp_rank,\n                    rank,\n                    sched,\n                )\n            return next_schedule\n\n    Wrapper.__name__ = name or (sched_cls.__name__ + \"3D\")\n    if sched_cls.__doc__ is not None:\n        Wrapper.__doc__ = \"[Wrapped for 3D parallelism] \" + sched_cls.__doc__\n\n    return Wrapper\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#zeus.utils","title":"zeus.utils","text":"<p>Utility functions and classes.</p>"},{"location":"reference/utils/async_utils/","title":"async_utils","text":""},{"location":"reference/utils/async_utils/#zeus.utils.async_utils","title":"zeus.utils.async_utils","text":"<p>Utilities for asyncio.</p>"},{"location":"reference/utils/async_utils/#zeus.utils.async_utils.create_task","title":"create_task","text":"<pre><code>create_task(coroutine, logger=None)\n</code></pre> <p>Create an <code>asyncio.Task</code> but ensure that exceptions are logged.</p> <p>Reference: https://quantlane.com/blog/ensure-asyncio-task-exceptions-get-logged/</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to be wrapped.</p> required <code>logger</code> <code>Logger | None</code> <p>The logger to be used for logging exceptions. If <code>None</code>, the the logger with the name <code>zeus.utils.async_utils</code> is used.</p> <code>None</code> Source code in <code>zeus/utils/async_utils.py</code> <pre><code>def create_task(\n    coroutine: Coroutine[Any, Any, T],\n    logger: logging.Logger | None = None,\n) -&gt; asyncio.Task[T]:\n    \"\"\"Create an `asyncio.Task` but ensure that exceptions are logged.\n\n    Reference: https://quantlane.com/blog/ensure-asyncio-task-exceptions-get-logged/\n\n    Args:\n        coroutine: The coroutine to be wrapped.\n        logger: The logger to be used for logging exceptions. If `None`, the\n            the logger with the name `zeus.utils.async_utils` is used.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    task = loop.create_task(coroutine)\n    task.add_done_callback(\n        functools.partial(_handle_task_exception, logger=logger or default_logger)\n    )\n    return task\n</code></pre>"},{"location":"reference/utils/async_utils/#zeus.utils.async_utils._handle_task_exception","title":"_handle_task_exception","text":"<pre><code>_handle_task_exception(task, logger)\n</code></pre> <p>Print out exception and tracebook when a task dies with an exception.</p> Source code in <code>zeus/utils/async_utils.py</code> <pre><code>def _handle_task_exception(task: asyncio.Task, logger: logging.Logger) -&gt; None:\n    \"\"\"Print out exception and tracebook when a task dies with an exception.\"\"\"\n    try:\n        task.result()\n    except asyncio.CancelledError:\n        # Cancellation should not be logged as an error.\n        pass\n    except Exception:\n        # `logger.exception` automatically handles exception and traceback info.\n        logger.exception(\"Job task died with an exception!\")\n</code></pre>"},{"location":"reference/utils/env/","title":"env","text":""},{"location":"reference/utils/env/#zeus.utils.env","title":"zeus.utils.env","text":"<p>Tools related to environment variables.</p>"},{"location":"reference/utils/env/#zeus.utils.env.get_env","title":"get_env","text":"<pre><code>get_env(name, valtype, default=None)\n</code></pre> <p>Fetch an environment variable and cast it to the given type.</p> Source code in <code>zeus/utils/env.py</code> <pre><code>def get_env(name: str, valtype: Type[T], default: T | None = None) -&gt; T:\n    \"\"\"Fetch an environment variable and cast it to the given type.\"\"\"\n    try:\n        if valtype == bool:\n            val = os.environ[name].lower()\n            if val not in [\"true\", \"false\"]:\n                raise ValueError(f\"Strange boolean environment variable value '{val}'\")\n            return cast(T, val == \"true\")\n        return valtype(os.environ[name])  # type: ignore\n    except KeyError:\n        if default is not None:\n            return default\n        raise ValueError(f\"Missing environment variable '{name}'\") from None\n</code></pre>"},{"location":"reference/utils/framework/","title":"framework","text":""},{"location":"reference/utils/framework/#zeus.utils.framework","title":"zeus.utils.framework","text":"<p>Utilities for framework-specific code.</p>"},{"location":"reference/utils/framework/#zeus.utils.framework.torch_is_available","title":"torch_is_available  <code>cached</code>","text":"<pre><code>torch_is_available()\n</code></pre> <p>Check if PyTorch is available.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>@lru_cache(maxsize=1)\ndef torch_is_available():\n    \"\"\"Check if PyTorch is available.\"\"\"\n    try:\n        import torch\n\n        assert (\n            torch.cuda.is_available()\n        ), \"PyTorch is available but does not have CUDA support.\"\n        MODULE_CACHE[\"torch\"] = torch\n        logger.info(\"PyTorch with CUDA support is available.\")\n        return True\n    except ImportError:\n        logger.info(\"PyTorch is not available.\")\n        return False\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.cuda_sync","title":"cuda_sync","text":"<pre><code>cuda_sync(device=None)\n</code></pre> <p>Synchronize CPU and CUDA.</p> <code>cupy.cuda.Device.synchronize</code> may be a good choice to make <p>CUDA device synchronization more general. Haven't tested it yet.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>int | None</code> <p>The device to synchronize.</p> <code>None</code> Source code in <code>zeus/utils/framework.py</code> <pre><code>def cuda_sync(device: int | None = None) -&gt; None:\n    \"\"\"Synchronize CPU and CUDA.\n\n    Note: `cupy.cuda.Device.synchronize` may be a good choice to make\n          CUDA device synchronization more general. Haven't tested it yet.\n\n    Args:\n        device: The device to synchronize.\n    \"\"\"\n    if torch_is_available():\n        torch = MODULE_CACHE[\"torch\"]\n        torch.cuda.synchronize(device)\n        return\n\n    raise RuntimeError(\"No frameworks are available.\")\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":""},{"location":"reference/utils/logging/#zeus.utils.logging","title":"zeus.utils.logging","text":"<p>Utilities for logging.</p>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole","title":"FileAndConsole","text":"<p>Like tee, but for Python prints.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>class FileAndConsole:\n    \"\"\"Like tee, but for Python prints.\"\"\"\n\n    def __init__(self, filepath: Path) -&gt; None:\n        \"\"\"Initialize the object.\"\"\"\n        self.file = open(filepath, \"w\")\n        self.stdout = sys.stdout\n\n    def write(self, message):\n        \"\"\"Write message.\"\"\"\n        self.file.write(message)\n        self.stdout.write(message)\n        self.file.flush()\n        self.stdout.flush()\n\n    def flush(self):\n        \"\"\"Flush both log file and stdout.\"\"\"\n        self.file.flush()\n        self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.__init__","title":"__init__","text":"<pre><code>__init__(filepath)\n</code></pre> Source code in <code>zeus/utils/logging.py</code> <pre><code>def __init__(self, filepath: Path) -&gt; None:\n    \"\"\"Initialize the object.\"\"\"\n    self.file = open(filepath, \"w\")\n    self.stdout = sys.stdout\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.write","title":"write","text":"<pre><code>write(message)\n</code></pre> <p>Write message.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>def write(self, message):\n    \"\"\"Write message.\"\"\"\n    self.file.write(message)\n    self.stdout.write(message)\n    self.file.flush()\n    self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.flush","title":"flush","text":"<pre><code>flush()\n</code></pre> <p>Flush both log file and stdout.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>def flush(self):\n    \"\"\"Flush both log file and stdout.\"\"\"\n    self.file.flush()\n    self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.get_logger","title":"get_logger","text":"<pre><code>get_logger(name, level=logging.INFO, propagate=False)\n</code></pre> <p>Get a logger with the given name with some formatting configs.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>def get_logger(\n    name: str,\n    level: int = logging.INFO,\n    propagate: bool = False,\n) -&gt; logging.Logger:\n    \"\"\"Get a logger with the given name with some formatting configs.\"\"\"\n    if name in logging.Logger.manager.loggerDict:\n        return logging.getLogger(name)\n\n    logger = logging.getLogger(name)\n    logger.propagate = propagate\n    logger.setLevel(os.environ.get(\"ZEUS_LOG_LEVEL\", level))\n    formatter = logging.Formatter(\n        \"[%(asctime)s] [%(name)s](%(filename)s:%(lineno)d) %(message)s\"\n    )\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"reference/utils/lr_scaler/","title":"lr_scaler","text":""},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler","title":"zeus.utils.lr_scaler","text":"<p>Classes that enclose learning rate scaling rules.</p>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.SquareRootScaler","title":"SquareRootScaler  <code>dataclass</code>","text":"<p>Square root scaling.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>int</code> <p>The initial batch size</p> required <code>lr</code> <code>float</code> <p>The initial learning rate</p> required Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>@dataclass\nclass SquareRootScaler:\n    \"\"\"Square root scaling.\n\n    Args:\n        bs: The initial batch size\n        lr: The initial learning rate\n    \"\"\"\n\n    bs: int\n    lr: float\n\n    def compute_lr(self, new_bs: int) -&gt; float:\n        \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n        return self.lr * math.sqrt(new_bs / self.bs)\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.SquareRootScaler.compute_lr","title":"compute_lr","text":"<pre><code>compute_lr(new_bs)\n</code></pre> <p>Compute the scaled learning rate given the new batch size.</p> Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>def compute_lr(self, new_bs: int) -&gt; float:\n    \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n    return self.lr * math.sqrt(new_bs / self.bs)\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.LinearScaler","title":"LinearScaler  <code>dataclass</code>","text":"<p>Linear scaling.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>int</code> <p>The initial batch size</p> required <code>lr</code> <code>float</code> <p>The initial learning rate</p> required Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>@dataclass\nclass LinearScaler:\n    \"\"\"Linear scaling.\n\n    Args:\n        bs: The initial batch size\n        lr: The initial learning rate\n    \"\"\"\n\n    bs: int\n    lr: float\n\n    def compute_lr(self, new_bs: int) -&gt; float:\n        \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n        return self.lr * new_bs / self.bs\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.LinearScaler.compute_lr","title":"compute_lr","text":"<pre><code>compute_lr(new_bs)\n</code></pre> <p>Compute the scaled learning rate given the new batch size.</p> Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>def compute_lr(self, new_bs: int) -&gt; float:\n    \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n    return self.lr * new_bs / self.bs\n</code></pre>"},{"location":"reference/utils/metric/","title":"metric","text":""},{"location":"reference/utils/metric/#zeus.utils.metric","title":"zeus.utils.metric","text":"<p>Defines the energy-time cost metric function.</p>"},{"location":"reference/utils/metric/#zeus.utils.metric.zeus_cost","title":"zeus_cost","text":"<pre><code>zeus_cost(energy, time, eta_knob, max_power)\n</code></pre> <p>Compute Zeus's energy-time cost metric.</p> <p>Trades off ETA and TTA based on the value of <code>eta_knob</code>. The caller is expected to do bound checking for <code>eta_knob</code>, because <code>eta_knob</code> does not change frequently.</p> <p>Parameters:</p> Name Type Description Default <code>energy</code> <code>float</code> <p>Joules</p> required <code>time</code> <code>float</code> <p>seconds</p> required <code>eta_knob</code> <code>float</code> <p>Real number in [0, 1].</p> required <code>max_power</code> <code>int | float</code> <p>The maximum power limit of the GPU.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cost of the DL training job.</p> Source code in <code>zeus/utils/metric.py</code> <pre><code>def zeus_cost(\n    energy: float, time: float, eta_knob: float, max_power: int | float\n) -&gt; float:\n    \"\"\"Compute Zeus's energy-time cost metric.\n\n    Trades off ETA and TTA based on the value of `eta_knob`.\n    The caller is expected to do bound checking for `eta_knob`,\n    because `eta_knob` does not change frequently.\n\n    Args:\n        energy: Joules\n        time: seconds\n        eta_knob: Real number in [0, 1].\n        max_power: The maximum power limit of the GPU.\n\n    Returns:\n        The cost of the DL training job.\n    \"\"\"\n    return eta_knob * energy + (1 - eta_knob) * max_power * time\n</code></pre>"},{"location":"reference/utils/metric/#zeus.utils.metric.energy","title":"energy","text":"<pre><code>energy(logfile, start=None, end=None)\n</code></pre> <p>Compute the energy consumption from the Zeus monitor power log file.</p> <p><code>start</code> and <code>end</code> are in units of seconds, relative to the beginning of the time window captured by the log file. Only the time window between <code>start</code> and <code>end</code> will be considered when computing energy.</p> <p><code>start</code> and <code>end</code> can be negative, in which case the pointers wrap around and effectively the absolute value is subtracted from the end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Path | str</code> <p>Path to the power log file produced by the Zeus monitor.</p> required <code>start</code> <code>float | None</code> <p>Start time of the window to consider.</p> <code>None</code> <code>end</code> <code>float | None</code> <p>End time of the window to consider.</p> <code>None</code> Source code in <code>zeus/utils/metric.py</code> <pre><code>def energy(\n    logfile: Path | str,\n    start: float | None = None,\n    end: float | None = None,\n) -&gt; float:\n    \"\"\"Compute the energy consumption from the Zeus monitor power log file.\n\n    `start` and `end` are in units of seconds, relative to the beginning of\n    the time window captured by the log file. Only the time window between\n    `start` and `end` will be considered when computing energy.\n\n    `start` and `end` can be negative, in which case the pointers wrap around\n    and effectively the absolute value is subtracted from the end of the window.\n\n    Args:\n        logfile: Path to the power log file produced by the Zeus monitor.\n        start: Start time of the window to consider.\n        end: End time of the window to consider.\n    \"\"\"\n    df = cast(pd.DataFrame, pd.read_csv(logfile, engine=\"python\", skipfooter=1))\n    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n    start_timestamp = df.iloc[0][\"Time\"]\n    end_timestamp = df.iloc[-1][\"Time\"]\n    if start is not None:\n        origin = start_timestamp if start &gt;= 0.0 else end_timestamp\n        df = df.loc[df[\"Time\"] &gt;= origin + timedelta(seconds=start)]\n    if end is not None:\n        origin = start_timestamp if end &gt;= 0.0 else end_timestamp\n        df = df.loc[df[\"Time\"] &lt;= origin + timedelta(seconds=end)]\n    seconds = _get_seconds(df)\n    watts = _get_watts(df)\n    return auc(seconds, watts)\n</code></pre>"},{"location":"reference/utils/metric/#zeus.utils.metric.avg_power","title":"avg_power","text":"<pre><code>avg_power(logfile, start=None, end=None)\n</code></pre> <p>Compute the average power consumption from the Zeus monitor power log file.</p> <p><code>start</code> and <code>end</code> are in units of seconds, relative to the beginning of the time window captured by the log file. Only the time window between <code>start</code> and <code>end</code> will be considered when computing average power.</p> <p><code>start</code> and <code>end</code> can be negative, in which case the pointers wrap around and effectively the absolute value is subtracted from the end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Path | str</code> <p>Path to the power log file produced by the Zeus monitor.</p> required <code>start</code> <code>float | None</code> <p>Start time of the window to consider.</p> <code>None</code> <code>end</code> <code>float | None</code> <p>End time of the window to consider.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>From <code>sklearn.metrics.auc</code>, when the duration of the profiling window is too small.</p> Source code in <code>zeus/utils/metric.py</code> <pre><code>def avg_power(\n    logfile: Path | str,\n    start: float | None = None,\n    end: float | None = None,\n) -&gt; float:\n    \"\"\"Compute the average power consumption from the Zeus monitor power log file.\n\n    `start` and `end` are in units of seconds, relative to the beginning of\n    the time window captured by the log file. Only the time window between\n    `start` and `end` will be considered when computing average power.\n\n    `start` and `end` can be negative, in which case the pointers wrap around\n    and effectively the absolute value is subtracted from the end of the window.\n\n    Args:\n        logfile: Path to the power log file produced by the Zeus monitor.\n        start: Start time of the window to consider.\n        end: End time of the window to consider.\n\n    Raises:\n        ValueError: From `sklearn.metrics.auc`, when the duration of the\n            profiling window is too small.\n    \"\"\"\n    df = cast(pd.DataFrame, pd.read_csv(logfile, engine=\"python\", skipfooter=1))\n    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n    if start is not None:\n        df = df.loc[df[\"Time\"] &gt;= df.iloc[0][\"Time\"] + timedelta(seconds=start)]\n    if end is not None:\n        df = df.loc[df[\"Time\"] &lt;= df.iloc[0][\"Time\"] + timedelta(seconds=end)]\n    seconds = _get_seconds(df)\n    watts = _get_watts(df)\n    area = auc(seconds, watts)\n    return area / (max(seconds) - min(seconds))\n</code></pre>"},{"location":"reference/utils/pydantic_v1/","title":"pydantic_v1","text":""},{"location":"reference/utils/pydantic_v1/#zeus.utils.pydantic_v1","title":"zeus.utils.pydantic_v1","text":"<p>Compatibility layer for Pydantic v1 and v2.</p> <p>We don't want to pin any specific version of Pydantic. With this, we can import things from <code>zeus.utils.pydantic_v1</code> and always use the V1 API regardless of the installed version of Pydantic.</p> <p>Inspired by Deepspeed: https://github.com/microsoft/DeepSpeed/blob/5d754606/deepspeed/pydantic_v1.py</p>"},{"location":"reference/utils/testing/","title":"testing","text":""},{"location":"reference/utils/testing/#zeus.utils.testing","title":"zeus.utils.testing","text":"<p>Utilities for testing.</p>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor","title":"ReplayZeusMonitor","text":"<p>               Bases: <code>ZeusMonitor</code></p> <p>A mock ZeusMonitor that replays windows recorded by a real monitor.</p> <p>This class is for testing only. Based on a CSV log file that records the time and energy measurements of <code>ZeusMonitor</code> measurement windows, users can drop-in replace <code>ZeusMonitor</code> with this class to replay the measurement windows and fast forward training and time/energy measurement.</p> <p>The methods exposed is identical to or a superset of <code>ZeusMonitor</code>, but behaves differently. Instead of monitoring the GPU, it replays events from a log file. The log file generated by <code>ZeusMonitor</code> (<code>log_file</code>) is guaranteed to be compatible and will replay time and energy measurements just like how the real monitor experienced them. Note that in the case of concurrent ongoing measurement windows, the log rows file should record windows in the order of <code>end_window</code> calls.</p> <p>Attributes:</p> Name Type Description <code>gpu_indices</code> <code>`list[int]`</code> <p>Indices of all the CUDA devices to monitor.</p> Source code in <code>zeus/utils/testing.py</code> <pre><code>class ReplayZeusMonitor(ZeusMonitor):\n    \"\"\"A mock ZeusMonitor that replays windows recorded by a real monitor.\n\n    This class is for testing only. Based on a CSV log file that records the time\n    and energy measurements of `ZeusMonitor` measurement windows, users can drop-in\n    replace `ZeusMonitor` with this class to replay the measurement windows and\n    *fast forward* training and time/energy measurement.\n\n    The methods exposed is identical to or a superset of `ZeusMonitor`, but behaves\n    differently. Instead of monitoring the GPU, it replays events from a log file.\n    The log file generated by `ZeusMonitor` (`log_file`) is guaranteed to be compatible\n    and will replay time and energy measurements just like how the real monitor\n    experienced them. Note that in the case of concurrent ongoing measurement windows,\n    the log rows file should record windows in the order of `end_window` calls.\n\n    Attributes:\n        gpu_indices (`list[int]`): Indices of all the CUDA devices to monitor.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        approx_instant_energy: bool = False,\n        log_file: str | Path | None = None,\n        ignore_sync_cuda: bool = False,\n        match_window_name: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the replay monitor.\n\n        The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]):\n        `start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy`\n\n        Args:\n            gpu_indices: Indices of all the CUDA devices to monitor. This should be consistent\n                with the indices used in the log file. If `None`, GPU indices will be inferred\n                from the log file header. Does not respect `CUDA_VISIBLE_DEVICES`.\n                (Default: `None`)\n            approx_instant_energy: Whether to approximate the instant energy consumption. Not used.\n            log_file: Path to the log CSV file to replay events from. `None` is not allowed.\n            ignore_sync_cuda: Whether to ignore `sync_cuda` calls. (Default: `False`)\n            match_window_name: Whether to make sure window names match. (Default: `True`)\n        \"\"\"\n        if log_file is None:\n            raise ValueError(\"`log_file` cannot be `None` for `ReplayZeusMonitor`.\")\n\n        self.approx_instant_energy = approx_instant_energy\n        self.log_file = open(log_file)\n        self.ignore_sync_cuda = ignore_sync_cuda\n        self.match_window_name = match_window_name\n\n        # Infer GPU indices from the log file if not provided.\n        header = self.log_file.readline()\n        if gpu_indices is None:\n            gpu_indices = [\n                int(gpu.split(\"_\")[0][3:]) for gpu in header.split(\",\")[3:] if gpu\n            ]\n        self.nvml_gpu_indices = self.gpu_indices = gpu_indices\n\n        self.logger = get_logger(type(self).__name__)\n        self.logger.info(\n            \"Replaying from '%s' with GPU indices %s\", log_file, gpu_indices\n        )\n\n        # Keep track of ongoing measurement windows.\n        self.ongoing_windows = []\n\n    def begin_window(self, key: str, sync_cuda: bool = True) -&gt; None:\n        \"\"\"Begin a new window.\n\n        This method just pushes the key into a list of ongoing measurement windows,\n        and just makes sure it's unique.\n\n        Args:\n            key: Name of the measurement window.\n            sync_cuda: Whether to synchronize CUDA before starting the measurement window.\n                (Default: `True`)\n        \"\"\"\n        if key in self.ongoing_windows:\n            raise RuntimeError(f\"Window {key} is already ongoing.\")\n        self.ongoing_windows.append(key)\n\n        if not self.ignore_sync_cuda and sync_cuda:\n            for gpu_index in self.gpu_indices:\n                cuda_sync(gpu_index)\n\n        self.logger.info(\"Measurement window '%s' started.\", key)\n\n    def end_window(\n        self, key: str, sync_cuda: bool = True, cancel: bool = False\n    ) -&gt; Measurement:\n        \"\"\"End an ongoing window.\n\n        This method pops the key from a list of ongoing measurement windows and\n        constructs a `Measurement` object corresponding to the name of the window\n        from the log file. If the name of the window does not match the expected\n        one, a `RuntimeError` is raised.\n\n        Args:\n            key: Name of the measurement window.\n            sync_cuda: Whether to synchronize CUDA before ending the measurement window.\n                (Default: `True`)\n            cancel: Whether to cancel the measurement window. This will not consume a\n                line from the log file. (Default: `False`)\n        \"\"\"\n        try:\n            self.ongoing_windows.remove(key)\n        except ValueError:\n            raise RuntimeError(f\"Window {key} is not ongoing.\") from None\n\n        if not self.ignore_sync_cuda and sync_cuda:\n            for gpu_index in self.gpu_indices:\n                cuda_sync(gpu_index)\n\n        if cancel:\n            self.logger.info(\"Measurement window '%s' cancelled.\", key)\n            return Measurement(\n                time=0.0, energy={gpu_index: 0.0 for gpu_index in self.gpu_indices}\n            )\n\n        # Read the next line from the log file.\n        line = self.log_file.readline()\n        if not line:\n            raise RuntimeError(\"No more lines in the log file.\")\n        _, window_name, *nums = line.split(\",\")\n        if self.match_window_name and window_name != key:\n            raise RuntimeError(f\"Was expecting {window_name}, not {key}.\")\n        if len(nums) != len(self.gpu_indices) + 1:\n            raise RuntimeError(\n                f\"Line has unexpected number of energy measurements: {line}\"\n            )\n        time_consumption, *energy_consumptions = map(float, nums)\n        energy = dict(zip(self.gpu_indices, energy_consumptions))\n        measurement = Measurement(time=time_consumption, energy=energy)\n\n        self.logger.info(\"Measurement window '%s' ended (%s).\", key, measurement)\n\n        return measurement\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.__init__","title":"__init__","text":"<pre><code>__init__(\n    gpu_indices=None,\n    approx_instant_energy=False,\n    log_file=None,\n    ignore_sync_cuda=False,\n    match_window_name=True,\n)\n</code></pre> <p>The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]): <code>start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy</code></p> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. This should be consistent with the indices used in the log file. If <code>None</code>, GPU indices will be inferred from the log file header. Does not respect <code>CUDA_VISIBLE_DEVICES</code>. (Default: <code>None</code>)</p> <code>None</code> <code>approx_instant_energy</code> <code>bool</code> <p>Whether to approximate the instant energy consumption. Not used.</p> <code>False</code> <code>log_file</code> <code>str | Path | None</code> <p>Path to the log CSV file to replay events from. <code>None</code> is not allowed.</p> <code>None</code> <code>ignore_sync_cuda</code> <code>bool</code> <p>Whether to ignore <code>sync_cuda</code> calls. (Default: <code>False</code>)</p> <code>False</code> <code>match_window_name</code> <code>bool</code> <p>Whether to make sure window names match. (Default: <code>True</code>)</p> <code>True</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    approx_instant_energy: bool = False,\n    log_file: str | Path | None = None,\n    ignore_sync_cuda: bool = False,\n    match_window_name: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the replay monitor.\n\n    The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]):\n    `start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy`\n\n    Args:\n        gpu_indices: Indices of all the CUDA devices to monitor. This should be consistent\n            with the indices used in the log file. If `None`, GPU indices will be inferred\n            from the log file header. Does not respect `CUDA_VISIBLE_DEVICES`.\n            (Default: `None`)\n        approx_instant_energy: Whether to approximate the instant energy consumption. Not used.\n        log_file: Path to the log CSV file to replay events from. `None` is not allowed.\n        ignore_sync_cuda: Whether to ignore `sync_cuda` calls. (Default: `False`)\n        match_window_name: Whether to make sure window names match. (Default: `True`)\n    \"\"\"\n    if log_file is None:\n        raise ValueError(\"`log_file` cannot be `None` for `ReplayZeusMonitor`.\")\n\n    self.approx_instant_energy = approx_instant_energy\n    self.log_file = open(log_file)\n    self.ignore_sync_cuda = ignore_sync_cuda\n    self.match_window_name = match_window_name\n\n    # Infer GPU indices from the log file if not provided.\n    header = self.log_file.readline()\n    if gpu_indices is None:\n        gpu_indices = [\n            int(gpu.split(\"_\")[0][3:]) for gpu in header.split(\",\")[3:] if gpu\n        ]\n    self.nvml_gpu_indices = self.gpu_indices = gpu_indices\n\n    self.logger = get_logger(type(self).__name__)\n    self.logger.info(\n        \"Replaying from '%s' with GPU indices %s\", log_file, gpu_indices\n    )\n\n    # Keep track of ongoing measurement windows.\n    self.ongoing_windows = []\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_cuda=True)\n</code></pre> <p>Begin a new window.</p> <p>This method just pushes the key into a list of ongoing measurement windows, and just makes sure it's unique.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_cuda</code> <code>bool</code> <p>Whether to synchronize CUDA before starting the measurement window. (Default: <code>True</code>)</p> <code>True</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def begin_window(self, key: str, sync_cuda: bool = True) -&gt; None:\n    \"\"\"Begin a new window.\n\n    This method just pushes the key into a list of ongoing measurement windows,\n    and just makes sure it's unique.\n\n    Args:\n        key: Name of the measurement window.\n        sync_cuda: Whether to synchronize CUDA before starting the measurement window.\n            (Default: `True`)\n    \"\"\"\n    if key in self.ongoing_windows:\n        raise RuntimeError(f\"Window {key} is already ongoing.\")\n    self.ongoing_windows.append(key)\n\n    if not self.ignore_sync_cuda and sync_cuda:\n        for gpu_index in self.gpu_indices:\n            cuda_sync(gpu_index)\n\n    self.logger.info(\"Measurement window '%s' started.\", key)\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_cuda=True, cancel=False)\n</code></pre> <p>End an ongoing window.</p> <p>This method pops the key from a list of ongoing measurement windows and constructs a <code>Measurement</code> object corresponding to the name of the window from the log file. If the name of the window does not match the expected one, a <code>RuntimeError</code> is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_cuda</code> <code>bool</code> <p>Whether to synchronize CUDA before ending the measurement window. (Default: <code>True</code>)</p> <code>True</code> <code>cancel</code> <code>bool</code> <p>Whether to cancel the measurement window. This will not consume a line from the log file. (Default: <code>False</code>)</p> <code>False</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def end_window(\n    self, key: str, sync_cuda: bool = True, cancel: bool = False\n) -&gt; Measurement:\n    \"\"\"End an ongoing window.\n\n    This method pops the key from a list of ongoing measurement windows and\n    constructs a `Measurement` object corresponding to the name of the window\n    from the log file. If the name of the window does not match the expected\n    one, a `RuntimeError` is raised.\n\n    Args:\n        key: Name of the measurement window.\n        sync_cuda: Whether to synchronize CUDA before ending the measurement window.\n            (Default: `True`)\n        cancel: Whether to cancel the measurement window. This will not consume a\n            line from the log file. (Default: `False`)\n    \"\"\"\n    try:\n        self.ongoing_windows.remove(key)\n    except ValueError:\n        raise RuntimeError(f\"Window {key} is not ongoing.\") from None\n\n    if not self.ignore_sync_cuda and sync_cuda:\n        for gpu_index in self.gpu_indices:\n            cuda_sync(gpu_index)\n\n    if cancel:\n        self.logger.info(\"Measurement window '%s' cancelled.\", key)\n        return Measurement(\n            time=0.0, energy={gpu_index: 0.0 for gpu_index in self.gpu_indices}\n        )\n\n    # Read the next line from the log file.\n    line = self.log_file.readline()\n    if not line:\n        raise RuntimeError(\"No more lines in the log file.\")\n    _, window_name, *nums = line.split(\",\")\n    if self.match_window_name and window_name != key:\n        raise RuntimeError(f\"Was expecting {window_name}, not {key}.\")\n    if len(nums) != len(self.gpu_indices) + 1:\n        raise RuntimeError(\n            f\"Line has unexpected number of energy measurements: {line}\"\n        )\n    time_consumption, *energy_consumptions = map(float, nums)\n    energy = dict(zip(self.gpu_indices, energy_consumptions))\n    measurement = Measurement(time=time_consumption, energy=energy)\n\n    self.logger.info(\"Measurement window '%s' ended (%s).\", key, measurement)\n\n    return measurement\n</code></pre>"},{"location":"research_overview/","title":"Research Overview","text":""},{"location":"research_overview/#research-overview","title":"Research Overview","text":"<p>Zeus is rooted on multiple research papers. Even more research is ongoing, and Zeus will continue to expand and get better at what it's doing.</p> <ol> <li>Zeus (2023): Paper | Blog | Slides</li> <li>Chase (2023): Paper</li> <li>Perseus (2023): Paper | Blog</li> </ol>"},{"location":"research_overview/perseus/","title":"Perseus","text":"<p>Perseus finds GPU frequency plans on the training time--energy Pareto frontier of large model training. These plans include one that does not make training any slower compared to not using Perseus, but yields free energy savings. If you have a bit more leeway as to when training should finish (e.g., You're good as long as training finishes by tomorrow morning), you can pick the frequency plan that slows down training by a couple percentages and save more energy.</p> Perseus discovers the entire iteration time--energy Pareto frontier. <p>How is this done? Large model training requires the distribution of work across multiple GPUs using a combination of multiple parallelization methods. The core observation of Perseus is that especially for pipeline parallelism, work cannot be perfectly split and balanced across every GPU; some GPUs have more work to do and some less. GPUs with smaller amounts of work finish before GPUs with more amounts of work, but ultimately training throughput is bound by GPUs with the most amount of work. In other words, GPUs with lighter load are running unnecessarily fast and wasting energy (i.e., there is energy bloat).</p> The pipeline frequency optimizer in action <p>We reduce energy bloat by controlling the execution speed of each pipeline instruction (forward and backward) in each stage by controlling the GPU's frequency in a fine-grained manner. We call the assignment of a GPU frequency to each pipeline instruction frequency plan, and Perseus gives you every Pareto-optimal frequency plan that you can choose any point on the iteration time--energy Pareto frontier.</p>"},{"location":"research_overview/perseus/#perseus-removing-energy-bloatfrom-large-model-training","title":"Perseus: Removing Energy Bloatfrom Large Model Training","text":"<p>Preprint</p>"},{"location":"research_overview/zeus/","title":"Zeus","text":""},{"location":"research_overview/zeus/#zeus-understanding-and-optimizinggpu-energy-consumption-of-dnn-training","title":"Zeus: Understanding and OptimizingGPU Energy Consumption of DNN Training","text":"<p>Paper | Slides | YouTube</p>"},{"location":"research_overview/zeus/#abstract","title":"Abstract","text":"<p>Training Deep Neural Networks (DNNs) is becoming more and more resource- and energy-intensive every year. Unfortunately, existing works primarily focus on optimizing DNN training for faster completion, often without considering the impact on energy efficiency.</p> <p>In this paper, we observe that common practices of DNN training can lead to inefficient energy usage. More importantly, we demonstrate that there is a tradeoff between energy consumption and performance optimization. To this end, we propose an optimization framework, Zeus, to navigate this tradeoff by automatically finding optimal job- and GPU-level configurations for recurring DNN training jobs. Zeus does not require any offline profiling and can adapt to data drifts.</p>"},{"location":"research_overview/zeus/#why-care-about-gpu-energy","title":"Why care about GPU energy?","text":"<p>Recent years have seen an increasing adoption of DNNs for intelligent applications. Large clusters of GPUs were created to support such growth, and the surge continues.</p> <p>GPUs are power-hungry hardware; GPUs consume ~ 70% of the power of the entire server when training DNNs.<sup>1</sup> At extreme scales, training the GPT-3 model just once consumes 1,287 MWh,<sup>2</sup> which is enough to supply an average US household for 120 years.<sup>3</sup></p> <p>However, latency and throughput have been the primary targets of existing optimization techniques, devoid of any careful consideration of how such optimizations might impact energy efficiency. We argue that energy should be considered as the third dimension.</p>"},{"location":"research_overview/zeus/#opportunity-for-energy-savings","title":"Opportunity for energy savings","text":"<p>We observe that common practices of DNN training can often lead to energy inefficiency.</p> <p>To see this, we trained<sup>4</sup> the same DNN multiple times using a sweep of possible batch sizes and GPU power limits.<sup>5</sup></p> Potential energy savings on an NVIDIA V100 GPU. <p>The baseline dotted line uses the default batch size from the model's publication and the default (maximum) GPU power limit. It can be seen that choosing the best batch size and power limit can lead to large energy savings.</p>"},{"location":"research_overview/zeus/#tradeoff-between-time-energy","title":"Tradeoff between time &amp; energy","text":"<p>Is energy reduction free?</p> <p>We discover that there is a tradeoff between DNN training time and energy consumption.</p> All (batch size, power limit) configurations and their time/energy consumption. The energy-time Pareto frontier zoomed in. <p>These results are from training DeepSpeech2 on LibriSpeech with an NVIDIA V100 GPU. Notice the yellow Pareto frontier of efficient (time, energy) pairs, resulting from a set of efficient (batch size, power limit) knobs.</p>"},{"location":"research_overview/zeus/#navigating-the-tradeoff","title":"Navigating the tradeoff","text":"<p>All points on the Pareto frontier are efficient, but which one is the best?</p> <p>Different users will have different answers, because they have different preferences of how they would like to trade off time and energy.<sup>6</sup></p> <p>To allow users to express their tradeoff preference, we define a simple cost metric<sup>7</sup></p> \\[ \\textrm{Cost} = \\eta \\cdot \\textrm{Energy} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{Time,} \\] <p>where the user picks the value of \\(\\eta\\) between 0 and 1. Smaller \\(\\eta\\) values will reduce more time, while larger ones will prefer to reduce more energy.</p>"},{"location":"research_overview/zeus/#finding-the-optimal-knob","title":"Finding the optimal knob","text":"<p>Given the user's preference via the value of \\(\\eta\\), how do we find the best (batch size, power limit) knob on the Pareto frontier?</p> <p>This is no easy problem. We only have the Pareto frontier in the previous plot because we trained all possible combinations of batch size and power limit until completion to characterize the tradeoff.<sup>8</sup></p> <p>Fortunately, DNN training jobs often recur in production GPU clusters,<sup>9</sup> allowing us to explore, observe, and optimize across job recurrences.</p> <p>This results in two main components in Zeus:</p> <ul> <li>Just-In-Time energy profiler: Finds the optimal power limit via online profiling.</li> <li>Multi-Armed Bandit + Thompson Sampling: Finds the optimal batch size across recurring training runs.</li> </ul>"},{"location":"research_overview/zeus/#research-reproducibility","title":"Research reproducibility","text":"<p>We have our trace-driven simulator open-sourced here with instructions.</p>"},{"location":"research_overview/zeus/#extending-the-zeus-simulator","title":"Extending the Zeus simulator","text":"<p>Users can implement custom policies that optimize batch size and power limit, and plug it into the Zeus simulator. We have training and energy traces for 6 different DNNs and 4 different NVIDIA GPU microarchitectures here, which the simulator runs with.</p> <p>Zeus defines two abstract classes <code>BatchSizeOptimizer</code> and <code>PowerLimitOptimizer</code> in <code>zeus._legacy.policy.interface</code>. Each class optimizes the batch size and power limit of a recurring training job respectively. As in our paper, the batch size optimizer is first invoked to decide which batch size to use, and then the power limit optimizer is invoked with both the job and the batch size chosen to decide which power limit to use. You can find examples of policy implementations in <code>zeus._legacy.policy.optimizer</code>.</p> <p>The Zeus simulator (<code>Simulator</code>) accepts one <code>BatchSizeOptimizer</code> and <code>PowerLimitOptimizer</code> in its constructor. A full-example can be found here.</p> <ol> <li> <p>Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 1877\u20131894, New York, NY, USA, 2022. Association for Computing Machinery.\u00a0\u21a9</p> </li> <li> <p>David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\u00a0\u21a9</p> </li> <li> <p>How much electricity does an American home use? https://www.eia.gov/tools/faqs/faq.php?id=97&amp;t=3.\u00a0\u21a9</p> </li> <li> <p>In all cases of training, we train until the DNN reaches a specific target validation metric. Thus, when we say time, it's TTA (Time To Accuracy). Likewise for energy, it's ETA (Enerty To Accuracy). Please refer to our paper for the complete workload table.\u00a0\u21a9</p> </li> <li> <p>It is possible to cap the maximum power draw of a GPU using NVML.\u00a0\u21a9</p> </li> <li> <p>For instance, some production training jobs might have tight deadlines; they probably don't want to trade time for energy savings. On the other hand, exploratory training jobs may have more leeway; it might make sense for them to reduce energy consumption at the cost of longer training time.\u00a0\u21a9</p> </li> <li> <p>\\(\\textrm{MaxPower}\\) is the maximum possible power limit of the GPU. It's just a constant number introduced to equalize the units of the left and right terms to Joules.\u00a0\u21a9</p> </li> <li> <p>Since doing this will consume so much time and energy, it may even offset or exceed the energy savings from choosing the optimal knobs if we decide to do it for every future incoming job!\u00a0\u21a9</p> </li> <li> <p>Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, et al. Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 620\u2013629. IEEE, 2018.\u00a0\u21a9</p> </li> </ol>"}]}